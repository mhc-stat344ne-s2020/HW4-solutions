{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSYV3U4JZdzt",
        "colab_type": "text"
      },
      "source": [
        "## Goals\n",
        "\n",
        "The goal of the coding part of this homework assignment is to practice the process of tuning hyperparameters to find a good neural network model.\n",
        "\n",
        "We will work with the same Reuters newswires data set from the last homework.  In that assignment, we accepted the model that Chollet used in the book.  Here we will see if we can improve that model at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO0Kphe9bPJk",
        "colab_type": "text"
      },
      "source": [
        "## Module Imports\n",
        "You don't need to make any changes to this code, just run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C74M21bXSNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras.datasets import reuters\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzsvi3njSYZO",
        "colab_type": "text"
      },
      "source": [
        "## Load data\n",
        "\n",
        "You don't need to make any changes to this code, just run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdlIkalQXS6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_and_val_data, train_and_val_labels), (test_data, test_labels) = reuters.load_data(\n",
        "    num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "x_train_and_val = vectorize_sequences(train_and_val_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "def to_one_hot(labels, dimension=46):\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "one_hot_train_and_val_labels = to_one_hot(train_and_val_labels)\n",
        "y_train_and_val = one_hot_train_and_val_labels\n",
        "\n",
        "one_hot_test_labels = to_one_hot(test_labels)\n",
        "y_test = one_hot_test_labels\n",
        "\n",
        "x_val = x_train_and_val[:1000]\n",
        "x_train = x_train_and_val[1000:]\n",
        "\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "y_train = one_hot_train_labels[1000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhrYx59ySdc_",
        "colab_type": "text"
      },
      "source": [
        "## Chollet's Model\n",
        "You don't have to do anything here other than run the code below.  This is our baseline model, from the book."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RETrtWsUXYFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chollet_model = models.Sequential()\n",
        "chollet_model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "chollet_model.add(layers.Dense(64, activation='relu'))\n",
        "chollet_model.add(layers.Dense(46, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WXMSq-hXhKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "68691421-ee43-4688-c93a-b970ffa71c6c"
      },
      "source": [
        "chollet_model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = chollet_model.fit(x_train,\n",
        "                    y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "7982/7982 [==============================] - 1s 131us/step - loss: 2.5443 - acc: 0.5405 - val_loss: 1.6874 - val_acc: 0.6520\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.3742 - acc: 0.7093 - val_loss: 1.2770 - val_acc: 0.7180\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.0264 - acc: 0.7757 - val_loss: 1.1146 - val_acc: 0.7500\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.8105 - acc: 0.8267 - val_loss: 1.0324 - val_acc: 0.7810\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.6437 - acc: 0.8613 - val_loss: 0.9676 - val_acc: 0.7920\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.5149 - acc: 0.8887 - val_loss: 0.9308 - val_acc: 0.8110\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.4186 - acc: 0.9085 - val_loss: 0.9275 - val_acc: 0.8150\n",
            "Epoch 8/20\n",
            "7982/7982 [==============================] - 0s 53us/step - loss: 0.3375 - acc: 0.9268 - val_loss: 0.9122 - val_acc: 0.8200\n",
            "Epoch 9/20\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.2855 - acc: 0.9390 - val_loss: 0.9163 - val_acc: 0.8170\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 0s 53us/step - loss: 0.2390 - acc: 0.9449 - val_loss: 0.9369 - val_acc: 0.8100\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.2056 - acc: 0.9511 - val_loss: 0.9143 - val_acc: 0.8190\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.1834 - acc: 0.9518 - val_loss: 0.9661 - val_acc: 0.8080\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 0s 52us/step - loss: 0.1619 - acc: 0.9523 - val_loss: 0.9527 - val_acc: 0.8120\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1482 - acc: 0.9549 - val_loss: 0.9962 - val_acc: 0.8090\n",
            "Epoch 15/20\n",
            "7982/7982 [==============================] - 0s 53us/step - loss: 0.1404 - acc: 0.9540 - val_loss: 1.0321 - val_acc: 0.7970\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.1303 - acc: 0.9560 - val_loss: 1.0349 - val_acc: 0.7990\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.1213 - acc: 0.9580 - val_loss: 1.0461 - val_acc: 0.8020\n",
            "Epoch 18/20\n",
            "7982/7982 [==============================] - 0s 52us/step - loss: 0.1212 - acc: 0.9565 - val_loss: 1.0610 - val_acc: 0.8020\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.1110 - acc: 0.9575 - val_loss: 1.0975 - val_acc: 0.8010\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 0s 51us/step - loss: 0.1132 - acc: 0.9568 - val_loss: 1.1298 - val_acc: 0.7960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI-FebYPXpKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "5e586721-8831-4b3c-b8f3-0499a9011151"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "\n",
        "plt.plot(epochs, acc, label='Training acc')\n",
        "plt.plot(epochs, val_acc, label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j_fcs-bSqFi",
        "colab_type": "text"
      },
      "source": [
        "The validation set accuracy of this model ended up at about 0.8, and is well below the training set accuracy.  We'll try to find a combination of hyperparameters with better validation set accuracy, and then refit both our final model and Chollet's model to the full combined training and validation set data and evaluate their performance on the test set.\n",
        "\n",
        "## Hyperparameter tuning, adding regularization\n",
        "\n",
        "We have a pretty good model with some evidence that it is overfitting the training data.  Let's see if we can tune some of the hyperparameters of the model to achieve better validation set accuracy.  We will consider adding regularization using an $L_2$ penalty and/or dropout.  In this case, the size of our input layer (10000 features) is much larger than the size of our hidden layers, so it seems possible that we'd want to use dropout on the input layer, potentially with a different (larger?) dropout rate than the dropout rate for the hidden layers.  This means we will have three hyperparameters to pick: (1) dropout rate for the input layer; (2) dropout rate for the hidden layers; and (3) the penalty for $L_2$ regularization, applied to both hidden layers.  Let's use random search to explore performance of models with different values of these three hyperparameters.\n",
        "\n",
        "To do that, we will need to:\n",
        "\n",
        " * randomly generate sets of values for each of these hyperparameters.  Let's generate 8 sets of values; this should be enough to give a sense of which hyperparameters have the biggest impact on performance of this model without taking forever to fit (it will take long enough with just 8).\n",
        " * Specify and fit a model with each combination of hyperparameter settings, and save the validation set accuracy.\n",
        "\n",
        "#### 1. Random generation of hyperparameter values.\n",
        "\n",
        " * Common values for the dropout rate are between 0.0 (no dropout) and 0.5.  However, the size of our input features is so large that it seems possible we would want an even larger dropout rate on the input layer; let's consider values between 0.0 and 0.9 for the input layer (but note that dropout rates this large are unusual).  You can generate these uniformly at random in these intervals using the np.random.uniform function.  You want 8 of them, so set your size appropriately.\n",
        " * Common values for the penalty parameter $\\lambda$ are from about 0 (no $L_2$ penalty) to 0.1.  However, the effect of the penalty does not scale linearly: the effect on model performance of going from a penalty of $\\lambda = 0.0001$ to $\\lambda = 0.0002$ is larger than the effect of going from $\\lambda = 0.01$ to $\\lambda = 0.0101$ (though to be honest the relationship looked pretty close to linear in my results).  This means we would like to explore more values of $\\lambda$ near 0 than near 0.1.  We can do this by working on a logarithmic scale.  We will:\n",
        "    * First, generate exponents $u$ uniformly distributed between -6 and -1.  You want 8 of these.\n",
        "    * Second, set the values of $\\lambda$ to try to $10^{u}$.  In this way, our values of lambda will be between $10^{-6}$ and $10^{-1}$, with more values closer to $10^{-6}$.\n",
        "\n",
        "We'll assume that Chollet did a pretty thorough exporation of the number of layers and number of units per layer, and keep those hyperparameters fixed.  We'll also stick with the rmsprop optimizer with batch size of 512 he used.\n",
        "\n",
        "**In order for the code I have provided in the next part to work, your input_dropout_rates, hidden_dropout_rates, and l2_penalties should be one-dimensional arrays.  This means you should use size (8,) instead of something like (8.1).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwG6y4ajnDH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "2563a6d8-44fa-43d4-e168-c9c2656e89cf"
      },
      "source": [
        "# Setting a seed so we all get comparable results\n",
        "np.random.seed(64730)\n",
        "\n",
        "# Generate dropout rates for the input layer:\n",
        "# 8 numbers uniformly distributed from 0 to 0.9\n",
        "input_dropout_rates = np.random.uniform(low = 0.0, high = 0.9, size = (8,))\n",
        "print(\"input dropout rates = \" + str(input_dropout_rates))\n",
        "\n",
        "# Generate dropout rates for the hidden layers:\n",
        "# 8 numbers uniformly distributed from 0 to 0.5\n",
        "hidden_dropout_rates = np.random.uniform(low = 0.0, high = 0.5, size = (8,))\n",
        "print(\"hidden dropout rates = \" + str(hidden_dropout_rates))\n",
        "\n",
        "# Generate L_2 penalty parameters, in two stages:\n",
        "# u should be uniformly distributed from -6 to -1\n",
        "# then l2_penalties is 10 to the power of u\n",
        "# Note that in python, lambda is a reserved keyword so it's best to use a\n",
        "# different name, which is why I've gone with l2_penalties here\n",
        "u = np.random.uniform(low = -6, high = -1, size = (8,))\n",
        "l2_penalties = 10**u\n",
        "print(\"l2_penalties = \" + str(l2_penalties))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input dropout rates = [0.72807915 0.78121579 0.33759414 0.60574263 0.43113313 0.17192187\n",
            " 0.41814111 0.78693711]\n",
            "hidden dropout rates = [0.30954935 0.09599261 0.06038929 0.03211626 0.43386078 0.01608936\n",
            " 0.48785323 0.40673594]\n",
            "l2_penalties = [1.17703389e-06 8.94505148e-05 1.94648204e-05 4.16051385e-03\n",
            " 2.23930277e-05 1.49160276e-06 7.48310905e-03 2.21459085e-04]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY4Oxi38jZTC",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Fit models and save validation set performance for each\n",
        "\n",
        "Write and code below to define and fit all 8 of your models with different values for the dropout rates and $L_2$ penalty parameter lambda.\n",
        "\n",
        " * Use dropout on the input layer as well as each hidden layer\n",
        " * Use 64 units and a relu activation for all hidden layers\n",
        " * Because regularization can slow down getting a good fit to the training data, let's use 100 epochs instead of 20.\n",
        "\n",
        "This will take several minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kvnOkc3mbyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04b05cf6-9f31-4086-bff4-8c6be80f8e14"
      },
      "source": [
        "val_acc = np.zeros((8,))\n",
        "\n",
        "for (i, input_dropout_rate, hidden_dropout_rate, l2_penalty) in \\\n",
        "  zip(range(8), input_dropout_rates, hidden_dropout_rates, l2_penalties):\n",
        "  # define model\n",
        "  model = models.Sequential()\n",
        "\n",
        "  # add dropout layer for inputs, with its input_dropout_rate\n",
        "  model.add(layers.Dropout(rate = input_dropout_rate))\n",
        "\n",
        "  # add first hidden layer with L2 regularization and another dropout layer\n",
        "  model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l = l2_penalty), input_shape=(10000,)))\n",
        "  model.add(layers.Dropout(rate = hidden_dropout_rate))\n",
        "\n",
        "  # add second hidden layer with L2 regularization and another dropout layer\n",
        "  model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l = l2_penalty)))\n",
        "  model.add(layers.Dropout(rate = hidden_dropout_rate))\n",
        "\n",
        "  # add output layer\n",
        "  model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "  # compile model\n",
        "  model.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  # fit model using x_train and y_train, 100 epochs, a batch_size of 512, and\n",
        "  # validation_data = (x_val, y_val)\n",
        "  history = model.fit(x_train,\n",
        "                      y_train,\n",
        "                      epochs=100,\n",
        "                      batch_size=512,\n",
        "                      validation_data=(x_val, y_val))\n",
        "  \n",
        "  # save the validation set classification accuracy after 100 epochs\n",
        "  val_acc[i] = history.history['val_acc'][-1]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 4s 502us/step - loss: 3.0310 - acc: 0.3502 - val_loss: 2.1354 - val_acc: 0.5470\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 2.1406 - acc: 0.5125 - val_loss: 1.6954 - val_acc: 0.5960\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.8626 - acc: 0.5585 - val_loss: 1.5388 - val_acc: 0.6400\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.7217 - acc: 0.5912 - val_loss: 1.4298 - val_acc: 0.6680\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.6255 - acc: 0.6160 - val_loss: 1.3368 - val_acc: 0.6760\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.5381 - acc: 0.6443 - val_loss: 1.2713 - val_acc: 0.7050\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.4729 - acc: 0.6577 - val_loss: 1.2202 - val_acc: 0.7220\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4057 - acc: 0.6699 - val_loss: 1.1658 - val_acc: 0.7420\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3587 - acc: 0.6827 - val_loss: 1.1261 - val_acc: 0.7450\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3154 - acc: 0.6956 - val_loss: 1.0964 - val_acc: 0.7600\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2766 - acc: 0.6968 - val_loss: 1.0703 - val_acc: 0.7620\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2381 - acc: 0.7141 - val_loss: 1.0426 - val_acc: 0.7670\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.2025 - acc: 0.7157 - val_loss: 1.0228 - val_acc: 0.7690\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.1627 - acc: 0.7243 - val_loss: 1.0074 - val_acc: 0.7770\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1316 - acc: 0.7286 - val_loss: 0.9887 - val_acc: 0.7850\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1201 - acc: 0.7311 - val_loss: 0.9701 - val_acc: 0.7910\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0867 - acc: 0.7414 - val_loss: 0.9492 - val_acc: 0.8030\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.0728 - acc: 0.7417 - val_loss: 0.9445 - val_acc: 0.7970\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.0525 - acc: 0.7425 - val_loss: 0.9325 - val_acc: 0.8010\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.0335 - acc: 0.7453 - val_loss: 0.9197 - val_acc: 0.8090\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.9893 - acc: 0.7547 - val_loss: 0.9136 - val_acc: 0.8040\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9850 - acc: 0.7590 - val_loss: 0.8946 - val_acc: 0.8080\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9734 - acc: 0.7581 - val_loss: 0.8926 - val_acc: 0.8130\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9845 - acc: 0.7577 - val_loss: 0.8900 - val_acc: 0.8140\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9451 - acc: 0.7690 - val_loss: 0.8767 - val_acc: 0.8170\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9459 - acc: 0.7646 - val_loss: 0.8712 - val_acc: 0.8170\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9266 - acc: 0.7690 - val_loss: 0.8614 - val_acc: 0.8240\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9159 - acc: 0.7711 - val_loss: 0.8555 - val_acc: 0.8230\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.8811 - acc: 0.7820 - val_loss: 0.8449 - val_acc: 0.8260\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.8948 - acc: 0.7750 - val_loss: 0.8447 - val_acc: 0.8240\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.8775 - acc: 0.7841 - val_loss: 0.8388 - val_acc: 0.8270\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.8723 - acc: 0.7824 - val_loss: 0.8381 - val_acc: 0.8280\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.8708 - acc: 0.7819 - val_loss: 0.8329 - val_acc: 0.8250\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.8480 - acc: 0.7853 - val_loss: 0.8255 - val_acc: 0.8300\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.8437 - acc: 0.7814 - val_loss: 0.8357 - val_acc: 0.8250\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.8320 - acc: 0.7918 - val_loss: 0.8290 - val_acc: 0.8290\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.8315 - acc: 0.7895 - val_loss: 0.8207 - val_acc: 0.8330\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.8223 - acc: 0.7905 - val_loss: 0.8216 - val_acc: 0.8310\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7980 - acc: 0.7979 - val_loss: 0.8194 - val_acc: 0.8290\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7988 - acc: 0.7958 - val_loss: 0.8149 - val_acc: 0.8300\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7842 - acc: 0.8047 - val_loss: 0.8168 - val_acc: 0.8290\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7928 - acc: 0.7973 - val_loss: 0.8172 - val_acc: 0.8270\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7844 - acc: 0.7962 - val_loss: 0.8169 - val_acc: 0.8240\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7912 - acc: 0.7977 - val_loss: 0.8083 - val_acc: 0.8280\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7784 - acc: 0.8052 - val_loss: 0.8115 - val_acc: 0.8250\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7846 - acc: 0.7972 - val_loss: 0.8020 - val_acc: 0.8300\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.7560 - acc: 0.8067 - val_loss: 0.8037 - val_acc: 0.8320\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7289 - acc: 0.8135 - val_loss: 0.8047 - val_acc: 0.8310\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7411 - acc: 0.8082 - val_loss: 0.7983 - val_acc: 0.8330\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7270 - acc: 0.8106 - val_loss: 0.8019 - val_acc: 0.8360\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7304 - acc: 0.8136 - val_loss: 0.8031 - val_acc: 0.8330\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7271 - acc: 0.8047 - val_loss: 0.7982 - val_acc: 0.8300\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.7316 - acc: 0.8101 - val_loss: 0.8001 - val_acc: 0.8350\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7048 - acc: 0.8172 - val_loss: 0.8071 - val_acc: 0.8310\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7186 - acc: 0.8140 - val_loss: 0.8114 - val_acc: 0.8310\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7049 - acc: 0.8138 - val_loss: 0.8095 - val_acc: 0.8330\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6987 - acc: 0.8161 - val_loss: 0.8072 - val_acc: 0.8350\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7113 - acc: 0.8126 - val_loss: 0.8057 - val_acc: 0.8330\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7076 - acc: 0.8170 - val_loss: 0.8095 - val_acc: 0.8330\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.7022 - acc: 0.8200 - val_loss: 0.8055 - val_acc: 0.8330\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7060 - acc: 0.8125 - val_loss: 0.8056 - val_acc: 0.8350\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6900 - acc: 0.8191 - val_loss: 0.8066 - val_acc: 0.8330\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6856 - acc: 0.8216 - val_loss: 0.8101 - val_acc: 0.8370\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6761 - acc: 0.8207 - val_loss: 0.8099 - val_acc: 0.8310\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6794 - acc: 0.8177 - val_loss: 0.8084 - val_acc: 0.8390\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6797 - acc: 0.8186 - val_loss: 0.8109 - val_acc: 0.8370\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6586 - acc: 0.8267 - val_loss: 0.8144 - val_acc: 0.8290\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.6812 - acc: 0.8232 - val_loss: 0.8113 - val_acc: 0.8340\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6579 - acc: 0.8285 - val_loss: 0.8105 - val_acc: 0.8350\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6601 - acc: 0.8256 - val_loss: 0.8108 - val_acc: 0.8340\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6626 - acc: 0.8274 - val_loss: 0.8131 - val_acc: 0.8350\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6450 - acc: 0.8296 - val_loss: 0.8186 - val_acc: 0.8270\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.6511 - acc: 0.8262 - val_loss: 0.8057 - val_acc: 0.8260\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6470 - acc: 0.8287 - val_loss: 0.8184 - val_acc: 0.8300\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.6324 - acc: 0.8322 - val_loss: 0.8131 - val_acc: 0.8280\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6423 - acc: 0.8309 - val_loss: 0.8169 - val_acc: 0.8340\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6310 - acc: 0.8315 - val_loss: 0.8176 - val_acc: 0.8320\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6442 - acc: 0.8341 - val_loss: 0.8214 - val_acc: 0.8330\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6374 - acc: 0.8300 - val_loss: 0.8197 - val_acc: 0.8300\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.6261 - acc: 0.8310 - val_loss: 0.8208 - val_acc: 0.8310\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6489 - acc: 0.8281 - val_loss: 0.8211 - val_acc: 0.8350\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6469 - acc: 0.8295 - val_loss: 0.8168 - val_acc: 0.8400\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6465 - acc: 0.8284 - val_loss: 0.8128 - val_acc: 0.8340\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.6076 - acc: 0.8395 - val_loss: 0.8185 - val_acc: 0.8300\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6186 - acc: 0.8331 - val_loss: 0.8229 - val_acc: 0.8280\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6045 - acc: 0.8424 - val_loss: 0.8182 - val_acc: 0.8400\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6250 - acc: 0.8309 - val_loss: 0.8147 - val_acc: 0.8340\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6284 - acc: 0.8327 - val_loss: 0.8201 - val_acc: 0.8370\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6225 - acc: 0.8400 - val_loss: 0.8221 - val_acc: 0.8320\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.5958 - acc: 0.8390 - val_loss: 0.8185 - val_acc: 0.8340\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6231 - acc: 0.8346 - val_loss: 0.8281 - val_acc: 0.8340\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6107 - acc: 0.8378 - val_loss: 0.8217 - val_acc: 0.8300\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6269 - acc: 0.8344 - val_loss: 0.8139 - val_acc: 0.8280\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.5993 - acc: 0.8384 - val_loss: 0.8240 - val_acc: 0.8300\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.6072 - acc: 0.8379 - val_loss: 0.8206 - val_acc: 0.8300\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.5863 - acc: 0.8447 - val_loss: 0.8191 - val_acc: 0.8330\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.6108 - acc: 0.8335 - val_loss: 0.8177 - val_acc: 0.8370\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.5855 - acc: 0.8415 - val_loss: 0.8340 - val_acc: 0.8350\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.5818 - acc: 0.8400 - val_loss: 0.8322 - val_acc: 0.8370\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.5885 - acc: 0.8408 - val_loss: 0.8296 - val_acc: 0.8380\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 4s 513us/step - loss: 2.7937 - acc: 0.4306 - val_loss: 1.9323 - val_acc: 0.5820\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.9632 - acc: 0.5715 - val_loss: 1.5502 - val_acc: 0.6490\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.7320 - acc: 0.6174 - val_loss: 1.3848 - val_acc: 0.6940\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.5871 - acc: 0.6424 - val_loss: 1.2915 - val_acc: 0.7180\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4773 - acc: 0.6654 - val_loss: 1.2177 - val_acc: 0.7380\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4050 - acc: 0.6827 - val_loss: 1.1623 - val_acc: 0.7510\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3396 - acc: 0.6932 - val_loss: 1.1358 - val_acc: 0.7600\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.2813 - acc: 0.7028 - val_loss: 1.0826 - val_acc: 0.7770\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2365 - acc: 0.7161 - val_loss: 1.0571 - val_acc: 0.7840\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1883 - acc: 0.7238 - val_loss: 1.0276 - val_acc: 0.7870\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1645 - acc: 0.7270 - val_loss: 1.0006 - val_acc: 0.8020\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.1021 - acc: 0.7439 - val_loss: 0.9737 - val_acc: 0.8100\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.0837 - acc: 0.7410 - val_loss: 0.9629 - val_acc: 0.8060\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.0520 - acc: 0.7491 - val_loss: 0.9400 - val_acc: 0.8110\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.0366 - acc: 0.7486 - val_loss: 0.9327 - val_acc: 0.8210\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9994 - acc: 0.7611 - val_loss: 0.9169 - val_acc: 0.8250\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9835 - acc: 0.7591 - val_loss: 0.9039 - val_acc: 0.8180\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9604 - acc: 0.7623 - val_loss: 0.8993 - val_acc: 0.8300\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9415 - acc: 0.7676 - val_loss: 0.8900 - val_acc: 0.8230\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9270 - acc: 0.7691 - val_loss: 0.8798 - val_acc: 0.8260\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9119 - acc: 0.7754 - val_loss: 0.8838 - val_acc: 0.8270\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8970 - acc: 0.7805 - val_loss: 0.8690 - val_acc: 0.8360\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8765 - acc: 0.7851 - val_loss: 0.8695 - val_acc: 0.8310\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8636 - acc: 0.7876 - val_loss: 0.8561 - val_acc: 0.8310\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.8643 - acc: 0.7821 - val_loss: 0.8536 - val_acc: 0.8270\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.8566 - acc: 0.7841 - val_loss: 0.8474 - val_acc: 0.8300\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.8189 - acc: 0.7982 - val_loss: 0.8421 - val_acc: 0.8270\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.8418 - acc: 0.7873 - val_loss: 0.8334 - val_acc: 0.8320\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.8138 - acc: 0.7958 - val_loss: 0.8381 - val_acc: 0.8300\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7954 - acc: 0.8008 - val_loss: 0.8267 - val_acc: 0.8300\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8013 - acc: 0.7987 - val_loss: 0.8359 - val_acc: 0.8260\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7859 - acc: 0.8029 - val_loss: 0.8370 - val_acc: 0.8270\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.7842 - acc: 0.8017 - val_loss: 0.8348 - val_acc: 0.8330\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7866 - acc: 0.8003 - val_loss: 0.8265 - val_acc: 0.8330\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7669 - acc: 0.8027 - val_loss: 0.8324 - val_acc: 0.8320\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7594 - acc: 0.8068 - val_loss: 0.8386 - val_acc: 0.8250\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7374 - acc: 0.8138 - val_loss: 0.8377 - val_acc: 0.8190\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7599 - acc: 0.8054 - val_loss: 0.8235 - val_acc: 0.8280\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7458 - acc: 0.8073 - val_loss: 0.8235 - val_acc: 0.8310\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7417 - acc: 0.8131 - val_loss: 0.8441 - val_acc: 0.8240\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7476 - acc: 0.8106 - val_loss: 0.8330 - val_acc: 0.8290\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7364 - acc: 0.8141 - val_loss: 0.8278 - val_acc: 0.8400\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7200 - acc: 0.8160 - val_loss: 0.8320 - val_acc: 0.8250\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7337 - acc: 0.8093 - val_loss: 0.8327 - val_acc: 0.8280\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7195 - acc: 0.8138 - val_loss: 0.8216 - val_acc: 0.8340\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7014 - acc: 0.8176 - val_loss: 0.8337 - val_acc: 0.8280\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.7037 - acc: 0.8208 - val_loss: 0.8266 - val_acc: 0.8320\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.7018 - acc: 0.8173 - val_loss: 0.8212 - val_acc: 0.8310\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6915 - acc: 0.8217 - val_loss: 0.8182 - val_acc: 0.8290\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.7141 - acc: 0.8180 - val_loss: 0.8235 - val_acc: 0.8320\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6777 - acc: 0.8229 - val_loss: 0.8365 - val_acc: 0.8330\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6860 - acc: 0.8244 - val_loss: 0.8231 - val_acc: 0.8290\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.6871 - acc: 0.8218 - val_loss: 0.8327 - val_acc: 0.8240\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6795 - acc: 0.8252 - val_loss: 0.8197 - val_acc: 0.8300\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6854 - acc: 0.8264 - val_loss: 0.8219 - val_acc: 0.8270\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6859 - acc: 0.8216 - val_loss: 0.8403 - val_acc: 0.8220\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6530 - acc: 0.8266 - val_loss: 0.8241 - val_acc: 0.8290\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6649 - acc: 0.8265 - val_loss: 0.8247 - val_acc: 0.8360\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.6727 - acc: 0.8266 - val_loss: 0.8423 - val_acc: 0.8190\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6692 - acc: 0.8250 - val_loss: 0.8368 - val_acc: 0.8190\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6709 - acc: 0.8290 - val_loss: 0.8255 - val_acc: 0.8260\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6585 - acc: 0.8246 - val_loss: 0.8213 - val_acc: 0.8300\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6528 - acc: 0.8282 - val_loss: 0.8290 - val_acc: 0.8260\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6617 - acc: 0.8281 - val_loss: 0.8221 - val_acc: 0.8270\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6403 - acc: 0.8336 - val_loss: 0.8194 - val_acc: 0.8370\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6493 - acc: 0.8338 - val_loss: 0.8254 - val_acc: 0.8250\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6511 - acc: 0.8311 - val_loss: 0.8392 - val_acc: 0.8290\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.6548 - acc: 0.8275 - val_loss: 0.8268 - val_acc: 0.8280\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6516 - acc: 0.8314 - val_loss: 0.8277 - val_acc: 0.8320\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6553 - acc: 0.8314 - val_loss: 0.8339 - val_acc: 0.8250\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6452 - acc: 0.8289 - val_loss: 0.8342 - val_acc: 0.8280\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6423 - acc: 0.8332 - val_loss: 0.8414 - val_acc: 0.8250\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6348 - acc: 0.8390 - val_loss: 0.8370 - val_acc: 0.8230\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6538 - acc: 0.8291 - val_loss: 0.8370 - val_acc: 0.8270\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.6354 - acc: 0.8306 - val_loss: 0.8382 - val_acc: 0.8300\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.6510 - acc: 0.8353 - val_loss: 0.8320 - val_acc: 0.8280\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6410 - acc: 0.8340 - val_loss: 0.8334 - val_acc: 0.8300\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6381 - acc: 0.8327 - val_loss: 0.8474 - val_acc: 0.8300\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6476 - acc: 0.8305 - val_loss: 0.8427 - val_acc: 0.8320\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6352 - acc: 0.8339 - val_loss: 0.8415 - val_acc: 0.8290\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.6483 - acc: 0.8297 - val_loss: 0.8526 - val_acc: 0.8260\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6364 - acc: 0.8341 - val_loss: 0.8560 - val_acc: 0.8280\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6277 - acc: 0.8365 - val_loss: 0.8512 - val_acc: 0.8320\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6314 - acc: 0.8296 - val_loss: 0.8489 - val_acc: 0.8270\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6395 - acc: 0.8302 - val_loss: 0.8594 - val_acc: 0.8220\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6202 - acc: 0.8383 - val_loss: 0.8546 - val_acc: 0.8280\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.6284 - acc: 0.8345 - val_loss: 0.8503 - val_acc: 0.8260\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.6210 - acc: 0.8345 - val_loss: 0.8446 - val_acc: 0.8260\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6019 - acc: 0.8414 - val_loss: 0.8647 - val_acc: 0.8250\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6353 - acc: 0.8378 - val_loss: 0.8693 - val_acc: 0.8250\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6224 - acc: 0.8346 - val_loss: 0.8692 - val_acc: 0.8290\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6290 - acc: 0.8312 - val_loss: 0.8535 - val_acc: 0.8260\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6172 - acc: 0.8418 - val_loss: 0.8691 - val_acc: 0.8250\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6158 - acc: 0.8369 - val_loss: 0.8719 - val_acc: 0.8210\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6166 - acc: 0.8348 - val_loss: 0.8760 - val_acc: 0.8220\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6127 - acc: 0.8379 - val_loss: 0.8927 - val_acc: 0.8160\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.6066 - acc: 0.8419 - val_loss: 0.8826 - val_acc: 0.8250\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6231 - acc: 0.8358 - val_loss: 0.8659 - val_acc: 0.8240\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6112 - acc: 0.8431 - val_loss: 0.8727 - val_acc: 0.8240\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.5964 - acc: 0.8457 - val_loss: 0.8779 - val_acc: 0.8220\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 4s 542us/step - loss: 2.6525 - acc: 0.4937 - val_loss: 1.7578 - val_acc: 0.6350\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.5511 - acc: 0.6784 - val_loss: 1.3278 - val_acc: 0.7050\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2239 - acc: 0.7390 - val_loss: 1.1785 - val_acc: 0.7530\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.0395 - acc: 0.7721 - val_loss: 1.1038 - val_acc: 0.7660\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9022 - acc: 0.7997 - val_loss: 1.0508 - val_acc: 0.7770\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.7844 - acc: 0.8286 - val_loss: 0.9890 - val_acc: 0.7980\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6917 - acc: 0.8474 - val_loss: 0.9621 - val_acc: 0.8040\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6154 - acc: 0.8690 - val_loss: 0.9256 - val_acc: 0.8110\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.5432 - acc: 0.8819 - val_loss: 0.9060 - val_acc: 0.8050\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4829 - acc: 0.8960 - val_loss: 0.8978 - val_acc: 0.8140\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4438 - acc: 0.9005 - val_loss: 0.8963 - val_acc: 0.8110\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.3923 - acc: 0.9146 - val_loss: 0.8892 - val_acc: 0.8100\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.3709 - acc: 0.9149 - val_loss: 0.8894 - val_acc: 0.8150\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.3436 - acc: 0.9214 - val_loss: 0.9065 - val_acc: 0.8170\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.3224 - acc: 0.9258 - val_loss: 0.9331 - val_acc: 0.8160\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.2936 - acc: 0.9298 - val_loss: 0.9150 - val_acc: 0.8220\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.2825 - acc: 0.9356 - val_loss: 0.9184 - val_acc: 0.8210\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2688 - acc: 0.9372 - val_loss: 0.9168 - val_acc: 0.8240\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.2581 - acc: 0.9371 - val_loss: 0.9281 - val_acc: 0.8210\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2405 - acc: 0.9417 - val_loss: 0.9395 - val_acc: 0.8160\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2348 - acc: 0.9424 - val_loss: 0.9248 - val_acc: 0.8170\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.2276 - acc: 0.9420 - val_loss: 0.9411 - val_acc: 0.8160\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2214 - acc: 0.9460 - val_loss: 0.9269 - val_acc: 0.8200\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2103 - acc: 0.9448 - val_loss: 0.9425 - val_acc: 0.8190\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1995 - acc: 0.9480 - val_loss: 0.9658 - val_acc: 0.8170\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.2013 - acc: 0.9508 - val_loss: 0.9691 - val_acc: 0.8100\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1943 - acc: 0.9490 - val_loss: 0.9549 - val_acc: 0.8210\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1881 - acc: 0.9515 - val_loss: 0.9657 - val_acc: 0.8180\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1832 - acc: 0.9529 - val_loss: 0.9819 - val_acc: 0.8140\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1822 - acc: 0.9520 - val_loss: 0.9826 - val_acc: 0.8130\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1801 - acc: 0.9491 - val_loss: 1.0153 - val_acc: 0.8110\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1808 - acc: 0.9501 - val_loss: 0.9693 - val_acc: 0.8190\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1699 - acc: 0.9559 - val_loss: 0.9981 - val_acc: 0.8100\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1706 - acc: 0.9530 - val_loss: 0.9855 - val_acc: 0.8200\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1792 - acc: 0.9508 - val_loss: 0.9964 - val_acc: 0.8210\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1668 - acc: 0.9516 - val_loss: 1.0275 - val_acc: 0.8130\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1714 - acc: 0.9521 - val_loss: 1.0101 - val_acc: 0.8170\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1484 - acc: 0.9575 - val_loss: 1.0097 - val_acc: 0.8170\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1653 - acc: 0.9513 - val_loss: 1.0018 - val_acc: 0.8110\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1601 - acc: 0.9549 - val_loss: 1.0157 - val_acc: 0.8080\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1617 - acc: 0.9546 - val_loss: 1.0017 - val_acc: 0.8090\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1558 - acc: 0.9535 - val_loss: 1.0340 - val_acc: 0.8060\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1549 - acc: 0.9548 - val_loss: 1.0324 - val_acc: 0.8110\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1580 - acc: 0.9510 - val_loss: 1.0048 - val_acc: 0.8130\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1457 - acc: 0.9560 - val_loss: 1.0333 - val_acc: 0.8080\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1546 - acc: 0.9525 - val_loss: 1.0205 - val_acc: 0.8060\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1482 - acc: 0.9560 - val_loss: 1.0222 - val_acc: 0.8080\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1434 - acc: 0.9568 - val_loss: 1.0527 - val_acc: 0.8040\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1451 - acc: 0.9567 - val_loss: 1.0585 - val_acc: 0.8110\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1402 - acc: 0.9584 - val_loss: 1.0490 - val_acc: 0.8140\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1440 - acc: 0.9574 - val_loss: 1.0610 - val_acc: 0.8090\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1472 - acc: 0.9550 - val_loss: 1.0502 - val_acc: 0.8120\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1421 - acc: 0.9580 - val_loss: 1.0564 - val_acc: 0.8140\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1436 - acc: 0.9569 - val_loss: 1.0675 - val_acc: 0.8100\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1417 - acc: 0.9577 - val_loss: 1.0893 - val_acc: 0.8050\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1337 - acc: 0.9593 - val_loss: 1.1069 - val_acc: 0.8060\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1383 - acc: 0.9579 - val_loss: 1.0627 - val_acc: 0.8110\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1424 - acc: 0.9553 - val_loss: 1.0706 - val_acc: 0.8060\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1395 - acc: 0.9558 - val_loss: 1.0875 - val_acc: 0.8050\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1361 - acc: 0.9602 - val_loss: 1.1120 - val_acc: 0.8040\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1407 - acc: 0.9546 - val_loss: 1.0754 - val_acc: 0.8090\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1413 - acc: 0.9538 - val_loss: 1.0805 - val_acc: 0.8080\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1354 - acc: 0.9569 - val_loss: 1.1229 - val_acc: 0.8090\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1344 - acc: 0.9555 - val_loss: 1.1061 - val_acc: 0.8110\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1286 - acc: 0.9603 - val_loss: 1.1230 - val_acc: 0.8080\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1328 - acc: 0.9578 - val_loss: 1.1316 - val_acc: 0.8020\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1282 - acc: 0.9594 - val_loss: 1.1201 - val_acc: 0.8090\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1310 - acc: 0.9569 - val_loss: 1.1051 - val_acc: 0.8000\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1326 - acc: 0.9572 - val_loss: 1.1217 - val_acc: 0.8070\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1317 - acc: 0.9579 - val_loss: 1.1082 - val_acc: 0.8120\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1248 - acc: 0.9604 - val_loss: 1.1382 - val_acc: 0.8080\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1378 - acc: 0.9534 - val_loss: 1.1245 - val_acc: 0.8090\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1269 - acc: 0.9585 - val_loss: 1.1301 - val_acc: 0.8100\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1339 - acc: 0.9609 - val_loss: 1.1381 - val_acc: 0.8120\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1250 - acc: 0.9578 - val_loss: 1.1311 - val_acc: 0.8120\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1270 - acc: 0.9588 - val_loss: 1.1493 - val_acc: 0.8160\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1284 - acc: 0.9588 - val_loss: 1.1355 - val_acc: 0.8070\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1271 - acc: 0.9582 - val_loss: 1.1425 - val_acc: 0.8070\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1323 - acc: 0.9578 - val_loss: 1.1321 - val_acc: 0.8110\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1314 - acc: 0.9580 - val_loss: 1.1155 - val_acc: 0.8130\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1325 - acc: 0.9593 - val_loss: 1.1267 - val_acc: 0.8100\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1294 - acc: 0.9574 - val_loss: 1.1569 - val_acc: 0.8140\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.1332 - acc: 0.9551 - val_loss: 1.1228 - val_acc: 0.8170\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.1319 - acc: 0.9564 - val_loss: 1.1213 - val_acc: 0.8140\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.1231 - acc: 0.9597 - val_loss: 1.1378 - val_acc: 0.8140\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1275 - acc: 0.9577 - val_loss: 1.1507 - val_acc: 0.8120\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.1231 - acc: 0.9598 - val_loss: 1.1535 - val_acc: 0.8160\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1262 - acc: 0.9583 - val_loss: 1.1465 - val_acc: 0.8170\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1199 - acc: 0.9604 - val_loss: 1.1720 - val_acc: 0.8090\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1238 - acc: 0.9572 - val_loss: 1.1814 - val_acc: 0.8030\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1241 - acc: 0.9582 - val_loss: 1.1791 - val_acc: 0.8090\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1220 - acc: 0.9612 - val_loss: 1.1612 - val_acc: 0.8120\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1259 - acc: 0.9583 - val_loss: 1.1568 - val_acc: 0.8090\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1246 - acc: 0.9577 - val_loss: 1.1584 - val_acc: 0.8090\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1207 - acc: 0.9605 - val_loss: 1.1815 - val_acc: 0.8080\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.1255 - acc: 0.9568 - val_loss: 1.1883 - val_acc: 0.8070\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1288 - acc: 0.9546 - val_loss: 1.1500 - val_acc: 0.8090\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1274 - acc: 0.9548 - val_loss: 1.1852 - val_acc: 0.8100\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1231 - acc: 0.9578 - val_loss: 1.1854 - val_acc: 0.8110\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.1209 - acc: 0.9582 - val_loss: 1.1698 - val_acc: 0.8090\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 4s 534us/step - loss: 3.3726 - acc: 0.4589 - val_loss: 2.3570 - val_acc: 0.5980\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 2.1862 - acc: 0.6186 - val_loss: 1.9052 - val_acc: 0.6620\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.9053 - acc: 0.6681 - val_loss: 1.7268 - val_acc: 0.7000\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.7579 - acc: 0.6929 - val_loss: 1.6414 - val_acc: 0.7190\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.6675 - acc: 0.7115 - val_loss: 1.5687 - val_acc: 0.7450\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.6012 - acc: 0.7236 - val_loss: 1.5244 - val_acc: 0.7470\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.5404 - acc: 0.7352 - val_loss: 1.4799 - val_acc: 0.7550\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.4895 - acc: 0.7447 - val_loss: 1.4537 - val_acc: 0.7520\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.4627 - acc: 0.7476 - val_loss: 1.4183 - val_acc: 0.7660\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4272 - acc: 0.7529 - val_loss: 1.3935 - val_acc: 0.7850\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3957 - acc: 0.7600 - val_loss: 1.3845 - val_acc: 0.7760\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.3792 - acc: 0.7617 - val_loss: 1.3836 - val_acc: 0.7800\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3588 - acc: 0.7642 - val_loss: 1.3653 - val_acc: 0.7850\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3399 - acc: 0.7675 - val_loss: 1.3552 - val_acc: 0.7850\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3115 - acc: 0.7715 - val_loss: 1.3397 - val_acc: 0.7860\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.2959 - acc: 0.7741 - val_loss: 1.3171 - val_acc: 0.7950\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.2799 - acc: 0.7799 - val_loss: 1.3491 - val_acc: 0.7620\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2716 - acc: 0.7820 - val_loss: 1.3157 - val_acc: 0.7820\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.2557 - acc: 0.7878 - val_loss: 1.2957 - val_acc: 0.7900\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2349 - acc: 0.7868 - val_loss: 1.2900 - val_acc: 0.7960\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.2305 - acc: 0.7874 - val_loss: 1.2892 - val_acc: 0.7870\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2034 - acc: 0.7974 - val_loss: 1.2805 - val_acc: 0.7890\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.2093 - acc: 0.7883 - val_loss: 1.2849 - val_acc: 0.7940\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1909 - acc: 0.7978 - val_loss: 1.2761 - val_acc: 0.7890\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1658 - acc: 0.8043 - val_loss: 1.2500 - val_acc: 0.8010\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1690 - acc: 0.7999 - val_loss: 1.2492 - val_acc: 0.8020\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.1621 - acc: 0.8014 - val_loss: 1.2457 - val_acc: 0.8000\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1524 - acc: 0.8016 - val_loss: 1.2520 - val_acc: 0.7990\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1450 - acc: 0.8016 - val_loss: 1.2461 - val_acc: 0.8120\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1368 - acc: 0.8051 - val_loss: 1.2543 - val_acc: 0.7950\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1275 - acc: 0.8106 - val_loss: 1.2252 - val_acc: 0.8030\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1424 - acc: 0.8062 - val_loss: 1.2281 - val_acc: 0.7970\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1207 - acc: 0.8127 - val_loss: 1.2366 - val_acc: 0.7920\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1277 - acc: 0.8067 - val_loss: 1.2229 - val_acc: 0.7980\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1238 - acc: 0.8099 - val_loss: 1.2320 - val_acc: 0.8130\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1042 - acc: 0.8131 - val_loss: 1.2109 - val_acc: 0.8090\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.0895 - acc: 0.8155 - val_loss: 1.2150 - val_acc: 0.8120\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.0889 - acc: 0.8146 - val_loss: 1.2382 - val_acc: 0.7930\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0867 - acc: 0.8138 - val_loss: 1.2130 - val_acc: 0.8070\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.0891 - acc: 0.8155 - val_loss: 1.2057 - val_acc: 0.8050\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.0716 - acc: 0.8183 - val_loss: 1.2189 - val_acc: 0.8040\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0671 - acc: 0.8213 - val_loss: 1.2005 - val_acc: 0.8140\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.0465 - acc: 0.8280 - val_loss: 1.1947 - val_acc: 0.8070\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0676 - acc: 0.8176 - val_loss: 1.2072 - val_acc: 0.8020\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0516 - acc: 0.8247 - val_loss: 1.1960 - val_acc: 0.8080\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.0532 - acc: 0.8241 - val_loss: 1.1940 - val_acc: 0.8050\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.0497 - acc: 0.8196 - val_loss: 1.1980 - val_acc: 0.8070\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0368 - acc: 0.8266 - val_loss: 1.1995 - val_acc: 0.8100\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.0258 - acc: 0.8319 - val_loss: 1.1815 - val_acc: 0.8050\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0207 - acc: 0.8294 - val_loss: 1.1732 - val_acc: 0.8040\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.0273 - acc: 0.8250 - val_loss: 1.1640 - val_acc: 0.8170\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.0287 - acc: 0.8287 - val_loss: 1.1550 - val_acc: 0.8160\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.0089 - acc: 0.8345 - val_loss: 1.1594 - val_acc: 0.8150\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.0237 - acc: 0.8269 - val_loss: 1.1585 - val_acc: 0.8050\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0131 - acc: 0.8285 - val_loss: 1.1569 - val_acc: 0.8150\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.0064 - acc: 0.8330 - val_loss: 1.1707 - val_acc: 0.8060\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9971 - acc: 0.8310 - val_loss: 1.1655 - val_acc: 0.8060\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.0018 - acc: 0.8304 - val_loss: 1.1460 - val_acc: 0.8270\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.0044 - acc: 0.8291 - val_loss: 1.1549 - val_acc: 0.8070\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9942 - acc: 0.8324 - val_loss: 1.1436 - val_acc: 0.8130\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9906 - acc: 0.8319 - val_loss: 1.1475 - val_acc: 0.8130\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.9905 - acc: 0.8317 - val_loss: 1.1342 - val_acc: 0.8210\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9871 - acc: 0.8344 - val_loss: 1.1397 - val_acc: 0.8160\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9733 - acc: 0.8371 - val_loss: 1.1399 - val_acc: 0.8160\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9618 - acc: 0.8421 - val_loss: 1.1395 - val_acc: 0.8170\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9802 - acc: 0.8374 - val_loss: 1.1371 - val_acc: 0.8100\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9737 - acc: 0.8399 - val_loss: 1.1456 - val_acc: 0.8090\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9658 - acc: 0.8376 - val_loss: 1.1524 - val_acc: 0.8100\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.9684 - acc: 0.8381 - val_loss: 1.1283 - val_acc: 0.8160\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9611 - acc: 0.8428 - val_loss: 1.1343 - val_acc: 0.8150\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.9654 - acc: 0.8383 - val_loss: 1.1248 - val_acc: 0.8180\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9586 - acc: 0.8376 - val_loss: 1.1431 - val_acc: 0.8050\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9605 - acc: 0.8398 - val_loss: 1.1397 - val_acc: 0.8080\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9493 - acc: 0.8404 - val_loss: 1.1199 - val_acc: 0.8160\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.9430 - acc: 0.8394 - val_loss: 1.1209 - val_acc: 0.8140\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.9550 - acc: 0.8396 - val_loss: 1.1271 - val_acc: 0.8210\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9465 - acc: 0.8416 - val_loss: 1.1415 - val_acc: 0.8090\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.9366 - acc: 0.8438 - val_loss: 1.1056 - val_acc: 0.8290\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9415 - acc: 0.8425 - val_loss: 1.1262 - val_acc: 0.8170\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9313 - acc: 0.8464 - val_loss: 1.1181 - val_acc: 0.8170\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9235 - acc: 0.8430 - val_loss: 1.1180 - val_acc: 0.8270\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9255 - acc: 0.8423 - val_loss: 1.1306 - val_acc: 0.8120\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9221 - acc: 0.8430 - val_loss: 1.1249 - val_acc: 0.8260\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9276 - acc: 0.8413 - val_loss: 1.1216 - val_acc: 0.8150\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.9203 - acc: 0.8428 - val_loss: 1.1040 - val_acc: 0.8220\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9183 - acc: 0.8447 - val_loss: 1.1198 - val_acc: 0.8110\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9093 - acc: 0.8463 - val_loss: 1.1110 - val_acc: 0.8210\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9096 - acc: 0.8448 - val_loss: 1.1242 - val_acc: 0.8080\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9129 - acc: 0.8494 - val_loss: 1.1446 - val_acc: 0.8080\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9053 - acc: 0.8485 - val_loss: 1.1147 - val_acc: 0.8160\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9156 - acc: 0.8423 - val_loss: 1.1396 - val_acc: 0.8030\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9040 - acc: 0.8457 - val_loss: 1.1125 - val_acc: 0.8200\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.8879 - acc: 0.8545 - val_loss: 1.1251 - val_acc: 0.8130\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9090 - acc: 0.8450 - val_loss: 1.0923 - val_acc: 0.8200\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9059 - acc: 0.8509 - val_loss: 1.1022 - val_acc: 0.8160\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.8952 - acc: 0.8518 - val_loss: 1.1293 - val_acc: 0.8070\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9000 - acc: 0.8470 - val_loss: 1.1095 - val_acc: 0.8100\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.8897 - acc: 0.8537 - val_loss: 1.1029 - val_acc: 0.8120\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8996 - acc: 0.8474 - val_loss: 1.1120 - val_acc: 0.8160\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.8964 - acc: 0.8510 - val_loss: 1.0949 - val_acc: 0.8190\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 4s 556us/step - loss: 3.0502 - acc: 0.3796 - val_loss: 2.1195 - val_acc: 0.5560\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 2.0968 - acc: 0.5378 - val_loss: 1.6362 - val_acc: 0.6420\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.7537 - acc: 0.5992 - val_loss: 1.4516 - val_acc: 0.6730\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.5702 - acc: 0.6406 - val_loss: 1.3516 - val_acc: 0.7010\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4475 - acc: 0.6626 - val_loss: 1.2829 - val_acc: 0.7020\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3607 - acc: 0.6863 - val_loss: 1.2264 - val_acc: 0.7190\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.2962 - acc: 0.6971 - val_loss: 1.1881 - val_acc: 0.7280\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.2468 - acc: 0.7132 - val_loss: 1.1600 - val_acc: 0.7420\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1899 - acc: 0.7167 - val_loss: 1.1302 - val_acc: 0.7440\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1258 - acc: 0.7339 - val_loss: 1.1065 - val_acc: 0.7610\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.0897 - acc: 0.7425 - val_loss: 1.0811 - val_acc: 0.7690\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.0436 - acc: 0.7506 - val_loss: 1.0587 - val_acc: 0.7770\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0062 - acc: 0.7561 - val_loss: 1.0365 - val_acc: 0.7930\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.9811 - acc: 0.7674 - val_loss: 1.0209 - val_acc: 0.7960\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.9385 - acc: 0.7760 - val_loss: 1.0115 - val_acc: 0.7920\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9200 - acc: 0.7771 - val_loss: 0.9984 - val_acc: 0.7960\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.8824 - acc: 0.7892 - val_loss: 0.9944 - val_acc: 0.7940\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.8712 - acc: 0.7878 - val_loss: 0.9833 - val_acc: 0.7970\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.8463 - acc: 0.7964 - val_loss: 0.9739 - val_acc: 0.8070\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.8318 - acc: 0.7983 - val_loss: 0.9717 - val_acc: 0.8080\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.8003 - acc: 0.8054 - val_loss: 0.9745 - val_acc: 0.8040\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7781 - acc: 0.8107 - val_loss: 0.9730 - val_acc: 0.8090\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7503 - acc: 0.8140 - val_loss: 0.9649 - val_acc: 0.8110\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7555 - acc: 0.8180 - val_loss: 0.9619 - val_acc: 0.8140\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7340 - acc: 0.8172 - val_loss: 0.9647 - val_acc: 0.8180\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7189 - acc: 0.8252 - val_loss: 0.9557 - val_acc: 0.8180\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.7054 - acc: 0.8264 - val_loss: 0.9500 - val_acc: 0.8190\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.6963 - acc: 0.8274 - val_loss: 0.9547 - val_acc: 0.8200\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.6613 - acc: 0.8358 - val_loss: 0.9654 - val_acc: 0.8210\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.6730 - acc: 0.8280 - val_loss: 0.9482 - val_acc: 0.8260\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.6520 - acc: 0.8378 - val_loss: 0.9562 - val_acc: 0.8220\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.6464 - acc: 0.8396 - val_loss: 0.9429 - val_acc: 0.8320\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.6246 - acc: 0.8428 - val_loss: 0.9495 - val_acc: 0.8240\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6205 - acc: 0.8414 - val_loss: 0.9463 - val_acc: 0.8260\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.6110 - acc: 0.8455 - val_loss: 0.9545 - val_acc: 0.8300\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6160 - acc: 0.8436 - val_loss: 0.9395 - val_acc: 0.8300\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.5940 - acc: 0.8508 - val_loss: 0.9448 - val_acc: 0.8330\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.5951 - acc: 0.8468 - val_loss: 0.9339 - val_acc: 0.8290\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.5807 - acc: 0.8485 - val_loss: 0.9431 - val_acc: 0.8250\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.5665 - acc: 0.8525 - val_loss: 0.9380 - val_acc: 0.8300\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.5665 - acc: 0.8612 - val_loss: 0.9499 - val_acc: 0.8270\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.5629 - acc: 0.8562 - val_loss: 0.9484 - val_acc: 0.8240\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.5526 - acc: 0.8540 - val_loss: 0.9512 - val_acc: 0.8280\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.5408 - acc: 0.8641 - val_loss: 0.9606 - val_acc: 0.8250\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.5486 - acc: 0.8584 - val_loss: 0.9530 - val_acc: 0.8260\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.5381 - acc: 0.8646 - val_loss: 0.9731 - val_acc: 0.8250\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.5321 - acc: 0.8614 - val_loss: 0.9641 - val_acc: 0.8260\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.5235 - acc: 0.8668 - val_loss: 0.9646 - val_acc: 0.8250\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.5281 - acc: 0.8683 - val_loss: 0.9651 - val_acc: 0.8240\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.5060 - acc: 0.8647 - val_loss: 0.9747 - val_acc: 0.8270\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.5069 - acc: 0.8718 - val_loss: 0.9722 - val_acc: 0.8260\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4932 - acc: 0.8675 - val_loss: 0.9750 - val_acc: 0.8260\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.5057 - acc: 0.8725 - val_loss: 0.9831 - val_acc: 0.8290\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4980 - acc: 0.8723 - val_loss: 0.9789 - val_acc: 0.8270\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.5055 - acc: 0.8661 - val_loss: 0.9788 - val_acc: 0.8340\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4749 - acc: 0.8797 - val_loss: 0.9791 - val_acc: 0.8280\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4907 - acc: 0.8738 - val_loss: 0.9982 - val_acc: 0.8260\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.4782 - acc: 0.8763 - val_loss: 0.9990 - val_acc: 0.8290\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.4603 - acc: 0.8802 - val_loss: 1.0135 - val_acc: 0.8290\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4925 - acc: 0.8761 - val_loss: 0.9945 - val_acc: 0.8270\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.4808 - acc: 0.8770 - val_loss: 0.9906 - val_acc: 0.8280\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4721 - acc: 0.8780 - val_loss: 0.9995 - val_acc: 0.8250\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.4651 - acc: 0.8780 - val_loss: 1.0069 - val_acc: 0.8270\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.4706 - acc: 0.8779 - val_loss: 0.9909 - val_acc: 0.8270\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.4702 - acc: 0.8766 - val_loss: 0.9868 - val_acc: 0.8270\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4454 - acc: 0.8866 - val_loss: 0.9932 - val_acc: 0.8300\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.4644 - acc: 0.8785 - val_loss: 0.9993 - val_acc: 0.8280\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.4764 - acc: 0.8789 - val_loss: 0.9846 - val_acc: 0.8230\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.4619 - acc: 0.8810 - val_loss: 0.9900 - val_acc: 0.8240\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4462 - acc: 0.8864 - val_loss: 1.0045 - val_acc: 0.8270\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4479 - acc: 0.8844 - val_loss: 1.0185 - val_acc: 0.8240\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.4435 - acc: 0.8825 - val_loss: 1.0104 - val_acc: 0.8290\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.4713 - acc: 0.8770 - val_loss: 1.0135 - val_acc: 0.8250\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4474 - acc: 0.8860 - val_loss: 0.9951 - val_acc: 0.8290\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.4459 - acc: 0.8832 - val_loss: 1.0094 - val_acc: 0.8230\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4529 - acc: 0.8832 - val_loss: 1.0107 - val_acc: 0.8230\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.4401 - acc: 0.8869 - val_loss: 0.9982 - val_acc: 0.8250\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.4414 - acc: 0.8844 - val_loss: 1.0196 - val_acc: 0.8240\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.4470 - acc: 0.8861 - val_loss: 1.0145 - val_acc: 0.8250\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.4572 - acc: 0.8844 - val_loss: 0.9979 - val_acc: 0.8260\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.4280 - acc: 0.8893 - val_loss: 1.0280 - val_acc: 0.8270\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.4482 - acc: 0.8807 - val_loss: 1.0297 - val_acc: 0.8250\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4552 - acc: 0.8835 - val_loss: 1.0327 - val_acc: 0.8230\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.4238 - acc: 0.8855 - val_loss: 1.0369 - val_acc: 0.8220\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.4485 - acc: 0.8802 - val_loss: 1.0454 - val_acc: 0.8180\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4487 - acc: 0.8872 - val_loss: 1.0341 - val_acc: 0.8180\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4174 - acc: 0.8903 - val_loss: 1.0498 - val_acc: 0.8200\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4356 - acc: 0.8872 - val_loss: 1.0221 - val_acc: 0.8230\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4477 - acc: 0.8831 - val_loss: 1.0151 - val_acc: 0.8260\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4186 - acc: 0.8911 - val_loss: 1.0244 - val_acc: 0.8260\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4348 - acc: 0.8884 - val_loss: 1.0304 - val_acc: 0.8230\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.4227 - acc: 0.8920 - val_loss: 1.0334 - val_acc: 0.8240\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.4210 - acc: 0.8886 - val_loss: 1.0416 - val_acc: 0.8210\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4143 - acc: 0.8905 - val_loss: 1.0494 - val_acc: 0.8200\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4325 - acc: 0.8889 - val_loss: 1.0314 - val_acc: 0.8210\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4184 - acc: 0.8904 - val_loss: 1.0468 - val_acc: 0.8250\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.4164 - acc: 0.8905 - val_loss: 1.0415 - val_acc: 0.8240\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.4122 - acc: 0.8926 - val_loss: 1.0452 - val_acc: 0.8190\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.4136 - acc: 0.8908 - val_loss: 1.0416 - val_acc: 0.8200\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.4088 - acc: 0.8930 - val_loss: 1.0606 - val_acc: 0.8180\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 5s 578us/step - loss: 2.6484 - acc: 0.5170 - val_loss: 1.7620 - val_acc: 0.6320\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.4702 - acc: 0.6947 - val_loss: 1.3198 - val_acc: 0.7170\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1215 - acc: 0.7547 - val_loss: 1.1427 - val_acc: 0.7500\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.9035 - acc: 0.8032 - val_loss: 1.0708 - val_acc: 0.7690\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7518 - acc: 0.8345 - val_loss: 0.9686 - val_acc: 0.7960\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6217 - acc: 0.8685 - val_loss: 0.9247 - val_acc: 0.8130\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.5308 - acc: 0.8876 - val_loss: 0.8861 - val_acc: 0.8180\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4439 - acc: 0.9050 - val_loss: 0.8841 - val_acc: 0.8150\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.3834 - acc: 0.9169 - val_loss: 0.8622 - val_acc: 0.8210\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.3303 - acc: 0.9301 - val_loss: 0.8457 - val_acc: 0.8240\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.2892 - acc: 0.9340 - val_loss: 0.8570 - val_acc: 0.8220\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2610 - acc: 0.9390 - val_loss: 0.8889 - val_acc: 0.8220\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2234 - acc: 0.9478 - val_loss: 0.9004 - val_acc: 0.8250\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.2083 - acc: 0.9483 - val_loss: 0.9072 - val_acc: 0.8180\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.1955 - acc: 0.9496 - val_loss: 0.9361 - val_acc: 0.8220\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1820 - acc: 0.9528 - val_loss: 0.9539 - val_acc: 0.8130\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1708 - acc: 0.9529 - val_loss: 0.9685 - val_acc: 0.8140\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1600 - acc: 0.9563 - val_loss: 0.9398 - val_acc: 0.8240\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1529 - acc: 0.9559 - val_loss: 0.9812 - val_acc: 0.8150\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1530 - acc: 0.9553 - val_loss: 0.9626 - val_acc: 0.8230\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1420 - acc: 0.9575 - val_loss: 0.9805 - val_acc: 0.8220\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1416 - acc: 0.9578 - val_loss: 1.0232 - val_acc: 0.8120\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1394 - acc: 0.9567 - val_loss: 1.0075 - val_acc: 0.8220\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1339 - acc: 0.9582 - val_loss: 0.9980 - val_acc: 0.8200\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1327 - acc: 0.9560 - val_loss: 1.0514 - val_acc: 0.8180\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1251 - acc: 0.9599 - val_loss: 1.0154 - val_acc: 0.8230\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1221 - acc: 0.9590 - val_loss: 1.0365 - val_acc: 0.8120\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1207 - acc: 0.9565 - val_loss: 1.0733 - val_acc: 0.8060\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1182 - acc: 0.9612 - val_loss: 1.0427 - val_acc: 0.8130\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1224 - acc: 0.9577 - val_loss: 1.0867 - val_acc: 0.8060\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1200 - acc: 0.9592 - val_loss: 1.0550 - val_acc: 0.8080\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.1139 - acc: 0.9588 - val_loss: 1.0602 - val_acc: 0.8170\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.1153 - acc: 0.9589 - val_loss: 1.0689 - val_acc: 0.8100\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1120 - acc: 0.9607 - val_loss: 1.0612 - val_acc: 0.8140\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1114 - acc: 0.9589 - val_loss: 1.0709 - val_acc: 0.8150\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1086 - acc: 0.9604 - val_loss: 1.0774 - val_acc: 0.8120\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1067 - acc: 0.9613 - val_loss: 1.1096 - val_acc: 0.8080\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1115 - acc: 0.9599 - val_loss: 1.0856 - val_acc: 0.8140\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1044 - acc: 0.9608 - val_loss: 1.0998 - val_acc: 0.8140\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1038 - acc: 0.9613 - val_loss: 1.0967 - val_acc: 0.8080\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1063 - acc: 0.9590 - val_loss: 1.1451 - val_acc: 0.8010\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.1018 - acc: 0.9608 - val_loss: 1.0790 - val_acc: 0.8160\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1027 - acc: 0.9595 - val_loss: 1.1178 - val_acc: 0.8070\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1001 - acc: 0.9599 - val_loss: 1.1006 - val_acc: 0.8120\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0972 - acc: 0.9617 - val_loss: 1.1008 - val_acc: 0.8090\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1021 - acc: 0.9605 - val_loss: 1.1118 - val_acc: 0.8110\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1019 - acc: 0.9600 - val_loss: 1.0951 - val_acc: 0.8080\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1011 - acc: 0.9607 - val_loss: 1.1212 - val_acc: 0.8090\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1012 - acc: 0.9590 - val_loss: 1.1358 - val_acc: 0.8090\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.0922 - acc: 0.9608 - val_loss: 1.1165 - val_acc: 0.8210\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0997 - acc: 0.9604 - val_loss: 1.1304 - val_acc: 0.8080\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0937 - acc: 0.9600 - val_loss: 1.1353 - val_acc: 0.8040\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0955 - acc: 0.9618 - val_loss: 1.1555 - val_acc: 0.7980\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.0949 - acc: 0.9595 - val_loss: 1.1334 - val_acc: 0.8080\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.0963 - acc: 0.9614 - val_loss: 1.1529 - val_acc: 0.8000\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0912 - acc: 0.9615 - val_loss: 1.1374 - val_acc: 0.8030\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0904 - acc: 0.9622 - val_loss: 1.1370 - val_acc: 0.8100\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.0913 - acc: 0.9620 - val_loss: 1.1494 - val_acc: 0.8180\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0918 - acc: 0.9614 - val_loss: 1.1799 - val_acc: 0.8030\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.0931 - acc: 0.9615 - val_loss: 1.1272 - val_acc: 0.8080\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0872 - acc: 0.9622 - val_loss: 1.1804 - val_acc: 0.7970\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.0864 - acc: 0.9625 - val_loss: 1.1437 - val_acc: 0.8040\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.0913 - acc: 0.9593 - val_loss: 1.1491 - val_acc: 0.8120\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0895 - acc: 0.9614 - val_loss: 1.1383 - val_acc: 0.8130\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0872 - acc: 0.9604 - val_loss: 1.1753 - val_acc: 0.8070\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.0855 - acc: 0.9627 - val_loss: 1.1796 - val_acc: 0.8050\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.0884 - acc: 0.9599 - val_loss: 1.1498 - val_acc: 0.8060\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0881 - acc: 0.9598 - val_loss: 1.1598 - val_acc: 0.8110\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.0875 - acc: 0.9605 - val_loss: 1.1673 - val_acc: 0.8110\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.0840 - acc: 0.9615 - val_loss: 1.1774 - val_acc: 0.8080\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.0854 - acc: 0.9612 - val_loss: 1.1983 - val_acc: 0.8020\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.0853 - acc: 0.9608 - val_loss: 1.2063 - val_acc: 0.8120\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.0825 - acc: 0.9603 - val_loss: 1.1811 - val_acc: 0.8090\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0859 - acc: 0.9602 - val_loss: 1.2043 - val_acc: 0.8000\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.0801 - acc: 0.9632 - val_loss: 1.1943 - val_acc: 0.8070\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.0858 - acc: 0.9597 - val_loss: 1.1891 - val_acc: 0.8010\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.0798 - acc: 0.9623 - val_loss: 1.1805 - val_acc: 0.8100\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.0849 - acc: 0.9612 - val_loss: 1.2292 - val_acc: 0.7950\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0800 - acc: 0.9620 - val_loss: 1.1920 - val_acc: 0.8060\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.0785 - acc: 0.9632 - val_loss: 1.2333 - val_acc: 0.8020\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0820 - acc: 0.9608 - val_loss: 1.2239 - val_acc: 0.8000\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.0812 - acc: 0.9618 - val_loss: 1.2173 - val_acc: 0.8030\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0829 - acc: 0.9603 - val_loss: 1.2146 - val_acc: 0.8060\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0788 - acc: 0.9625 - val_loss: 1.2275 - val_acc: 0.8030\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.0807 - acc: 0.9622 - val_loss: 1.2293 - val_acc: 0.8030\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0773 - acc: 0.9622 - val_loss: 1.2214 - val_acc: 0.8040\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0808 - acc: 0.9624 - val_loss: 1.2035 - val_acc: 0.8000\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0793 - acc: 0.9624 - val_loss: 1.2115 - val_acc: 0.7950\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0770 - acc: 0.9612 - val_loss: 1.2316 - val_acc: 0.8020\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.0788 - acc: 0.9600 - val_loss: 1.2354 - val_acc: 0.8030\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0786 - acc: 0.9600 - val_loss: 1.2356 - val_acc: 0.8020\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0794 - acc: 0.9628 - val_loss: 1.2217 - val_acc: 0.8040\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0764 - acc: 0.9624 - val_loss: 1.2460 - val_acc: 0.8000\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.0766 - acc: 0.9620 - val_loss: 1.2326 - val_acc: 0.8040\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0793 - acc: 0.9614 - val_loss: 1.2336 - val_acc: 0.8010\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0759 - acc: 0.9603 - val_loss: 1.2655 - val_acc: 0.7980\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0768 - acc: 0.9632 - val_loss: 1.2517 - val_acc: 0.8070\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0797 - acc: 0.9608 - val_loss: 1.2506 - val_acc: 0.7990\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0765 - acc: 0.9624 - val_loss: 1.2589 - val_acc: 0.8010\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0781 - acc: 0.9622 - val_loss: 1.2380 - val_acc: 0.7960\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 5s 581us/step - loss: 4.0009 - acc: 0.2869 - val_loss: 2.8773 - val_acc: 0.5480\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 2.8190 - acc: 0.5004 - val_loss: 2.2574 - val_acc: 0.6000\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 2.3970 - acc: 0.5644 - val_loss: 2.0356 - val_acc: 0.6570\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 2.2155 - acc: 0.5985 - val_loss: 1.9212 - val_acc: 0.6740\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 2.0887 - acc: 0.6188 - val_loss: 1.8325 - val_acc: 0.6850\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.9960 - acc: 0.6346 - val_loss: 1.7641 - val_acc: 0.6900\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.9334 - acc: 0.6428 - val_loss: 1.7146 - val_acc: 0.6960\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.8556 - acc: 0.6553 - val_loss: 1.6727 - val_acc: 0.6920\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.8084 - acc: 0.6619 - val_loss: 1.6360 - val_acc: 0.6970\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.7693 - acc: 0.6668 - val_loss: 1.6064 - val_acc: 0.7010\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.7295 - acc: 0.6733 - val_loss: 1.5777 - val_acc: 0.7010\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6860 - acc: 0.6810 - val_loss: 1.5408 - val_acc: 0.7070\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.6512 - acc: 0.6867 - val_loss: 1.5230 - val_acc: 0.7000\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.6403 - acc: 0.6880 - val_loss: 1.5133 - val_acc: 0.7050\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.6209 - acc: 0.6897 - val_loss: 1.5004 - val_acc: 0.7070\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.5963 - acc: 0.6938 - val_loss: 1.4757 - val_acc: 0.7110\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.5854 - acc: 0.6941 - val_loss: 1.4678 - val_acc: 0.7100\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5798 - acc: 0.6902 - val_loss: 1.4552 - val_acc: 0.7170\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.5550 - acc: 0.6988 - val_loss: 1.4400 - val_acc: 0.7180\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.5390 - acc: 0.6973 - val_loss: 1.4276 - val_acc: 0.7220\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.5291 - acc: 0.6993 - val_loss: 1.4219 - val_acc: 0.7240\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.5185 - acc: 0.7033 - val_loss: 1.4166 - val_acc: 0.7240\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.5093 - acc: 0.7030 - val_loss: 1.4045 - val_acc: 0.7300\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.4894 - acc: 0.7087 - val_loss: 1.4097 - val_acc: 0.7220\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.5031 - acc: 0.7075 - val_loss: 1.3913 - val_acc: 0.7250\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4758 - acc: 0.7086 - val_loss: 1.3899 - val_acc: 0.7330\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.4828 - acc: 0.7105 - val_loss: 1.3712 - val_acc: 0.7280\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.4647 - acc: 0.7106 - val_loss: 1.3671 - val_acc: 0.7340\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4433 - acc: 0.7172 - val_loss: 1.3634 - val_acc: 0.7380\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4566 - acc: 0.7130 - val_loss: 1.3515 - val_acc: 0.7360\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.4348 - acc: 0.7177 - val_loss: 1.3496 - val_acc: 0.7450\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.4369 - acc: 0.7200 - val_loss: 1.3444 - val_acc: 0.7420\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.4417 - acc: 0.7209 - val_loss: 1.3471 - val_acc: 0.7360\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.4212 - acc: 0.7239 - val_loss: 1.3351 - val_acc: 0.7470\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.4247 - acc: 0.7224 - val_loss: 1.3209 - val_acc: 0.7470\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4163 - acc: 0.7235 - val_loss: 1.3245 - val_acc: 0.7510\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4110 - acc: 0.7240 - val_loss: 1.3329 - val_acc: 0.7440\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3958 - acc: 0.7259 - val_loss: 1.3232 - val_acc: 0.7600\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3908 - acc: 0.7328 - val_loss: 1.3143 - val_acc: 0.7470\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.3928 - acc: 0.7290 - val_loss: 1.3074 - val_acc: 0.7590\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3977 - acc: 0.7289 - val_loss: 1.3069 - val_acc: 0.7540\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3859 - acc: 0.7335 - val_loss: 1.3039 - val_acc: 0.7530\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3896 - acc: 0.7306 - val_loss: 1.3009 - val_acc: 0.7540\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3745 - acc: 0.7324 - val_loss: 1.2879 - val_acc: 0.7610\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3704 - acc: 0.7349 - val_loss: 1.2920 - val_acc: 0.7520\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3520 - acc: 0.7407 - val_loss: 1.2915 - val_acc: 0.7550\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3644 - acc: 0.7373 - val_loss: 1.2841 - val_acc: 0.7610\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3659 - acc: 0.7379 - val_loss: 1.2779 - val_acc: 0.7590\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3494 - acc: 0.7388 - val_loss: 1.2875 - val_acc: 0.7600\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3618 - acc: 0.7335 - val_loss: 1.2774 - val_acc: 0.7620\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3412 - acc: 0.7422 - val_loss: 1.2792 - val_acc: 0.7560\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.3452 - acc: 0.7380 - val_loss: 1.2780 - val_acc: 0.7620\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3413 - acc: 0.7397 - val_loss: 1.2728 - val_acc: 0.7690\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.3467 - acc: 0.7409 - val_loss: 1.2688 - val_acc: 0.7600\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3497 - acc: 0.7378 - val_loss: 1.2698 - val_acc: 0.7610\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3305 - acc: 0.7448 - val_loss: 1.2594 - val_acc: 0.7670\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3377 - acc: 0.7378 - val_loss: 1.2611 - val_acc: 0.7600\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3210 - acc: 0.7433 - val_loss: 1.2606 - val_acc: 0.7570\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3361 - acc: 0.7407 - val_loss: 1.2612 - val_acc: 0.7640\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3355 - acc: 0.7423 - val_loss: 1.2519 - val_acc: 0.7580\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.3153 - acc: 0.7487 - val_loss: 1.2523 - val_acc: 0.7570\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.3245 - acc: 0.7399 - val_loss: 1.2501 - val_acc: 0.7650\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3213 - acc: 0.7420 - val_loss: 1.2580 - val_acc: 0.7570\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3106 - acc: 0.7451 - val_loss: 1.2477 - val_acc: 0.7670\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3143 - acc: 0.7497 - val_loss: 1.2431 - val_acc: 0.7600\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3047 - acc: 0.7466 - val_loss: 1.2357 - val_acc: 0.7610\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3060 - acc: 0.7438 - val_loss: 1.2319 - val_acc: 0.7670\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3087 - acc: 0.7481 - val_loss: 1.2357 - val_acc: 0.7650\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3053 - acc: 0.7471 - val_loss: 1.2366 - val_acc: 0.7680\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3033 - acc: 0.7448 - val_loss: 1.2253 - val_acc: 0.7650\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.2991 - acc: 0.7456 - val_loss: 1.2449 - val_acc: 0.7620\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3001 - acc: 0.7466 - val_loss: 1.2314 - val_acc: 0.7640\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2904 - acc: 0.7446 - val_loss: 1.2334 - val_acc: 0.7720\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.2815 - acc: 0.7508 - val_loss: 1.2358 - val_acc: 0.7620\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3008 - acc: 0.7437 - val_loss: 1.2451 - val_acc: 0.7530\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2748 - acc: 0.7507 - val_loss: 1.2225 - val_acc: 0.7660\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2884 - acc: 0.7506 - val_loss: 1.2198 - val_acc: 0.7700\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2849 - acc: 0.7479 - val_loss: 1.2288 - val_acc: 0.7620\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2898 - acc: 0.7511 - val_loss: 1.2204 - val_acc: 0.7620\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2715 - acc: 0.7521 - val_loss: 1.2271 - val_acc: 0.7690\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2704 - acc: 0.7563 - val_loss: 1.2184 - val_acc: 0.7700\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.2726 - acc: 0.7522 - val_loss: 1.2284 - val_acc: 0.7690\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2689 - acc: 0.7529 - val_loss: 1.2196 - val_acc: 0.7660\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2597 - acc: 0.7524 - val_loss: 1.2206 - val_acc: 0.7750\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2548 - acc: 0.7527 - val_loss: 1.2237 - val_acc: 0.7690\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2541 - acc: 0.7538 - val_loss: 1.2195 - val_acc: 0.7660\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2539 - acc: 0.7567 - val_loss: 1.2328 - val_acc: 0.7620\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2677 - acc: 0.7513 - val_loss: 1.2102 - val_acc: 0.7720\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.2692 - acc: 0.7496 - val_loss: 1.2115 - val_acc: 0.7740\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2580 - acc: 0.7554 - val_loss: 1.2011 - val_acc: 0.7780\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.2718 - acc: 0.7492 - val_loss: 1.2070 - val_acc: 0.7770\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2586 - acc: 0.7551 - val_loss: 1.2024 - val_acc: 0.7720\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.2584 - acc: 0.7549 - val_loss: 1.2083 - val_acc: 0.7690\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2582 - acc: 0.7526 - val_loss: 1.2058 - val_acc: 0.7740\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2402 - acc: 0.7573 - val_loss: 1.2276 - val_acc: 0.7550\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2284 - acc: 0.7600 - val_loss: 1.1977 - val_acc: 0.7730\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2378 - acc: 0.7578 - val_loss: 1.2005 - val_acc: 0.7770\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.2464 - acc: 0.7544 - val_loss: 1.1924 - val_acc: 0.7770\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2381 - acc: 0.7563 - val_loss: 1.2021 - val_acc: 0.7710\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.2352 - acc: 0.7566 - val_loss: 1.1930 - val_acc: 0.7780\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 5s 585us/step - loss: 3.1760 - acc: 0.3251 - val_loss: 2.2886 - val_acc: 0.5350\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 2.3491 - acc: 0.4863 - val_loss: 1.8019 - val_acc: 0.5790\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 2.0633 - acc: 0.5347 - val_loss: 1.6406 - val_acc: 0.6230\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.9383 - acc: 0.5579 - val_loss: 1.5540 - val_acc: 0.6610\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.8383 - acc: 0.5653 - val_loss: 1.4920 - val_acc: 0.6820\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.7588 - acc: 0.5888 - val_loss: 1.4320 - val_acc: 0.6910\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.7068 - acc: 0.6106 - val_loss: 1.3849 - val_acc: 0.6940\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.6413 - acc: 0.6189 - val_loss: 1.3467 - val_acc: 0.7000\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.6156 - acc: 0.6284 - val_loss: 1.3089 - val_acc: 0.7060\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.5624 - acc: 0.6386 - val_loss: 1.2874 - val_acc: 0.7090\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.5705 - acc: 0.6403 - val_loss: 1.2635 - val_acc: 0.7110\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.5140 - acc: 0.6439 - val_loss: 1.2383 - val_acc: 0.7210\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.5080 - acc: 0.6488 - val_loss: 1.2248 - val_acc: 0.7320\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.4814 - acc: 0.6580 - val_loss: 1.2037 - val_acc: 0.7340\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4655 - acc: 0.6582 - val_loss: 1.1846 - val_acc: 0.7430\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4391 - acc: 0.6673 - val_loss: 1.1695 - val_acc: 0.7390\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4207 - acc: 0.6689 - val_loss: 1.1512 - val_acc: 0.7470\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4069 - acc: 0.6698 - val_loss: 1.1458 - val_acc: 0.7540\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3971 - acc: 0.6780 - val_loss: 1.1341 - val_acc: 0.7560\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3724 - acc: 0.6805 - val_loss: 1.1165 - val_acc: 0.7550\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3810 - acc: 0.6815 - val_loss: 1.1141 - val_acc: 0.7630\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.3616 - acc: 0.6879 - val_loss: 1.0992 - val_acc: 0.7640\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3315 - acc: 0.6849 - val_loss: 1.0885 - val_acc: 0.7640\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3260 - acc: 0.6888 - val_loss: 1.0792 - val_acc: 0.7730\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.3225 - acc: 0.6924 - val_loss: 1.0715 - val_acc: 0.7720\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3092 - acc: 0.6984 - val_loss: 1.0568 - val_acc: 0.7700\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3118 - acc: 0.6958 - val_loss: 1.0525 - val_acc: 0.7730\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2973 - acc: 0.6989 - val_loss: 1.0445 - val_acc: 0.7700\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3005 - acc: 0.7005 - val_loss: 1.0409 - val_acc: 0.7780\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.2968 - acc: 0.6993 - val_loss: 1.0383 - val_acc: 0.7780\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.2717 - acc: 0.7037 - val_loss: 1.0261 - val_acc: 0.7790\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2599 - acc: 0.7083 - val_loss: 1.0210 - val_acc: 0.7780\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.2919 - acc: 0.6946 - val_loss: 1.0261 - val_acc: 0.7820\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2586 - acc: 0.7046 - val_loss: 1.0159 - val_acc: 0.7810\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2608 - acc: 0.7027 - val_loss: 1.0121 - val_acc: 0.7830\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2515 - acc: 0.7073 - val_loss: 1.0100 - val_acc: 0.7820\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2397 - acc: 0.7102 - val_loss: 0.9977 - val_acc: 0.7900\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2400 - acc: 0.7090 - val_loss: 0.9941 - val_acc: 0.7880\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2458 - acc: 0.7110 - val_loss: 0.9889 - val_acc: 0.7920\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2332 - acc: 0.7142 - val_loss: 0.9855 - val_acc: 0.7960\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2128 - acc: 0.7167 - val_loss: 0.9811 - val_acc: 0.7940\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.2171 - acc: 0.7126 - val_loss: 0.9782 - val_acc: 0.8010\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.2255 - acc: 0.7120 - val_loss: 0.9712 - val_acc: 0.7910\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.2165 - acc: 0.7147 - val_loss: 0.9702 - val_acc: 0.7930\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2187 - acc: 0.7103 - val_loss: 0.9761 - val_acc: 0.7930\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.2026 - acc: 0.7201 - val_loss: 0.9659 - val_acc: 0.7980\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2043 - acc: 0.7167 - val_loss: 0.9632 - val_acc: 0.7960\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1740 - acc: 0.7195 - val_loss: 0.9591 - val_acc: 0.7980\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.2107 - acc: 0.7164 - val_loss: 0.9574 - val_acc: 0.7960\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.2017 - acc: 0.7165 - val_loss: 0.9628 - val_acc: 0.7920\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1992 - acc: 0.7195 - val_loss: 0.9587 - val_acc: 0.7940\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1805 - acc: 0.7225 - val_loss: 0.9516 - val_acc: 0.7980\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1947 - acc: 0.7191 - val_loss: 0.9479 - val_acc: 0.8010\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1864 - acc: 0.7197 - val_loss: 0.9452 - val_acc: 0.7990\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1836 - acc: 0.7248 - val_loss: 0.9464 - val_acc: 0.7960\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1716 - acc: 0.7209 - val_loss: 0.9445 - val_acc: 0.7900\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1714 - acc: 0.7201 - val_loss: 0.9431 - val_acc: 0.7970\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1741 - acc: 0.7212 - val_loss: 0.9403 - val_acc: 0.7990\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1746 - acc: 0.7258 - val_loss: 0.9315 - val_acc: 0.8050\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1701 - acc: 0.7244 - val_loss: 0.9336 - val_acc: 0.7960\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1474 - acc: 0.7348 - val_loss: 0.9309 - val_acc: 0.7980\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1784 - acc: 0.7223 - val_loss: 0.9299 - val_acc: 0.7990\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1742 - acc: 0.7192 - val_loss: 0.9271 - val_acc: 0.8020\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1708 - acc: 0.7254 - val_loss: 0.9324 - val_acc: 0.7960\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1575 - acc: 0.7253 - val_loss: 0.9296 - val_acc: 0.8000\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1609 - acc: 0.7238 - val_loss: 0.9257 - val_acc: 0.8030\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1635 - acc: 0.7225 - val_loss: 0.9187 - val_acc: 0.8050\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1392 - acc: 0.7319 - val_loss: 0.9191 - val_acc: 0.8030\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1528 - acc: 0.7279 - val_loss: 0.9221 - val_acc: 0.8000\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.1443 - acc: 0.7270 - val_loss: 0.9152 - val_acc: 0.8080\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.1490 - acc: 0.7249 - val_loss: 0.9116 - val_acc: 0.8070\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1577 - acc: 0.7250 - val_loss: 0.9142 - val_acc: 0.8080\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1522 - acc: 0.7238 - val_loss: 0.9110 - val_acc: 0.8080\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1480 - acc: 0.7270 - val_loss: 0.9087 - val_acc: 0.8040\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1532 - acc: 0.7269 - val_loss: 0.9047 - val_acc: 0.8050\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1406 - acc: 0.7305 - val_loss: 0.9047 - val_acc: 0.8030\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1410 - acc: 0.7296 - val_loss: 0.9052 - val_acc: 0.8070\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1483 - acc: 0.7281 - val_loss: 0.9017 - val_acc: 0.8110\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1424 - acc: 0.7284 - val_loss: 0.9018 - val_acc: 0.8030\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1266 - acc: 0.7310 - val_loss: 0.8974 - val_acc: 0.8110\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1406 - acc: 0.7291 - val_loss: 0.8988 - val_acc: 0.8100\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1210 - acc: 0.7271 - val_loss: 0.9002 - val_acc: 0.8040\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.1330 - acc: 0.7270 - val_loss: 0.8978 - val_acc: 0.8070\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.1259 - acc: 0.7288 - val_loss: 0.8988 - val_acc: 0.8070\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1441 - acc: 0.7249 - val_loss: 0.9015 - val_acc: 0.8040\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1306 - acc: 0.7299 - val_loss: 0.8942 - val_acc: 0.8060\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1355 - acc: 0.7319 - val_loss: 0.8954 - val_acc: 0.8030\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.1397 - acc: 0.7303 - val_loss: 0.8956 - val_acc: 0.8050\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1299 - acc: 0.7328 - val_loss: 0.8876 - val_acc: 0.8100\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1306 - acc: 0.7359 - val_loss: 0.8879 - val_acc: 0.8030\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1235 - acc: 0.7355 - val_loss: 0.8862 - val_acc: 0.8120\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1188 - acc: 0.7319 - val_loss: 0.8858 - val_acc: 0.8100\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1279 - acc: 0.7261 - val_loss: 0.8825 - val_acc: 0.8150\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1345 - acc: 0.7294 - val_loss: 0.8873 - val_acc: 0.8080\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.0984 - acc: 0.7335 - val_loss: 0.8829 - val_acc: 0.8050\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1239 - acc: 0.7261 - val_loss: 0.8829 - val_acc: 0.8070\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1086 - acc: 0.7394 - val_loss: 0.8831 - val_acc: 0.8100\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1285 - acc: 0.7313 - val_loss: 0.8814 - val_acc: 0.8080\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1154 - acc: 0.7280 - val_loss: 0.8804 - val_acc: 0.8090\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1080 - acc: 0.7340 - val_loss: 0.8767 - val_acc: 0.8080\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG1TuzcgwJRh",
        "colab_type": "text"
      },
      "source": [
        "#### 3. Make scatter plots showing how validation accuracy varies with the hyperparameter values.\n",
        "\n",
        "You should make three scatter plots: one showing val_acc vs input_dropout_rates; a second showing val_acc vs hidden_dropout_rates; and a third showing val_acc vs l2_penalties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjYsEWD1n5QZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "outputId": "ca8e2cc2-892d-46b5-c183-773fe9325fbc"
      },
      "source": [
        "plt.scatter(input_dropout_rates, val_acc)\n",
        "plt.show()\n",
        "plt.scatter(hidden_dropout_rates, val_acc)\n",
        "plt.show()\n",
        "plt.scatter(l2_penalties, val_acc)\n",
        "plt.show()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB2dbglEodTu7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTJP1yoJwn2I",
        "colab_type": "text"
      },
      "source": [
        "#### 4. Based on the scatter plots in part 3, which hyperparameter settings seem best?  Please pick a specific number for each hyperparameter.  We don't know the exact best values, but we can make a good enough determination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWUmZhPfw39N",
        "colab_type": "text"
      },
      "source": [
        "(put your answer here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aaEpqD-w6vL",
        "colab_type": "text"
      },
      "source": [
        "#### 5. Suppose we had used a grid search with 8 combinations of parameter settings instead of the random search procedure with 8 combinations.  How many unique values would we have been able to try for each of the three hyperparameters?  Would the results have given us as good of a view of how model performance depended on each model parameter as what we saw here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K60ZZAIxWHN",
        "colab_type": "text"
      },
      "source": [
        "(put your answer here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNQIUuV2YEuY",
        "colab_type": "text"
      },
      "source": [
        "## Refit models to the combined training and validation set data\n",
        "When we are making test set predictions, we want to do so with a model that was fit using as much data as possible.\n",
        "\n",
        "### Chollet's Model\n",
        "\n",
        "You don't need to do anything here other than run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf4WT59OYMyc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "18783a58-2c2c-43a7-c060-59c77bb48b04"
      },
      "source": [
        "history = chollet_model.fit(x_train_and_val,\n",
        "                    y_train_and_val,\n",
        "                    epochs=20,\n",
        "                    batch_size=512)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.2128 - acc: 0.9418\n",
            "Epoch 2/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.1675 - acc: 0.9481\n",
            "Epoch 3/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1514 - acc: 0.9506\n",
            "Epoch 4/20\n",
            "8982/8982 [==============================] - 0s 45us/step - loss: 0.1317 - acc: 0.9520\n",
            "Epoch 5/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1276 - acc: 0.9538\n",
            "Epoch 6/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.1217 - acc: 0.9537\n",
            "Epoch 7/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1178 - acc: 0.9552\n",
            "Epoch 8/20\n",
            "8982/8982 [==============================] - 0s 48us/step - loss: 0.1141 - acc: 0.9552\n",
            "Epoch 9/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.1145 - acc: 0.9544\n",
            "Epoch 10/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1115 - acc: 0.9557\n",
            "Epoch 11/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1095 - acc: 0.9547\n",
            "Epoch 12/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.1079 - acc: 0.9561\n",
            "Epoch 13/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.1042 - acc: 0.9561\n",
            "Epoch 14/20\n",
            "8982/8982 [==============================] - 0s 45us/step - loss: 0.1073 - acc: 0.9534\n",
            "Epoch 15/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1022 - acc: 0.9544\n",
            "Epoch 16/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.1021 - acc: 0.9550\n",
            "Epoch 17/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.1032 - acc: 0.9555\n",
            "Epoch 18/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1004 - acc: 0.9562\n",
            "Epoch 19/20\n",
            "8982/8982 [==============================] - 0s 45us/step - loss: 0.1004 - acc: 0.9532\n",
            "Epoch 20/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1022 - acc: 0.9559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7sruJY_Zmeq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "030bb0c8-714e-4c9f-c58e-82bd2f3b2193"
      },
      "source": [
        "chollet_model.evaluate(x_test, y_test)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2246/2246 [==============================] - 0s 80us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3978797875656577, 0.783615316064466]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCQ4YhOjxiz-",
        "colab_type": "text"
      },
      "source": [
        "#### 6. Refit your final model to the combined training and validation set data\n",
        "\n",
        "You will need to redefine and refit your model using the hyperparameter settings you selected in part 4.  You should be fitting to x_train_and_val and y_train_and_val using rmsprop with 100 epochs and a batch size of 512."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsbUROQ02N5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9ec4880-d695-4e79-e218-a7854f143427"
      },
      "source": [
        "final_model = models.Sequential()\n",
        "\n",
        "final_model.add(layers.Dropout(rate = 0.75))\n",
        "final_model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "final_model.add(layers.Dropout(rate = 0.3))\n",
        "final_model.add(layers.Dense(64, activation='relu'))\n",
        "final_model.add(layers.Dropout(rate = 0.3))\n",
        "final_model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "final_model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = final_model.fit(x_train_and_val,\n",
        "                    y_train_and_val,\n",
        "                    epochs=100,\n",
        "                    batch_size=512)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8982/8982 [==============================] - 4s 486us/step - loss: 2.9893 - acc: 0.3831\n",
            "Epoch 2/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 2.1302 - acc: 0.5156\n",
            "Epoch 3/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 1.8566 - acc: 0.5627\n",
            "Epoch 4/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 1.7146 - acc: 0.6087\n",
            "Epoch 5/100\n",
            "8982/8982 [==============================] - 1s 62us/step - loss: 1.6050 - acc: 0.6278\n",
            "Epoch 6/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 1.5151 - acc: 0.6466\n",
            "Epoch 7/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 1.4704 - acc: 0.6562\n",
            "Epoch 8/100\n",
            "8982/8982 [==============================] - 1s 62us/step - loss: 1.4134 - acc: 0.6706\n",
            "Epoch 9/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 1.3546 - acc: 0.6820\n",
            "Epoch 10/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 1.3184 - acc: 0.6888\n",
            "Epoch 11/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 1.2658 - acc: 0.6976\n",
            "Epoch 12/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 1.2364 - acc: 0.7065\n",
            "Epoch 13/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 1.2213 - acc: 0.7070\n",
            "Epoch 14/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 1.1895 - acc: 0.7192\n",
            "Epoch 15/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 1.1829 - acc: 0.7147\n",
            "Epoch 16/100\n",
            "8982/8982 [==============================] - 1s 62us/step - loss: 1.1265 - acc: 0.7320\n",
            "Epoch 17/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 1.1276 - acc: 0.7280\n",
            "Epoch 18/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 1.1164 - acc: 0.7320\n",
            "Epoch 19/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 1.0726 - acc: 0.7356\n",
            "Epoch 20/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 1.0711 - acc: 0.7359\n",
            "Epoch 21/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 1.0623 - acc: 0.7419\n",
            "Epoch 22/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 1.0301 - acc: 0.7488\n",
            "Epoch 23/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 1.0177 - acc: 0.7546\n",
            "Epoch 24/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.9955 - acc: 0.7571\n",
            "Epoch 25/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.9904 - acc: 0.7561\n",
            "Epoch 26/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.9784 - acc: 0.7537\n",
            "Epoch 27/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.9462 - acc: 0.7653\n",
            "Epoch 28/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.9629 - acc: 0.7574\n",
            "Epoch 29/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.9225 - acc: 0.7659\n",
            "Epoch 30/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.9300 - acc: 0.7730\n",
            "Epoch 31/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.9273 - acc: 0.7700\n",
            "Epoch 32/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.8959 - acc: 0.7766\n",
            "Epoch 33/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.9078 - acc: 0.7768\n",
            "Epoch 34/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.8846 - acc: 0.7817\n",
            "Epoch 35/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.8789 - acc: 0.7773\n",
            "Epoch 36/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.8749 - acc: 0.7794\n",
            "Epoch 37/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.8657 - acc: 0.7796\n",
            "Epoch 38/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.8721 - acc: 0.7787\n",
            "Epoch 39/100\n",
            "8982/8982 [==============================] - 1s 63us/step - loss: 0.8532 - acc: 0.7890\n",
            "Epoch 40/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.8442 - acc: 0.7861\n",
            "Epoch 41/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.8476 - acc: 0.7852\n",
            "Epoch 42/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.8408 - acc: 0.7901\n",
            "Epoch 43/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.8297 - acc: 0.7926\n",
            "Epoch 44/100\n",
            "8982/8982 [==============================] - 1s 62us/step - loss: 0.8196 - acc: 0.7961\n",
            "Epoch 45/100\n",
            "8982/8982 [==============================] - 1s 63us/step - loss: 0.8227 - acc: 0.7904\n",
            "Epoch 46/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.7940 - acc: 0.7965\n",
            "Epoch 47/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.8042 - acc: 0.7988\n",
            "Epoch 48/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.8036 - acc: 0.7956\n",
            "Epoch 49/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.7919 - acc: 0.7974\n",
            "Epoch 50/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.7913 - acc: 0.8043\n",
            "Epoch 51/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.7848 - acc: 0.8014\n",
            "Epoch 52/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.7824 - acc: 0.8003\n",
            "Epoch 53/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.7814 - acc: 0.7993\n",
            "Epoch 54/100\n",
            "8982/8982 [==============================] - 1s 63us/step - loss: 0.7679 - acc: 0.7989\n",
            "Epoch 55/100\n",
            "8982/8982 [==============================] - 1s 62us/step - loss: 0.7659 - acc: 0.8025\n",
            "Epoch 56/100\n",
            "8982/8982 [==============================] - 1s 63us/step - loss: 0.7510 - acc: 0.8069\n",
            "Epoch 57/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.7677 - acc: 0.8045\n",
            "Epoch 58/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.7392 - acc: 0.8100\n",
            "Epoch 59/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.7332 - acc: 0.8098\n",
            "Epoch 60/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.7349 - acc: 0.8108\n",
            "Epoch 61/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.7378 - acc: 0.8094\n",
            "Epoch 62/100\n",
            "8982/8982 [==============================] - 1s 62us/step - loss: 0.7313 - acc: 0.8084\n",
            "Epoch 63/100\n",
            "8982/8982 [==============================] - 1s 62us/step - loss: 0.7327 - acc: 0.8100\n",
            "Epoch 64/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.7272 - acc: 0.8131\n",
            "Epoch 65/100\n",
            "8982/8982 [==============================] - 1s 62us/step - loss: 0.7099 - acc: 0.8112\n",
            "Epoch 66/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.7319 - acc: 0.8066\n",
            "Epoch 67/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.7179 - acc: 0.8131\n",
            "Epoch 68/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.7336 - acc: 0.8103\n",
            "Epoch 69/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.7180 - acc: 0.8145\n",
            "Epoch 70/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.7142 - acc: 0.8173\n",
            "Epoch 71/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.7080 - acc: 0.8220\n",
            "Epoch 72/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.7059 - acc: 0.8145\n",
            "Epoch 73/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.7031 - acc: 0.8143\n",
            "Epoch 74/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6869 - acc: 0.8159\n",
            "Epoch 75/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.7019 - acc: 0.8175\n",
            "Epoch 76/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.6992 - acc: 0.8170\n",
            "Epoch 77/100\n",
            "8982/8982 [==============================] - 1s 63us/step - loss: 0.6774 - acc: 0.8249\n",
            "Epoch 78/100\n",
            "8982/8982 [==============================] - 1s 62us/step - loss: 0.6732 - acc: 0.8218\n",
            "Epoch 79/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.6712 - acc: 0.8250\n",
            "Epoch 80/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6724 - acc: 0.8220\n",
            "Epoch 81/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6845 - acc: 0.8212\n",
            "Epoch 82/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6740 - acc: 0.8219\n",
            "Epoch 83/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6800 - acc: 0.8218\n",
            "Epoch 84/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6868 - acc: 0.8212\n",
            "Epoch 85/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.6893 - acc: 0.8230\n",
            "Epoch 86/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.6504 - acc: 0.8253\n",
            "Epoch 87/100\n",
            "8982/8982 [==============================] - 1s 63us/step - loss: 0.6570 - acc: 0.8234\n",
            "Epoch 88/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.6544 - acc: 0.8284\n",
            "Epoch 89/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.6520 - acc: 0.8309\n",
            "Epoch 90/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.6623 - acc: 0.8281\n",
            "Epoch 91/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.6484 - acc: 0.8304\n",
            "Epoch 92/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.6456 - acc: 0.8307\n",
            "Epoch 93/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.6399 - acc: 0.8311\n",
            "Epoch 94/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6629 - acc: 0.8192\n",
            "Epoch 95/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.6567 - acc: 0.8268\n",
            "Epoch 96/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6508 - acc: 0.8273\n",
            "Epoch 97/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6544 - acc: 0.8267\n",
            "Epoch 98/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.6384 - acc: 0.8288\n",
            "Epoch 99/100\n",
            "8982/8982 [==============================] - 1s 63us/step - loss: 0.6445 - acc: 0.8282\n",
            "Epoch 100/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.6230 - acc: 0.8323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeNbclNmyqy3",
        "colab_type": "text"
      },
      "source": [
        "#### 7. Find the test set performance of your final model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WucnaK6g2pAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ef6320f3-01d1-4ccd-e8fa-481319525a34"
      },
      "source": [
        "final_model.evaluate(x_test, y_test)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2246/2246 [==============================] - 2s 824us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9153138020159619, 0.811665182599826]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCBSumHDyu0Z",
        "colab_type": "text"
      },
      "source": [
        "#### 8. Discuss the relative performance of Chollet's model and your final model on the combined training/validation set and on the test set.  Does it seem like you have addressed the problem of overfitting the training data?  Which model is better?  (You need at least one sentence for each of these questions.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsTeGEQezAWj",
        "colab_type": "text"
      },
      "source": [
        "(put your answer here)"
      ]
    }
  ]
}