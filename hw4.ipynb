{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSYV3U4JZdzt",
        "colab_type": "text"
      },
      "source": [
        "## Goals\n",
        "\n",
        "The goal of the coding part of this homework assignment is to practice the process of tuning hyperparameters to find a good neural network model.\n",
        "\n",
        "We will work with the same Reuters newswires data set from the last homework.  In that assignment, we accepted the model that Chollet used in the book.  Here we will see if we can improve that model at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO0Kphe9bPJk",
        "colab_type": "text"
      },
      "source": [
        "## Module Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C74M21bXSNw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "02a55b4c-db0c-48e2-8b24-dbc6fe5e4468"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras.datasets import reuters\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "import numpy as np\n",
        "import copy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzsvi3njSYZO",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdlIkalQXS6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5b0defa9-4288-42d0-d304-ed79e598718d"
      },
      "source": [
        "(train_and_val_data, train_and_val_labels), (test_data, test_labels) = reuters.load_data(\n",
        "    num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "x_train_and_val = vectorize_sequences(train_and_val_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "def to_one_hot(labels, dimension=46):\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "one_hot_train_and_val_labels = to_one_hot(train_and_val_labels)\n",
        "one_hot_test_labels = to_one_hot(test_labels)\n",
        "y_test = one_hot_test_labels\n",
        "\n",
        "x_val = x_train_and_val[:1000]\n",
        "x_train = x_train_and_val[1000:]\n",
        "\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "y_train = one_hot_train_labels[1000:]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhrYx59ySdc_",
        "colab_type": "text"
      },
      "source": [
        "## Chollet's Model\n",
        "You don't have to do anything with the code below.  This is our baseline model, the one from the book."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RETrtWsUXYFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chollet_model = models.Sequential()\n",
        "chollet_model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "chollet_model.add(layers.Dense(64, activation='relu'))\n",
        "chollet_model.add(layers.Dense(46, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WXMSq-hXhKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "ec606835-9cc9-43b7-c3e5-51b876994099"
      },
      "source": [
        "chollet_model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = chollet_model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 2.5443 - acc: 0.5405 - val_loss: 1.6874 - val_acc: 0.6520\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.3742 - acc: 0.7093 - val_loss: 1.2770 - val_acc: 0.7180\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0264 - acc: 0.7757 - val_loss: 1.1146 - val_acc: 0.7500\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8105 - acc: 0.8267 - val_loss: 1.0324 - val_acc: 0.7810\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.6437 - acc: 0.8613 - val_loss: 0.9676 - val_acc: 0.7920\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.5149 - acc: 0.8887 - val_loss: 0.9308 - val_acc: 0.8110\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.4186 - acc: 0.9085 - val_loss: 0.9275 - val_acc: 0.8150\n",
            "Epoch 8/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3375 - acc: 0.9268 - val_loss: 0.9122 - val_acc: 0.8200\n",
            "Epoch 9/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2855 - acc: 0.9390 - val_loss: 0.9163 - val_acc: 0.8170\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2390 - acc: 0.9449 - val_loss: 0.9369 - val_acc: 0.8100\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2056 - acc: 0.9511 - val_loss: 0.9143 - val_acc: 0.8190\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1834 - acc: 0.9518 - val_loss: 0.9661 - val_acc: 0.8080\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1619 - acc: 0.9523 - val_loss: 0.9527 - val_acc: 0.8120\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.1482 - acc: 0.9549 - val_loss: 0.9962 - val_acc: 0.8090\n",
            "Epoch 15/20\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1404 - acc: 0.9540 - val_loss: 1.0321 - val_acc: 0.7970\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1303 - acc: 0.9560 - val_loss: 1.0349 - val_acc: 0.7990\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1213 - acc: 0.9580 - val_loss: 1.0461 - val_acc: 0.8020\n",
            "Epoch 18/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.1212 - acc: 0.9565 - val_loss: 1.0610 - val_acc: 0.8020\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1110 - acc: 0.9575 - val_loss: 1.0975 - val_acc: 0.8010\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.1132 - acc: 0.9568 - val_loss: 1.1298 - val_acc: 0.7960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI-FebYPXpKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "32611c4f-d58e-44e6-e4f7-7a823904b787"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "\n",
        "plt.plot(epochs, acc, label='Training acc')\n",
        "plt.plot(epochs, val_acc, label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j_fcs-bSqFi",
        "colab_type": "text"
      },
      "source": [
        "Note that validation set accuracy of this model ended up at about 0.8.  We'll try to find a combination of hyperparameters with better validation set accuracy, and then refit both our final model and Chollet's model to the full combined training and validation set data and evaluate their performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-NkEJIxZFs2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c758f84-4c8d-4755-ccdf-21338c3af6b2"
      },
      "source": [
        "np.random.seed(6473)\n",
        "dropout_rates = np.random.uniform(low = 0.0, high = 0.5, size = (8,))\n",
        "l2_penalties_exp = np.random.uniform(low = -6, high = -1, size = (8,))\n",
        "num_units = np.random.uniform(low = 46, high = 128, size = (8,)).astype(int)\n",
        "l2_penalties = 10**l2_penalties_exp\n",
        "\n",
        "val_acc = np.zeros((8,))\n",
        "\n",
        "for (i, (dropout_rate, l2_penalty, num_unit)) in enumerate(zip(dropout_rates, l2_penalties, num_units)):\n",
        "  model_dropout = models.Sequential()\n",
        "\n",
        "  model_dropout.add(layers.Dropout(rate = dropout_rate))\n",
        "  model_dropout.add(layers.Dense(num_unit, activation='relu', kernel_regularizer=regularizers.l2(l = l2_penalty), input_shape=(10000,)))\n",
        "  model_dropout.add(layers.Dropout(rate = dropout_rate))\n",
        "  model_dropout.add(layers.Dense(num_unit, activation='relu', kernel_regularizer=regularizers.l2(l = l2_penalty)))\n",
        "  model_dropout.add(layers.Dropout(rate = dropout_rate))\n",
        "  model_dropout.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "  model_dropout.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  history = model_dropout.fit(partial_x_train,\n",
        "                      partial_y_train,\n",
        "                      epochs=100,\n",
        "                      batch_size=512,\n",
        "                      validation_data=(x_val, y_val))\n",
        "  \n",
        "  val_acc[i] = history.history['val_acc'][-1]"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 3.3158 - acc: 0.3266 - val_loss: 2.3515 - val_acc: 0.5260\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 2.1381 - acc: 0.5114 - val_loss: 1.6845 - val_acc: 0.6050\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7165 - acc: 0.5981 - val_loss: 1.4768 - val_acc: 0.6710\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.5072 - acc: 0.6596 - val_loss: 1.3303 - val_acc: 0.7000\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 1.3415 - acc: 0.6989 - val_loss: 1.2395 - val_acc: 0.7210\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.2375 - acc: 0.7214 - val_loss: 1.1771 - val_acc: 0.7560\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.1461 - acc: 0.7497 - val_loss: 1.1216 - val_acc: 0.7610\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 1.0396 - acc: 0.7715 - val_loss: 1.0744 - val_acc: 0.7760\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.9705 - acc: 0.7863 - val_loss: 1.0444 - val_acc: 0.7870\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.9011 - acc: 0.7964 - val_loss: 1.0264 - val_acc: 0.7940\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8418 - acc: 0.8133 - val_loss: 1.0066 - val_acc: 0.8060\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.7905 - acc: 0.8208 - val_loss: 0.9977 - val_acc: 0.8050\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.7468 - acc: 0.8317 - val_loss: 0.9898 - val_acc: 0.8100\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7082 - acc: 0.8381 - val_loss: 0.9815 - val_acc: 0.8160\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6697 - acc: 0.8479 - val_loss: 0.9767 - val_acc: 0.8240\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6372 - acc: 0.8566 - val_loss: 0.9850 - val_acc: 0.8140\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.6183 - acc: 0.8622 - val_loss: 0.9714 - val_acc: 0.8230\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.5943 - acc: 0.8658 - val_loss: 0.9763 - val_acc: 0.8230\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.5593 - acc: 0.8776 - val_loss: 0.9584 - val_acc: 0.8240\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.5460 - acc: 0.8750 - val_loss: 0.9654 - val_acc: 0.8210\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.5286 - acc: 0.8855 - val_loss: 0.9569 - val_acc: 0.8210\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.5013 - acc: 0.8919 - val_loss: 0.9517 - val_acc: 0.8220\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.5030 - acc: 0.8929 - val_loss: 0.9700 - val_acc: 0.8220\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.4697 - acc: 0.9019 - val_loss: 0.9638 - val_acc: 0.8250\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4743 - acc: 0.8965 - val_loss: 0.9758 - val_acc: 0.8200\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.4662 - acc: 0.9004 - val_loss: 0.9738 - val_acc: 0.8200\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.4501 - acc: 0.9055 - val_loss: 0.9682 - val_acc: 0.8230\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.4458 - acc: 0.9082 - val_loss: 0.9851 - val_acc: 0.8260\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.4259 - acc: 0.9108 - val_loss: 0.9975 - val_acc: 0.8250\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.4171 - acc: 0.9147 - val_loss: 0.9881 - val_acc: 0.8280\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.4243 - acc: 0.9109 - val_loss: 1.0141 - val_acc: 0.8170\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.4069 - acc: 0.9121 - val_loss: 1.0155 - val_acc: 0.8260\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4070 - acc: 0.9167 - val_loss: 1.0121 - val_acc: 0.8230\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3989 - acc: 0.9196 - val_loss: 1.0125 - val_acc: 0.8300\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3906 - acc: 0.9223 - val_loss: 1.0159 - val_acc: 0.8260\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3970 - acc: 0.9189 - val_loss: 1.0232 - val_acc: 0.8230\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.3848 - acc: 0.9263 - val_loss: 1.0154 - val_acc: 0.8270\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3925 - acc: 0.9222 - val_loss: 1.0118 - val_acc: 0.8240\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3937 - acc: 0.9233 - val_loss: 1.0099 - val_acc: 0.8290\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3795 - acc: 0.9266 - val_loss: 1.0125 - val_acc: 0.8260\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3839 - acc: 0.9243 - val_loss: 1.0240 - val_acc: 0.8280\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3710 - acc: 0.9286 - val_loss: 1.0458 - val_acc: 0.8290\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3778 - acc: 0.9231 - val_loss: 1.0225 - val_acc: 0.8340\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3649 - acc: 0.9291 - val_loss: 1.0368 - val_acc: 0.8280\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3623 - acc: 0.9282 - val_loss: 1.0470 - val_acc: 0.8280\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3592 - acc: 0.9302 - val_loss: 1.0517 - val_acc: 0.8250\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3519 - acc: 0.9321 - val_loss: 1.0440 - val_acc: 0.8280\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3471 - acc: 0.9337 - val_loss: 1.0370 - val_acc: 0.8280\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3541 - acc: 0.9355 - val_loss: 1.0516 - val_acc: 0.8270\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3526 - acc: 0.9306 - val_loss: 1.0496 - val_acc: 0.8320\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3588 - acc: 0.9315 - val_loss: 1.0507 - val_acc: 0.8290\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3504 - acc: 0.9346 - val_loss: 1.0524 - val_acc: 0.8280\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3565 - acc: 0.9318 - val_loss: 1.0588 - val_acc: 0.8280\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3519 - acc: 0.9328 - val_loss: 1.0687 - val_acc: 0.8300\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3532 - acc: 0.9328 - val_loss: 1.0646 - val_acc: 0.8310\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 74us/step - loss: 0.3471 - acc: 0.9337 - val_loss: 1.0554 - val_acc: 0.8320\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3410 - acc: 0.9357 - val_loss: 1.0684 - val_acc: 0.8310\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3513 - acc: 0.9342 - val_loss: 1.0528 - val_acc: 0.8250\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3338 - acc: 0.9407 - val_loss: 1.0555 - val_acc: 0.8280\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3396 - acc: 0.9354 - val_loss: 1.0744 - val_acc: 0.8290\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3453 - acc: 0.9361 - val_loss: 1.0555 - val_acc: 0.8300\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3345 - acc: 0.9389 - val_loss: 1.0715 - val_acc: 0.8280\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3313 - acc: 0.9400 - val_loss: 1.0699 - val_acc: 0.8310\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 74us/step - loss: 0.3395 - acc: 0.9359 - val_loss: 1.0649 - val_acc: 0.8300\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3319 - acc: 0.9412 - val_loss: 1.0668 - val_acc: 0.8300\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3415 - acc: 0.9401 - val_loss: 1.0699 - val_acc: 0.8220\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3297 - acc: 0.9367 - val_loss: 1.0579 - val_acc: 0.8240\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3382 - acc: 0.9370 - val_loss: 1.0621 - val_acc: 0.8260\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3262 - acc: 0.9402 - val_loss: 1.0698 - val_acc: 0.8260\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3273 - acc: 0.9379 - val_loss: 1.0846 - val_acc: 0.8290\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 74us/step - loss: 0.3381 - acc: 0.9337 - val_loss: 1.0714 - val_acc: 0.8200\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3320 - acc: 0.9392 - val_loss: 1.0885 - val_acc: 0.8280\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3291 - acc: 0.9391 - val_loss: 1.0858 - val_acc: 0.8320\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3293 - acc: 0.9404 - val_loss: 1.0981 - val_acc: 0.8310\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3265 - acc: 0.9405 - val_loss: 1.0823 - val_acc: 0.8320\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3319 - acc: 0.9376 - val_loss: 1.0903 - val_acc: 0.8270\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3317 - acc: 0.9394 - val_loss: 1.0980 - val_acc: 0.8220\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3222 - acc: 0.9399 - val_loss: 1.1131 - val_acc: 0.8270\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3243 - acc: 0.9409 - val_loss: 1.1048 - val_acc: 0.8240\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3292 - acc: 0.9362 - val_loss: 1.0929 - val_acc: 0.8220\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3331 - acc: 0.9352 - val_loss: 1.0961 - val_acc: 0.8210\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3298 - acc: 0.9405 - val_loss: 1.1136 - val_acc: 0.8150\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3313 - acc: 0.9387 - val_loss: 1.0939 - val_acc: 0.8190\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3160 - acc: 0.9422 - val_loss: 1.1141 - val_acc: 0.8210\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3307 - acc: 0.9374 - val_loss: 1.1043 - val_acc: 0.8290\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3158 - acc: 0.9406 - val_loss: 1.1161 - val_acc: 0.8220\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3298 - acc: 0.9384 - val_loss: 1.1317 - val_acc: 0.8250\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3288 - acc: 0.9384 - val_loss: 1.1143 - val_acc: 0.8210\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3151 - acc: 0.9414 - val_loss: 1.1202 - val_acc: 0.8220\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3159 - acc: 0.9442 - val_loss: 1.1360 - val_acc: 0.8250\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3062 - acc: 0.9432 - val_loss: 1.1327 - val_acc: 0.8180\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3100 - acc: 0.9455 - val_loss: 1.1333 - val_acc: 0.8190\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3190 - acc: 0.9407 - val_loss: 1.1240 - val_acc: 0.8230\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3142 - acc: 0.9460 - val_loss: 1.1303 - val_acc: 0.8170\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3149 - acc: 0.9429 - val_loss: 1.1353 - val_acc: 0.8190\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3134 - acc: 0.9399 - val_loss: 1.1371 - val_acc: 0.8190\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3204 - acc: 0.9392 - val_loss: 1.1595 - val_acc: 0.8210\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3144 - acc: 0.9432 - val_loss: 1.1314 - val_acc: 0.8260\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3093 - acc: 0.9456 - val_loss: 1.1399 - val_acc: 0.8180\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3167 - acc: 0.9424 - val_loss: 1.1380 - val_acc: 0.8190\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 3.4652 - acc: 0.3782 - val_loss: 2.6056 - val_acc: 0.5630\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 2.1512 - acc: 0.5862 - val_loss: 1.6646 - val_acc: 0.6600\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.5488 - acc: 0.6825 - val_loss: 1.3794 - val_acc: 0.7250\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 1.2780 - acc: 0.7427 - val_loss: 1.2536 - val_acc: 0.7570\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 1.1099 - acc: 0.7870 - val_loss: 1.1864 - val_acc: 0.7770\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.9829 - acc: 0.8156 - val_loss: 1.1307 - val_acc: 0.7940\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.8666 - acc: 0.8425 - val_loss: 1.0840 - val_acc: 0.8120\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.7800 - acc: 0.8690 - val_loss: 1.0686 - val_acc: 0.8070\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.7163 - acc: 0.8804 - val_loss: 1.0529 - val_acc: 0.8220\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6554 - acc: 0.8936 - val_loss: 1.0514 - val_acc: 0.8270\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.6185 - acc: 0.9049 - val_loss: 1.0437 - val_acc: 0.8320\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.5796 - acc: 0.9161 - val_loss: 1.0536 - val_acc: 0.8230\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.5550 - acc: 0.9221 - val_loss: 1.0469 - val_acc: 0.8240\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.5274 - acc: 0.9237 - val_loss: 1.0577 - val_acc: 0.8230\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.5108 - acc: 0.9315 - val_loss: 1.0595 - val_acc: 0.8280\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.5124 - acc: 0.9297 - val_loss: 1.0525 - val_acc: 0.8240\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4939 - acc: 0.9323 - val_loss: 1.0558 - val_acc: 0.8280\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.4691 - acc: 0.9399 - val_loss: 1.0600 - val_acc: 0.8210\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.4700 - acc: 0.9389 - val_loss: 1.0629 - val_acc: 0.8150\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.4583 - acc: 0.9385 - val_loss: 1.0659 - val_acc: 0.8190\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.4486 - acc: 0.9434 - val_loss: 1.0603 - val_acc: 0.8220\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4435 - acc: 0.9449 - val_loss: 1.0581 - val_acc: 0.8240\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.4304 - acc: 0.9450 - val_loss: 1.0659 - val_acc: 0.8210\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.4317 - acc: 0.9463 - val_loss: 1.0884 - val_acc: 0.8180\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.4245 - acc: 0.9459 - val_loss: 1.0723 - val_acc: 0.8300\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.4232 - acc: 0.9450 - val_loss: 1.0904 - val_acc: 0.8240\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.4177 - acc: 0.9504 - val_loss: 1.0998 - val_acc: 0.8140\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4184 - acc: 0.9460 - val_loss: 1.0900 - val_acc: 0.8250\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4111 - acc: 0.9480 - val_loss: 1.0788 - val_acc: 0.8190\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.4070 - acc: 0.9523 - val_loss: 1.0763 - val_acc: 0.8220\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4080 - acc: 0.9475 - val_loss: 1.0735 - val_acc: 0.8170\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.4002 - acc: 0.9494 - val_loss: 1.0756 - val_acc: 0.8220\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.3968 - acc: 0.9505 - val_loss: 1.0978 - val_acc: 0.8270\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3906 - acc: 0.9505 - val_loss: 1.0922 - val_acc: 0.8140\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3938 - acc: 0.9505 - val_loss: 1.0830 - val_acc: 0.8190\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.3915 - acc: 0.9515 - val_loss: 1.0833 - val_acc: 0.8170\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3790 - acc: 0.9519 - val_loss: 1.1042 - val_acc: 0.8210\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3844 - acc: 0.9501 - val_loss: 1.1077 - val_acc: 0.8140\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3810 - acc: 0.9518 - val_loss: 1.0709 - val_acc: 0.8240\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3746 - acc: 0.9520 - val_loss: 1.0982 - val_acc: 0.8190\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3723 - acc: 0.9538 - val_loss: 1.1156 - val_acc: 0.8170\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3715 - acc: 0.9520 - val_loss: 1.1090 - val_acc: 0.8200\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3708 - acc: 0.9531 - val_loss: 1.1133 - val_acc: 0.8190\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3719 - acc: 0.9511 - val_loss: 1.0883 - val_acc: 0.8190\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3702 - acc: 0.9509 - val_loss: 1.0951 - val_acc: 0.8160\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 75us/step - loss: 0.3621 - acc: 0.9536 - val_loss: 1.0998 - val_acc: 0.8100\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 75us/step - loss: 0.3663 - acc: 0.9498 - val_loss: 1.0908 - val_acc: 0.8230\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3653 - acc: 0.9524 - val_loss: 1.0992 - val_acc: 0.8210\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3519 - acc: 0.9553 - val_loss: 1.1182 - val_acc: 0.8160\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3579 - acc: 0.9551 - val_loss: 1.1073 - val_acc: 0.8170\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3511 - acc: 0.9550 - val_loss: 1.1144 - val_acc: 0.8190\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3556 - acc: 0.9544 - val_loss: 1.1154 - val_acc: 0.8160\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3572 - acc: 0.9536 - val_loss: 1.1007 - val_acc: 0.8170\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3488 - acc: 0.9540 - val_loss: 1.1054 - val_acc: 0.8160\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 74us/step - loss: 0.3519 - acc: 0.9548 - val_loss: 1.1145 - val_acc: 0.8120\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3448 - acc: 0.9538 - val_loss: 1.0945 - val_acc: 0.8190\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3488 - acc: 0.9520 - val_loss: 1.0876 - val_acc: 0.8130\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3488 - acc: 0.9524 - val_loss: 1.1065 - val_acc: 0.8150\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3440 - acc: 0.9551 - val_loss: 1.1026 - val_acc: 0.8140\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3426 - acc: 0.9536 - val_loss: 1.1129 - val_acc: 0.8160\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3339 - acc: 0.9545 - val_loss: 1.0731 - val_acc: 0.8230\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 74us/step - loss: 0.3404 - acc: 0.9553 - val_loss: 1.0836 - val_acc: 0.8160\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3385 - acc: 0.9577 - val_loss: 1.0923 - val_acc: 0.8110\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3315 - acc: 0.9551 - val_loss: 1.1257 - val_acc: 0.8140\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3343 - acc: 0.9562 - val_loss: 1.1166 - val_acc: 0.8190\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3360 - acc: 0.9541 - val_loss: 1.1147 - val_acc: 0.8150\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 75us/step - loss: 0.3360 - acc: 0.9548 - val_loss: 1.1314 - val_acc: 0.8170\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3300 - acc: 0.9550 - val_loss: 1.1270 - val_acc: 0.8210\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3323 - acc: 0.9546 - val_loss: 1.1241 - val_acc: 0.8170\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3376 - acc: 0.9539 - val_loss: 1.1063 - val_acc: 0.8170\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3294 - acc: 0.9564 - val_loss: 1.1181 - val_acc: 0.8120\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3273 - acc: 0.9565 - val_loss: 1.1161 - val_acc: 0.8130\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3235 - acc: 0.9568 - val_loss: 1.1011 - val_acc: 0.8210\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3239 - acc: 0.9580 - val_loss: 1.0981 - val_acc: 0.8120\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3243 - acc: 0.9557 - val_loss: 1.0958 - val_acc: 0.8160\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3251 - acc: 0.9551 - val_loss: 1.1084 - val_acc: 0.8180\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3166 - acc: 0.9577 - val_loss: 1.0888 - val_acc: 0.8190\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 74us/step - loss: 0.3176 - acc: 0.9563 - val_loss: 1.1314 - val_acc: 0.8140\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3209 - acc: 0.9582 - val_loss: 1.1151 - val_acc: 0.8180\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3184 - acc: 0.9580 - val_loss: 1.1087 - val_acc: 0.8210\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3245 - acc: 0.9543 - val_loss: 1.1084 - val_acc: 0.8180\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3169 - acc: 0.9568 - val_loss: 1.1022 - val_acc: 0.8200\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3154 - acc: 0.9579 - val_loss: 1.1186 - val_acc: 0.8180\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3101 - acc: 0.9575 - val_loss: 1.1357 - val_acc: 0.8200\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3162 - acc: 0.9553 - val_loss: 1.1189 - val_acc: 0.8160\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3165 - acc: 0.9562 - val_loss: 1.1128 - val_acc: 0.8200\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3099 - acc: 0.9565 - val_loss: 1.1155 - val_acc: 0.8160\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3106 - acc: 0.9545 - val_loss: 1.1066 - val_acc: 0.8240\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3114 - acc: 0.9546 - val_loss: 1.1183 - val_acc: 0.8190\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3115 - acc: 0.9543 - val_loss: 1.0899 - val_acc: 0.8180\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3082 - acc: 0.9582 - val_loss: 1.1111 - val_acc: 0.8170\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3133 - acc: 0.9563 - val_loss: 1.0861 - val_acc: 0.8220\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3118 - acc: 0.9564 - val_loss: 1.0908 - val_acc: 0.8200\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3060 - acc: 0.9570 - val_loss: 1.0990 - val_acc: 0.8170\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3058 - acc: 0.9562 - val_loss: 1.1008 - val_acc: 0.8210\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3048 - acc: 0.9590 - val_loss: 1.1083 - val_acc: 0.8140\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3058 - acc: 0.9575 - val_loss: 1.0977 - val_acc: 0.8170\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3013 - acc: 0.9603 - val_loss: 1.1209 - val_acc: 0.8110\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3003 - acc: 0.9583 - val_loss: 1.0930 - val_acc: 0.8120\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3073 - acc: 0.9548 - val_loss: 1.1073 - val_acc: 0.8170\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 4.3689 - acc: 0.4330 - val_loss: 3.1182 - val_acc: 0.5440\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.5939 - acc: 0.5956 - val_loss: 2.1976 - val_acc: 0.6420\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.0487 - acc: 0.6891 - val_loss: 1.9188 - val_acc: 0.6980\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8027 - acc: 0.7326 - val_loss: 1.7851 - val_acc: 0.7200\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6428 - acc: 0.7578 - val_loss: 1.6854 - val_acc: 0.7460\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.5359 - acc: 0.7759 - val_loss: 1.6139 - val_acc: 0.7610\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.4514 - acc: 0.7930 - val_loss: 1.5690 - val_acc: 0.7660\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.3880 - acc: 0.8056 - val_loss: 1.5405 - val_acc: 0.7640\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.3355 - acc: 0.8170 - val_loss: 1.4993 - val_acc: 0.7740\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.2988 - acc: 0.8197 - val_loss: 1.4810 - val_acc: 0.7800\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.2562 - acc: 0.8304 - val_loss: 1.4572 - val_acc: 0.7820\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.2316 - acc: 0.8299 - val_loss: 1.4365 - val_acc: 0.7830\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.1978 - acc: 0.8394 - val_loss: 1.4356 - val_acc: 0.7830\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.1802 - acc: 0.8386 - val_loss: 1.4238 - val_acc: 0.7810\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.1537 - acc: 0.8460 - val_loss: 1.3944 - val_acc: 0.7900\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.1350 - acc: 0.8517 - val_loss: 1.3939 - val_acc: 0.7900\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.1179 - acc: 0.8510 - val_loss: 1.3792 - val_acc: 0.7890\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.1012 - acc: 0.8545 - val_loss: 1.3612 - val_acc: 0.7960\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.0796 - acc: 0.8614 - val_loss: 1.3683 - val_acc: 0.7930\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.0624 - acc: 0.8617 - val_loss: 1.3513 - val_acc: 0.7950\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.0566 - acc: 0.8647 - val_loss: 1.3379 - val_acc: 0.8060\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.0432 - acc: 0.8685 - val_loss: 1.3318 - val_acc: 0.8000\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.0308 - acc: 0.8682 - val_loss: 1.3419 - val_acc: 0.7940\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.0233 - acc: 0.8686 - val_loss: 1.3358 - val_acc: 0.8030\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.0090 - acc: 0.8727 - val_loss: 1.3182 - val_acc: 0.8000\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.9975 - acc: 0.8732 - val_loss: 1.3307 - val_acc: 0.7970\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.9813 - acc: 0.8761 - val_loss: 1.3078 - val_acc: 0.7960\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.9706 - acc: 0.8792 - val_loss: 1.3058 - val_acc: 0.8020\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.9670 - acc: 0.8810 - val_loss: 1.2970 - val_acc: 0.8040\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.9566 - acc: 0.8812 - val_loss: 1.3065 - val_acc: 0.8000\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.9439 - acc: 0.8814 - val_loss: 1.2990 - val_acc: 0.8020\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.9436 - acc: 0.8847 - val_loss: 1.2925 - val_acc: 0.8100\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.9321 - acc: 0.8846 - val_loss: 1.2828 - val_acc: 0.8090\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.9292 - acc: 0.8859 - val_loss: 1.2813 - val_acc: 0.8130\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.9148 - acc: 0.8881 - val_loss: 1.2695 - val_acc: 0.8000\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.9070 - acc: 0.8910 - val_loss: 1.2765 - val_acc: 0.8100\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.9064 - acc: 0.8880 - val_loss: 1.2764 - val_acc: 0.8140\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.8946 - acc: 0.8954 - val_loss: 1.2668 - val_acc: 0.8040\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.8921 - acc: 0.8920 - val_loss: 1.2681 - val_acc: 0.8190\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.8937 - acc: 0.8915 - val_loss: 1.2683 - val_acc: 0.8030\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8761 - acc: 0.8996 - val_loss: 1.2598 - val_acc: 0.8130\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.8729 - acc: 0.8954 - val_loss: 1.2608 - val_acc: 0.8090\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8682 - acc: 0.8978 - val_loss: 1.2560 - val_acc: 0.8060\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.8625 - acc: 0.8994 - val_loss: 1.2472 - val_acc: 0.8080\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8587 - acc: 0.8964 - val_loss: 1.2557 - val_acc: 0.8100\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.8493 - acc: 0.9008 - val_loss: 1.2484 - val_acc: 0.8080\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.8410 - acc: 0.9043 - val_loss: 1.2569 - val_acc: 0.8090\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.8390 - acc: 0.9035 - val_loss: 1.2433 - val_acc: 0.8110\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.8348 - acc: 0.9052 - val_loss: 1.2312 - val_acc: 0.8130\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8202 - acc: 0.9068 - val_loss: 1.2424 - val_acc: 0.8100\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.8190 - acc: 0.9084 - val_loss: 1.2505 - val_acc: 0.8090\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.8193 - acc: 0.9052 - val_loss: 1.2392 - val_acc: 0.8080\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.8159 - acc: 0.9072 - val_loss: 1.2390 - val_acc: 0.8130\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.8145 - acc: 0.9080 - val_loss: 1.2256 - val_acc: 0.8110\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.8099 - acc: 0.9077 - val_loss: 1.2295 - val_acc: 0.8160\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.8115 - acc: 0.9116 - val_loss: 1.2293 - val_acc: 0.8170\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.8019 - acc: 0.9099 - val_loss: 1.2319 - val_acc: 0.8140\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.7990 - acc: 0.9104 - val_loss: 1.2334 - val_acc: 0.8030\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.7878 - acc: 0.9123 - val_loss: 1.2395 - val_acc: 0.8090\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.7914 - acc: 0.9113 - val_loss: 1.2266 - val_acc: 0.8050\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.7711 - acc: 0.9173 - val_loss: 1.2145 - val_acc: 0.8130\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.7846 - acc: 0.9122 - val_loss: 1.2350 - val_acc: 0.8130\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7812 - acc: 0.9121 - val_loss: 1.2326 - val_acc: 0.8070\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7690 - acc: 0.9171 - val_loss: 1.2282 - val_acc: 0.8050\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7821 - acc: 0.9113 - val_loss: 1.2250 - val_acc: 0.8130\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.7723 - acc: 0.9129 - val_loss: 1.2406 - val_acc: 0.8150\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7628 - acc: 0.9149 - val_loss: 1.2274 - val_acc: 0.8080\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.7573 - acc: 0.9191 - val_loss: 1.2173 - val_acc: 0.8150\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7537 - acc: 0.9171 - val_loss: 1.2076 - val_acc: 0.8080\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7479 - acc: 0.9183 - val_loss: 1.2223 - val_acc: 0.8200\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.7465 - acc: 0.9198 - val_loss: 1.2103 - val_acc: 0.8180\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7406 - acc: 0.9207 - val_loss: 1.2175 - val_acc: 0.8120\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7453 - acc: 0.9191 - val_loss: 1.2161 - val_acc: 0.8110\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7401 - acc: 0.9183 - val_loss: 1.2158 - val_acc: 0.8070\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7438 - acc: 0.9166 - val_loss: 1.2020 - val_acc: 0.8150\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7397 - acc: 0.9171 - val_loss: 1.2153 - val_acc: 0.8140\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7238 - acc: 0.9240 - val_loss: 1.1985 - val_acc: 0.8140\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7266 - acc: 0.9212 - val_loss: 1.2062 - val_acc: 0.8150\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7229 - acc: 0.9221 - val_loss: 1.2023 - val_acc: 0.8230\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7129 - acc: 0.9232 - val_loss: 1.2036 - val_acc: 0.8160\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7134 - acc: 0.9217 - val_loss: 1.2049 - val_acc: 0.8100\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7079 - acc: 0.9240 - val_loss: 1.1890 - val_acc: 0.8210\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7028 - acc: 0.9272 - val_loss: 1.1905 - val_acc: 0.8150\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7125 - acc: 0.9227 - val_loss: 1.2124 - val_acc: 0.8120\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7140 - acc: 0.9226 - val_loss: 1.1924 - val_acc: 0.8100\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7019 - acc: 0.9231 - val_loss: 1.1928 - val_acc: 0.8190\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.7019 - acc: 0.9267 - val_loss: 1.1929 - val_acc: 0.8090\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.6930 - acc: 0.9285 - val_loss: 1.1929 - val_acc: 0.8190\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6943 - acc: 0.9290 - val_loss: 1.1916 - val_acc: 0.8220\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6958 - acc: 0.9243 - val_loss: 1.1855 - val_acc: 0.8240\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.6879 - acc: 0.9273 - val_loss: 1.1987 - val_acc: 0.8190\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.6863 - acc: 0.9283 - val_loss: 1.1858 - val_acc: 0.8160\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6849 - acc: 0.9277 - val_loss: 1.1937 - val_acc: 0.8090\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6800 - acc: 0.9263 - val_loss: 1.1842 - val_acc: 0.8180\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.6817 - acc: 0.9280 - val_loss: 1.1992 - val_acc: 0.8110\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6776 - acc: 0.9297 - val_loss: 1.1871 - val_acc: 0.8190\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.6824 - acc: 0.9280 - val_loss: 1.1903 - val_acc: 0.8240\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.6743 - acc: 0.9287 - val_loss: 1.1973 - val_acc: 0.8060\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.6720 - acc: 0.9287 - val_loss: 1.1922 - val_acc: 0.8230\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.6651 - acc: 0.9291 - val_loss: 1.1730 - val_acc: 0.8220\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 3.4459 - acc: 0.2865 - val_loss: 2.7227 - val_acc: 0.4220\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 2.3262 - acc: 0.4990 - val_loss: 1.7502 - val_acc: 0.6100\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7671 - acc: 0.5906 - val_loss: 1.4764 - val_acc: 0.6670\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.5078 - acc: 0.6527 - val_loss: 1.3226 - val_acc: 0.6970\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.3526 - acc: 0.6907 - val_loss: 1.2190 - val_acc: 0.7350\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.2262 - acc: 0.7204 - val_loss: 1.1421 - val_acc: 0.7480\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.1211 - acc: 0.7438 - val_loss: 1.0941 - val_acc: 0.7610\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.0386 - acc: 0.7642 - val_loss: 1.0450 - val_acc: 0.7770\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.9573 - acc: 0.7795 - val_loss: 1.0072 - val_acc: 0.7830\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8878 - acc: 0.7944 - val_loss: 0.9745 - val_acc: 0.7880\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.8214 - acc: 0.8068 - val_loss: 0.9563 - val_acc: 0.7940\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.7636 - acc: 0.8170 - val_loss: 0.9379 - val_acc: 0.8000\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.7217 - acc: 0.8244 - val_loss: 0.9238 - val_acc: 0.8050\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.6830 - acc: 0.8336 - val_loss: 0.9109 - val_acc: 0.8150\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.6501 - acc: 0.8430 - val_loss: 0.9096 - val_acc: 0.8110\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.5938 - acc: 0.8576 - val_loss: 0.9000 - val_acc: 0.8140\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.5680 - acc: 0.8616 - val_loss: 0.8951 - val_acc: 0.8180\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.5399 - acc: 0.8651 - val_loss: 0.8925 - val_acc: 0.8130\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.5141 - acc: 0.8741 - val_loss: 0.8824 - val_acc: 0.8180\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4863 - acc: 0.8766 - val_loss: 0.8909 - val_acc: 0.8190\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.4525 - acc: 0.8856 - val_loss: 0.8914 - val_acc: 0.8140\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.4503 - acc: 0.8875 - val_loss: 0.8900 - val_acc: 0.8180\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.4301 - acc: 0.8920 - val_loss: 0.8877 - val_acc: 0.8230\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.4185 - acc: 0.8963 - val_loss: 0.8941 - val_acc: 0.8230\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.4036 - acc: 0.8975 - val_loss: 0.9042 - val_acc: 0.8170\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3927 - acc: 0.8980 - val_loss: 0.9047 - val_acc: 0.8240\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3857 - acc: 0.9005 - val_loss: 0.9144 - val_acc: 0.8260\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.3560 - acc: 0.9093 - val_loss: 0.9197 - val_acc: 0.8240\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.3458 - acc: 0.9098 - val_loss: 0.9238 - val_acc: 0.8290\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3398 - acc: 0.9132 - val_loss: 0.9366 - val_acc: 0.8240\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3374 - acc: 0.9107 - val_loss: 0.9246 - val_acc: 0.8300\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3307 - acc: 0.9152 - val_loss: 0.9247 - val_acc: 0.8300\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3121 - acc: 0.9187 - val_loss: 0.9294 - val_acc: 0.8270\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.3161 - acc: 0.9171 - val_loss: 0.9241 - val_acc: 0.8280\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.3057 - acc: 0.9204 - val_loss: 0.9278 - val_acc: 0.8320\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2935 - acc: 0.9232 - val_loss: 0.9434 - val_acc: 0.8310\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.2942 - acc: 0.9247 - val_loss: 0.9346 - val_acc: 0.8300\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2814 - acc: 0.9252 - val_loss: 0.9522 - val_acc: 0.8280\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.2792 - acc: 0.9255 - val_loss: 0.9583 - val_acc: 0.8280\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2742 - acc: 0.9260 - val_loss: 0.9708 - val_acc: 0.8290\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.2708 - acc: 0.9272 - val_loss: 0.9557 - val_acc: 0.8290\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.2631 - acc: 0.9258 - val_loss: 0.9746 - val_acc: 0.8290\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.2646 - acc: 0.9313 - val_loss: 0.9629 - val_acc: 0.8260\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2514 - acc: 0.9344 - val_loss: 0.9902 - val_acc: 0.8240\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.2572 - acc: 0.9320 - val_loss: 0.9956 - val_acc: 0.8270\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2487 - acc: 0.9331 - val_loss: 1.0099 - val_acc: 0.8240\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.2389 - acc: 0.9336 - val_loss: 1.0174 - val_acc: 0.8220\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.2411 - acc: 0.9356 - val_loss: 1.0100 - val_acc: 0.8220\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2448 - acc: 0.9327 - val_loss: 1.0130 - val_acc: 0.8210\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2411 - acc: 0.9300 - val_loss: 1.0117 - val_acc: 0.8210\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2304 - acc: 0.9376 - val_loss: 1.0173 - val_acc: 0.8180\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2260 - acc: 0.9389 - val_loss: 1.0337 - val_acc: 0.8180\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.2292 - acc: 0.9386 - val_loss: 1.0163 - val_acc: 0.8220\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.2233 - acc: 0.9400 - val_loss: 1.0246 - val_acc: 0.8190\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2223 - acc: 0.9410 - val_loss: 1.0266 - val_acc: 0.8190\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2251 - acc: 0.9371 - val_loss: 1.0395 - val_acc: 0.8210\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.2131 - acc: 0.9407 - val_loss: 1.0486 - val_acc: 0.8190\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2215 - acc: 0.9409 - val_loss: 1.0405 - val_acc: 0.8200\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2167 - acc: 0.9381 - val_loss: 1.0451 - val_acc: 0.8180\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.2135 - acc: 0.9387 - val_loss: 1.0587 - val_acc: 0.8190\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2105 - acc: 0.9405 - val_loss: 1.0771 - val_acc: 0.8180\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2161 - acc: 0.9402 - val_loss: 1.0728 - val_acc: 0.8170\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.2019 - acc: 0.9449 - val_loss: 1.0802 - val_acc: 0.8160\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2115 - acc: 0.9420 - val_loss: 1.0653 - val_acc: 0.8190\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2123 - acc: 0.9390 - val_loss: 1.0725 - val_acc: 0.8210\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.2049 - acc: 0.9439 - val_loss: 1.0691 - val_acc: 0.8170\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 76us/step - loss: 0.2005 - acc: 0.9440 - val_loss: 1.0872 - val_acc: 0.8160\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.2056 - acc: 0.9410 - val_loss: 1.0868 - val_acc: 0.8150\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.2052 - acc: 0.9424 - val_loss: 1.0951 - val_acc: 0.8130\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1996 - acc: 0.9427 - val_loss: 1.1003 - val_acc: 0.8150\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.2073 - acc: 0.9420 - val_loss: 1.0994 - val_acc: 0.8160\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.2035 - acc: 0.9431 - val_loss: 1.1003 - val_acc: 0.8180\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.1927 - acc: 0.9434 - val_loss: 1.1161 - val_acc: 0.8210\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.1987 - acc: 0.9432 - val_loss: 1.1035 - val_acc: 0.8200\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.2017 - acc: 0.9426 - val_loss: 1.1126 - val_acc: 0.8210\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1942 - acc: 0.9446 - val_loss: 1.1123 - val_acc: 0.8170\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.1853 - acc: 0.9455 - val_loss: 1.1093 - val_acc: 0.8220\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1928 - acc: 0.9449 - val_loss: 1.1267 - val_acc: 0.8200\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.1884 - acc: 0.9459 - val_loss: 1.1304 - val_acc: 0.8180\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1883 - acc: 0.9481 - val_loss: 1.1461 - val_acc: 0.8190\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1903 - acc: 0.9461 - val_loss: 1.1297 - val_acc: 0.8200\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.1845 - acc: 0.9491 - val_loss: 1.1211 - val_acc: 0.8200\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1810 - acc: 0.9473 - val_loss: 1.1444 - val_acc: 0.8230\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1841 - acc: 0.9463 - val_loss: 1.1392 - val_acc: 0.8210\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1805 - acc: 0.9480 - val_loss: 1.1532 - val_acc: 0.8190\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1791 - acc: 0.9501 - val_loss: 1.1514 - val_acc: 0.8210\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1803 - acc: 0.9455 - val_loss: 1.1444 - val_acc: 0.8210\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.1812 - acc: 0.9476 - val_loss: 1.1382 - val_acc: 0.8250\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1879 - acc: 0.9456 - val_loss: 1.1496 - val_acc: 0.8250\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.1762 - acc: 0.9473 - val_loss: 1.1559 - val_acc: 0.8230\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.1866 - acc: 0.9459 - val_loss: 1.1390 - val_acc: 0.8190\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.1887 - acc: 0.9464 - val_loss: 1.1434 - val_acc: 0.8160\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.1734 - acc: 0.9494 - val_loss: 1.1501 - val_acc: 0.8190\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.1729 - acc: 0.9494 - val_loss: 1.1522 - val_acc: 0.8250\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.1760 - acc: 0.9479 - val_loss: 1.1689 - val_acc: 0.8160\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.1738 - acc: 0.9469 - val_loss: 1.1758 - val_acc: 0.8190\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.1737 - acc: 0.9486 - val_loss: 1.1699 - val_acc: 0.8230\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1754 - acc: 0.9488 - val_loss: 1.1685 - val_acc: 0.8250\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1747 - acc: 0.9486 - val_loss: 1.1795 - val_acc: 0.8190\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1781 - acc: 0.9469 - val_loss: 1.1631 - val_acc: 0.8190\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 8.4009 - acc: 0.2881 - val_loss: 5.5186 - val_acc: 0.3540\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 4.7809 - acc: 0.4342 - val_loss: 3.9953 - val_acc: 0.5430\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 3.8542 - acc: 0.5284 - val_loss: 3.3607 - val_acc: 0.5950\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 3.3051 - acc: 0.5739 - val_loss: 2.9527 - val_acc: 0.6270\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 2.9681 - acc: 0.5928 - val_loss: 2.6683 - val_acc: 0.6360\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 2.7072 - acc: 0.6054 - val_loss: 2.4550 - val_acc: 0.6590\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 2.5143 - acc: 0.6210 - val_loss: 2.3003 - val_acc: 0.6640\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.3530 - acc: 0.6252 - val_loss: 2.1677 - val_acc: 0.6640\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.2334 - acc: 0.6309 - val_loss: 2.0697 - val_acc: 0.6710\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.1424 - acc: 0.6367 - val_loss: 1.9928 - val_acc: 0.6680\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.0714 - acc: 0.6463 - val_loss: 1.9294 - val_acc: 0.6740\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 2.0144 - acc: 0.6480 - val_loss: 1.8885 - val_acc: 0.6760\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.9620 - acc: 0.6528 - val_loss: 1.8474 - val_acc: 0.6740\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.9291 - acc: 0.6527 - val_loss: 1.8153 - val_acc: 0.6790\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.9086 - acc: 0.6548 - val_loss: 1.7886 - val_acc: 0.6840\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.8747 - acc: 0.6664 - val_loss: 1.7696 - val_acc: 0.6850\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8548 - acc: 0.6590 - val_loss: 1.7519 - val_acc: 0.6890\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.8384 - acc: 0.6622 - val_loss: 1.7307 - val_acc: 0.6860\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.8264 - acc: 0.6710 - val_loss: 1.7253 - val_acc: 0.6860\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.8070 - acc: 0.6700 - val_loss: 1.7111 - val_acc: 0.6920\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.8066 - acc: 0.6670 - val_loss: 1.7017 - val_acc: 0.6890\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.7859 - acc: 0.6718 - val_loss: 1.6999 - val_acc: 0.6870\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.7855 - acc: 0.6708 - val_loss: 1.6894 - val_acc: 0.6860\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7846 - acc: 0.6706 - val_loss: 1.6881 - val_acc: 0.6930\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7747 - acc: 0.6739 - val_loss: 1.6719 - val_acc: 0.6860\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7567 - acc: 0.6709 - val_loss: 1.6690 - val_acc: 0.6870\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 1.7533 - acc: 0.6731 - val_loss: 1.6561 - val_acc: 0.6980\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7379 - acc: 0.6775 - val_loss: 1.6506 - val_acc: 0.6890\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7404 - acc: 0.6775 - val_loss: 1.6515 - val_acc: 0.6900\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7405 - acc: 0.6736 - val_loss: 1.6498 - val_acc: 0.6870\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7268 - acc: 0.6771 - val_loss: 1.6387 - val_acc: 0.6940\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7274 - acc: 0.6763 - val_loss: 1.6341 - val_acc: 0.6980\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7129 - acc: 0.6789 - val_loss: 1.6338 - val_acc: 0.6910\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.7194 - acc: 0.6788 - val_loss: 1.6247 - val_acc: 0.6970\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7012 - acc: 0.6813 - val_loss: 1.6258 - val_acc: 0.6880\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6999 - acc: 0.6794 - val_loss: 1.6218 - val_acc: 0.6950\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7000 - acc: 0.6818 - val_loss: 1.6085 - val_acc: 0.6990\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7016 - acc: 0.6820 - val_loss: 1.6123 - val_acc: 0.6960\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6977 - acc: 0.6805 - val_loss: 1.6071 - val_acc: 0.7000\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6852 - acc: 0.6815 - val_loss: 1.6010 - val_acc: 0.6940\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.6709 - acc: 0.6855 - val_loss: 1.5950 - val_acc: 0.7000\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 1.6774 - acc: 0.6827 - val_loss: 1.5925 - val_acc: 0.7000\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6853 - acc: 0.6809 - val_loss: 1.6023 - val_acc: 0.6930\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6778 - acc: 0.6850 - val_loss: 1.5989 - val_acc: 0.6960\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6730 - acc: 0.6824 - val_loss: 1.5899 - val_acc: 0.6960\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6610 - acc: 0.6891 - val_loss: 1.5905 - val_acc: 0.6980\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6670 - acc: 0.6844 - val_loss: 1.5774 - val_acc: 0.6990\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 1.6575 - acc: 0.6888 - val_loss: 1.5826 - val_acc: 0.7010\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6481 - acc: 0.6868 - val_loss: 1.5782 - val_acc: 0.7070\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.6606 - acc: 0.6883 - val_loss: 1.5691 - val_acc: 0.7030\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.6625 - acc: 0.6884 - val_loss: 1.5784 - val_acc: 0.6990\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6516 - acc: 0.6882 - val_loss: 1.5802 - val_acc: 0.7020\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6588 - acc: 0.6883 - val_loss: 1.5720 - val_acc: 0.7000\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6453 - acc: 0.6880 - val_loss: 1.5679 - val_acc: 0.7010\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6419 - acc: 0.6908 - val_loss: 1.5765 - val_acc: 0.6970\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6448 - acc: 0.6860 - val_loss: 1.5788 - val_acc: 0.6920\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6339 - acc: 0.6903 - val_loss: 1.5685 - val_acc: 0.7000\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6310 - acc: 0.6909 - val_loss: 1.5619 - val_acc: 0.7070\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6404 - acc: 0.6873 - val_loss: 1.5626 - val_acc: 0.7010\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6328 - acc: 0.6929 - val_loss: 1.5631 - val_acc: 0.7010\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6324 - acc: 0.6904 - val_loss: 1.5547 - val_acc: 0.7090\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.6246 - acc: 0.6956 - val_loss: 1.5549 - val_acc: 0.7020\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6282 - acc: 0.6944 - val_loss: 1.5472 - val_acc: 0.7040\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6324 - acc: 0.6924 - val_loss: 1.5538 - val_acc: 0.7130\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6182 - acc: 0.6976 - val_loss: 1.5540 - val_acc: 0.7060\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6155 - acc: 0.6953 - val_loss: 1.5439 - val_acc: 0.7070\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6079 - acc: 0.6951 - val_loss: 1.5500 - val_acc: 0.7020\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6102 - acc: 0.7010 - val_loss: 1.5403 - val_acc: 0.7080\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6166 - acc: 0.6958 - val_loss: 1.5396 - val_acc: 0.7060\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.6079 - acc: 0.6938 - val_loss: 1.5420 - val_acc: 0.7070\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.6159 - acc: 0.6967 - val_loss: 1.5418 - val_acc: 0.7000\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.6070 - acc: 0.6964 - val_loss: 1.5329 - val_acc: 0.7050\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.6052 - acc: 0.6993 - val_loss: 1.5222 - val_acc: 0.7150\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.6088 - acc: 0.6957 - val_loss: 1.5388 - val_acc: 0.7030\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6059 - acc: 0.6976 - val_loss: 1.5337 - val_acc: 0.7080\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.5983 - acc: 0.6989 - val_loss: 1.5316 - val_acc: 0.7100\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6108 - acc: 0.6964 - val_loss: 1.5321 - val_acc: 0.7130\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.5799 - acc: 0.7026 - val_loss: 1.5223 - val_acc: 0.7160\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6051 - acc: 0.6964 - val_loss: 1.5354 - val_acc: 0.7070\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5888 - acc: 0.7037 - val_loss: 1.5246 - val_acc: 0.7140\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.5860 - acc: 0.7027 - val_loss: 1.5323 - val_acc: 0.7040\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5964 - acc: 0.6964 - val_loss: 1.5220 - val_acc: 0.7170\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5858 - acc: 0.7018 - val_loss: 1.5141 - val_acc: 0.7150\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.5859 - acc: 0.7007 - val_loss: 1.5170 - val_acc: 0.7170\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.5852 - acc: 0.7015 - val_loss: 1.5188 - val_acc: 0.7220\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.5760 - acc: 0.7023 - val_loss: 1.5210 - val_acc: 0.7070\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.5734 - acc: 0.7060 - val_loss: 1.5192 - val_acc: 0.7130\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5753 - acc: 0.7087 - val_loss: 1.5162 - val_acc: 0.7160\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5780 - acc: 0.7013 - val_loss: 1.5177 - val_acc: 0.7180\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5703 - acc: 0.7065 - val_loss: 1.5146 - val_acc: 0.7170\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.5723 - acc: 0.7022 - val_loss: 1.5194 - val_acc: 0.7230\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.5734 - acc: 0.7106 - val_loss: 1.5185 - val_acc: 0.7180\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.5654 - acc: 0.7058 - val_loss: 1.5244 - val_acc: 0.7160\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5716 - acc: 0.7037 - val_loss: 1.5124 - val_acc: 0.7230\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5684 - acc: 0.7058 - val_loss: 1.5112 - val_acc: 0.7190\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5691 - acc: 0.7041 - val_loss: 1.5122 - val_acc: 0.7190\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5598 - acc: 0.7050 - val_loss: 1.5183 - val_acc: 0.7090\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.5558 - acc: 0.7046 - val_loss: 1.5099 - val_acc: 0.7260\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5728 - acc: 0.7052 - val_loss: 1.5118 - val_acc: 0.7170\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.5651 - acc: 0.7042 - val_loss: 1.4954 - val_acc: 0.7230\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 14.3295 - acc: 0.3046 - val_loss: 8.6030 - val_acc: 0.5190\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 7.2637 - acc: 0.4758 - val_loss: 6.0485 - val_acc: 0.5340\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 5.6194 - acc: 0.5104 - val_loss: 4.8297 - val_acc: 0.5500\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 4.5694 - acc: 0.5277 - val_loss: 4.0070 - val_acc: 0.5610\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 3.8655 - acc: 0.5405 - val_loss: 3.4334 - val_acc: 0.5700\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 3.3656 - acc: 0.5509 - val_loss: 3.0145 - val_acc: 0.5690\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.9879 - acc: 0.5585 - val_loss: 2.7111 - val_acc: 0.5780\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 2.7163 - acc: 0.5596 - val_loss: 2.4846 - val_acc: 0.5740\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.5074 - acc: 0.5700 - val_loss: 2.3223 - val_acc: 0.5910\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 2.3731 - acc: 0.5689 - val_loss: 2.2052 - val_acc: 0.5790\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 2.2546 - acc: 0.5729 - val_loss: 2.1170 - val_acc: 0.5910\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 2.1788 - acc: 0.5713 - val_loss: 2.0534 - val_acc: 0.5870\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.1207 - acc: 0.5803 - val_loss: 2.0093 - val_acc: 0.5940\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.0786 - acc: 0.5787 - val_loss: 1.9765 - val_acc: 0.6030\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 2.0447 - acc: 0.5881 - val_loss: 1.9555 - val_acc: 0.5890\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.0325 - acc: 0.5857 - val_loss: 1.9315 - val_acc: 0.6090\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.0047 - acc: 0.5960 - val_loss: 1.9119 - val_acc: 0.6090\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.9917 - acc: 0.5972 - val_loss: 1.9033 - val_acc: 0.6220\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9824 - acc: 0.5946 - val_loss: 1.8894 - val_acc: 0.6240\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.9729 - acc: 0.6012 - val_loss: 1.8821 - val_acc: 0.6100\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 1.9576 - acc: 0.6031 - val_loss: 1.8701 - val_acc: 0.6250\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.9586 - acc: 0.6016 - val_loss: 1.8626 - val_acc: 0.6390\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9453 - acc: 0.6107 - val_loss: 1.8568 - val_acc: 0.6260\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.9393 - acc: 0.6154 - val_loss: 1.8499 - val_acc: 0.6210\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.9336 - acc: 0.6100 - val_loss: 1.8413 - val_acc: 0.6420\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9272 - acc: 0.6171 - val_loss: 1.8364 - val_acc: 0.6440\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9199 - acc: 0.6198 - val_loss: 1.8317 - val_acc: 0.6460\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.9138 - acc: 0.6209 - val_loss: 1.8226 - val_acc: 0.6440\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9035 - acc: 0.6252 - val_loss: 1.8139 - val_acc: 0.6560\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9086 - acc: 0.6223 - val_loss: 1.8100 - val_acc: 0.6560\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9046 - acc: 0.6279 - val_loss: 1.8207 - val_acc: 0.6470\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9057 - acc: 0.6235 - val_loss: 1.8049 - val_acc: 0.6600\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8933 - acc: 0.6263 - val_loss: 1.7982 - val_acc: 0.6600\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8918 - acc: 0.6250 - val_loss: 1.8008 - val_acc: 0.6600\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.8868 - acc: 0.6272 - val_loss: 1.7969 - val_acc: 0.6510\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 1.8793 - acc: 0.6294 - val_loss: 1.7910 - val_acc: 0.6710\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.8775 - acc: 0.6315 - val_loss: 1.7973 - val_acc: 0.6530\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.8814 - acc: 0.6324 - val_loss: 1.7824 - val_acc: 0.6650\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8673 - acc: 0.6409 - val_loss: 1.7855 - val_acc: 0.6490\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8633 - acc: 0.6399 - val_loss: 1.7792 - val_acc: 0.6710\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8610 - acc: 0.6394 - val_loss: 1.7700 - val_acc: 0.6590\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 1.8671 - acc: 0.6349 - val_loss: 1.7700 - val_acc: 0.6760\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.8607 - acc: 0.6423 - val_loss: 1.7602 - val_acc: 0.6700\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.8585 - acc: 0.6437 - val_loss: 1.7625 - val_acc: 0.6650\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.8598 - acc: 0.6447 - val_loss: 1.7741 - val_acc: 0.6570\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.8610 - acc: 0.6403 - val_loss: 1.7575 - val_acc: 0.6750\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8465 - acc: 0.6452 - val_loss: 1.7494 - val_acc: 0.6800\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8519 - acc: 0.6409 - val_loss: 1.7538 - val_acc: 0.6720\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8364 - acc: 0.6447 - val_loss: 1.7501 - val_acc: 0.6730\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8403 - acc: 0.6477 - val_loss: 1.7416 - val_acc: 0.6770\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8440 - acc: 0.6468 - val_loss: 1.7515 - val_acc: 0.6740\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8424 - acc: 0.6427 - val_loss: 1.7388 - val_acc: 0.6770\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.8314 - acc: 0.6471 - val_loss: 1.7372 - val_acc: 0.6720\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8324 - acc: 0.6455 - val_loss: 1.7343 - val_acc: 0.6810\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8305 - acc: 0.6482 - val_loss: 1.7321 - val_acc: 0.6780\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8225 - acc: 0.6525 - val_loss: 1.7370 - val_acc: 0.6690\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.8299 - acc: 0.6482 - val_loss: 1.7289 - val_acc: 0.6790\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8243 - acc: 0.6542 - val_loss: 1.7281 - val_acc: 0.6770\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.8191 - acc: 0.6505 - val_loss: 1.7265 - val_acc: 0.6850\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.8173 - acc: 0.6521 - val_loss: 1.7257 - val_acc: 0.6790\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8073 - acc: 0.6506 - val_loss: 1.7129 - val_acc: 0.6850\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.8216 - acc: 0.6527 - val_loss: 1.7263 - val_acc: 0.6820\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8126 - acc: 0.6590 - val_loss: 1.7196 - val_acc: 0.6770\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8069 - acc: 0.6565 - val_loss: 1.7171 - val_acc: 0.6760\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.8108 - acc: 0.6538 - val_loss: 1.7152 - val_acc: 0.6840\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7926 - acc: 0.6574 - val_loss: 1.7042 - val_acc: 0.6830\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8014 - acc: 0.6596 - val_loss: 1.7119 - val_acc: 0.6820\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.8036 - acc: 0.6580 - val_loss: 1.7226 - val_acc: 0.6850\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.8001 - acc: 0.6626 - val_loss: 1.7097 - val_acc: 0.6820\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7903 - acc: 0.6606 - val_loss: 1.7090 - val_acc: 0.6820\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7928 - acc: 0.6587 - val_loss: 1.7060 - val_acc: 0.6870\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7922 - acc: 0.6590 - val_loss: 1.6970 - val_acc: 0.6780\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.8006 - acc: 0.6601 - val_loss: 1.7101 - val_acc: 0.6820\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7955 - acc: 0.6636 - val_loss: 1.7064 - val_acc: 0.6780\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7886 - acc: 0.6592 - val_loss: 1.6972 - val_acc: 0.6910\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7959 - acc: 0.6622 - val_loss: 1.6990 - val_acc: 0.6820\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.7889 - acc: 0.6645 - val_loss: 1.6940 - val_acc: 0.6840\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7905 - acc: 0.6602 - val_loss: 1.7046 - val_acc: 0.6770\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.7933 - acc: 0.6625 - val_loss: 1.7016 - val_acc: 0.6800\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7773 - acc: 0.6660 - val_loss: 1.6981 - val_acc: 0.6850\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.7786 - acc: 0.6605 - val_loss: 1.6949 - val_acc: 0.6870\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7807 - acc: 0.6637 - val_loss: 1.6826 - val_acc: 0.6880\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7758 - acc: 0.6636 - val_loss: 1.7047 - val_acc: 0.6750\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7839 - acc: 0.6639 - val_loss: 1.6932 - val_acc: 0.6890\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7924 - acc: 0.6626 - val_loss: 1.6833 - val_acc: 0.6870\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7787 - acc: 0.6644 - val_loss: 1.6865 - val_acc: 0.6870\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7783 - acc: 0.6642 - val_loss: 1.6859 - val_acc: 0.6850\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7753 - acc: 0.6607 - val_loss: 1.6770 - val_acc: 0.6840\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7720 - acc: 0.6666 - val_loss: 1.6856 - val_acc: 0.6920\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7739 - acc: 0.6660 - val_loss: 1.6912 - val_acc: 0.6910\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7760 - acc: 0.6685 - val_loss: 1.6776 - val_acc: 0.6860\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7589 - acc: 0.6671 - val_loss: 1.6824 - val_acc: 0.6830\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7623 - acc: 0.6668 - val_loss: 1.6817 - val_acc: 0.6870\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7635 - acc: 0.6695 - val_loss: 1.6645 - val_acc: 0.6920\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.7747 - acc: 0.6651 - val_loss: 1.6731 - val_acc: 0.6890\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 1.7695 - acc: 0.6678 - val_loss: 1.6776 - val_acc: 0.6850\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7572 - acc: 0.6678 - val_loss: 1.6759 - val_acc: 0.6900\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.7610 - acc: 0.6671 - val_loss: 1.6770 - val_acc: 0.6840\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7624 - acc: 0.6681 - val_loss: 1.6745 - val_acc: 0.6850\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.7535 - acc: 0.6642 - val_loss: 1.6672 - val_acc: 0.6890\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 3.4785 - acc: 0.2854 - val_loss: 2.6588 - val_acc: 0.4570\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 2.4818 - acc: 0.4545 - val_loss: 1.8563 - val_acc: 0.5660\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.9788 - acc: 0.5462 - val_loss: 1.6112 - val_acc: 0.6440\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.7318 - acc: 0.6039 - val_loss: 1.4750 - val_acc: 0.6880\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.5846 - acc: 0.6503 - val_loss: 1.3790 - val_acc: 0.7030\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4803 - acc: 0.6764 - val_loss: 1.3191 - val_acc: 0.7190\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4050 - acc: 0.6911 - val_loss: 1.2744 - val_acc: 0.7330\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3425 - acc: 0.7093 - val_loss: 1.2326 - val_acc: 0.7440\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2818 - acc: 0.7234 - val_loss: 1.2109 - val_acc: 0.7500\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.2482 - acc: 0.7325 - val_loss: 1.1799 - val_acc: 0.7690\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2122 - acc: 0.7410 - val_loss: 1.1607 - val_acc: 0.7730\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1560 - acc: 0.7577 - val_loss: 1.1445 - val_acc: 0.7770\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1210 - acc: 0.7640 - val_loss: 1.1340 - val_acc: 0.7800\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.0885 - acc: 0.7735 - val_loss: 1.1206 - val_acc: 0.7920\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.0727 - acc: 0.7706 - val_loss: 1.1134 - val_acc: 0.7930\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.0347 - acc: 0.7781 - val_loss: 1.1059 - val_acc: 0.7990\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.0208 - acc: 0.7858 - val_loss: 1.0955 - val_acc: 0.7970\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.9812 - acc: 0.7908 - val_loss: 1.0898 - val_acc: 0.8020\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9685 - acc: 0.7993 - val_loss: 1.0948 - val_acc: 0.8080\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9659 - acc: 0.7987 - val_loss: 1.0863 - val_acc: 0.8040\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9374 - acc: 0.8092 - val_loss: 1.0850 - val_acc: 0.8100\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9303 - acc: 0.8087 - val_loss: 1.0750 - val_acc: 0.8130\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9125 - acc: 0.8126 - val_loss: 1.0841 - val_acc: 0.8070\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.8938 - acc: 0.8162 - val_loss: 1.0708 - val_acc: 0.8180\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8748 - acc: 0.8220 - val_loss: 1.0806 - val_acc: 0.8090\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.8833 - acc: 0.8197 - val_loss: 1.0795 - val_acc: 0.8180\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8627 - acc: 0.8225 - val_loss: 1.0887 - val_acc: 0.8110\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.8757 - acc: 0.8217 - val_loss: 1.0698 - val_acc: 0.8160\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.8425 - acc: 0.8305 - val_loss: 1.0599 - val_acc: 0.8140\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8331 - acc: 0.8316 - val_loss: 1.0739 - val_acc: 0.8140\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.8338 - acc: 0.8349 - val_loss: 1.0706 - val_acc: 0.8190\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8203 - acc: 0.8344 - val_loss: 1.0660 - val_acc: 0.8220\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.8262 - acc: 0.8374 - val_loss: 1.0671 - val_acc: 0.8210\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.8083 - acc: 0.8409 - val_loss: 1.0707 - val_acc: 0.8200\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8149 - acc: 0.8371 - val_loss: 1.0679 - val_acc: 0.8240\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.8137 - acc: 0.8404 - val_loss: 1.0606 - val_acc: 0.8220\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.7959 - acc: 0.8434 - val_loss: 1.0588 - val_acc: 0.8150\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8027 - acc: 0.8413 - val_loss: 1.0599 - val_acc: 0.8190\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7946 - acc: 0.8490 - val_loss: 1.0567 - val_acc: 0.8170\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7693 - acc: 0.8504 - val_loss: 1.0634 - val_acc: 0.8200\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7942 - acc: 0.8410 - val_loss: 1.0695 - val_acc: 0.8180\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7736 - acc: 0.8500 - val_loss: 1.0579 - val_acc: 0.8240\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7628 - acc: 0.8525 - val_loss: 1.0678 - val_acc: 0.8190\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7601 - acc: 0.8555 - val_loss: 1.0710 - val_acc: 0.8230\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7688 - acc: 0.8539 - val_loss: 1.0645 - val_acc: 0.8230\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7531 - acc: 0.8533 - val_loss: 1.0716 - val_acc: 0.8210\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7448 - acc: 0.8557 - val_loss: 1.0656 - val_acc: 0.8230\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7375 - acc: 0.8587 - val_loss: 1.0743 - val_acc: 0.8230\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7279 - acc: 0.8622 - val_loss: 1.0754 - val_acc: 0.8190\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7416 - acc: 0.8617 - val_loss: 1.0642 - val_acc: 0.8210\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.7489 - acc: 0.8549 - val_loss: 1.0835 - val_acc: 0.8180\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7443 - acc: 0.8591 - val_loss: 1.0733 - val_acc: 0.8160\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7152 - acc: 0.8638 - val_loss: 1.0801 - val_acc: 0.8230\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7194 - acc: 0.8657 - val_loss: 1.0813 - val_acc: 0.8190\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7127 - acc: 0.8626 - val_loss: 1.0819 - val_acc: 0.8190\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7169 - acc: 0.8647 - val_loss: 1.0805 - val_acc: 0.8140\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7241 - acc: 0.8643 - val_loss: 1.0895 - val_acc: 0.8180\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7191 - acc: 0.8673 - val_loss: 1.0817 - val_acc: 0.8220\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7159 - acc: 0.8647 - val_loss: 1.0774 - val_acc: 0.8250\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7164 - acc: 0.8678 - val_loss: 1.0812 - val_acc: 0.8220\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7277 - acc: 0.8629 - val_loss: 1.0823 - val_acc: 0.8280\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6974 - acc: 0.8720 - val_loss: 1.0748 - val_acc: 0.8230\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7047 - acc: 0.8705 - val_loss: 1.0784 - val_acc: 0.8270\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6978 - acc: 0.8730 - val_loss: 1.0973 - val_acc: 0.8200\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7091 - acc: 0.8667 - val_loss: 1.0843 - val_acc: 0.8220\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6998 - acc: 0.8696 - val_loss: 1.0818 - val_acc: 0.8240\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6984 - acc: 0.8703 - val_loss: 1.0802 - val_acc: 0.8270\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7031 - acc: 0.8750 - val_loss: 1.0824 - val_acc: 0.8210\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6811 - acc: 0.8748 - val_loss: 1.0922 - val_acc: 0.8270\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6863 - acc: 0.8780 - val_loss: 1.0801 - val_acc: 0.8260\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6900 - acc: 0.8741 - val_loss: 1.0818 - val_acc: 0.8290\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6889 - acc: 0.8767 - val_loss: 1.0802 - val_acc: 0.8240\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6951 - acc: 0.8726 - val_loss: 1.0793 - val_acc: 0.8250\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6762 - acc: 0.8799 - val_loss: 1.0798 - val_acc: 0.8260\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6910 - acc: 0.8775 - val_loss: 1.0917 - val_acc: 0.8240\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6909 - acc: 0.8747 - val_loss: 1.0846 - val_acc: 0.8280\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6764 - acc: 0.8771 - val_loss: 1.0887 - val_acc: 0.8270\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6812 - acc: 0.8768 - val_loss: 1.0972 - val_acc: 0.8250\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6682 - acc: 0.8797 - val_loss: 1.0851 - val_acc: 0.8280\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6842 - acc: 0.8799 - val_loss: 1.0764 - val_acc: 0.8340\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6739 - acc: 0.8805 - val_loss: 1.0920 - val_acc: 0.8310\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6662 - acc: 0.8820 - val_loss: 1.0933 - val_acc: 0.8250\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.6678 - acc: 0.8830 - val_loss: 1.0872 - val_acc: 0.8330\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6796 - acc: 0.8767 - val_loss: 1.0908 - val_acc: 0.8270\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6695 - acc: 0.8801 - val_loss: 1.0769 - val_acc: 0.8310\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6573 - acc: 0.8825 - val_loss: 1.0845 - val_acc: 0.8390\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6766 - acc: 0.8751 - val_loss: 1.0753 - val_acc: 0.8340\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6589 - acc: 0.8795 - val_loss: 1.0782 - val_acc: 0.8310\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.6673 - acc: 0.8841 - val_loss: 1.0879 - val_acc: 0.8320\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.6734 - acc: 0.8789 - val_loss: 1.0666 - val_acc: 0.8350\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6570 - acc: 0.8865 - val_loss: 1.0773 - val_acc: 0.8360\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6648 - acc: 0.8814 - val_loss: 1.0804 - val_acc: 0.8320\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6581 - acc: 0.8819 - val_loss: 1.0829 - val_acc: 0.8260\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6642 - acc: 0.8804 - val_loss: 1.0893 - val_acc: 0.8260\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6586 - acc: 0.8847 - val_loss: 1.1025 - val_acc: 0.8250\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6497 - acc: 0.8816 - val_loss: 1.1014 - val_acc: 0.8250\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6557 - acc: 0.8846 - val_loss: 1.0908 - val_acc: 0.8230\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6462 - acc: 0.8825 - val_loss: 1.0898 - val_acc: 0.8270\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.6594 - acc: 0.8844 - val_loss: 1.0720 - val_acc: 0.8260\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6417 - acc: 0.8885 - val_loss: 1.0968 - val_acc: 0.8280\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 3.5306 - acc: 0.2632 - val_loss: 2.9105 - val_acc: 0.4710\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 2.5025 - acc: 0.4897 - val_loss: 1.8527 - val_acc: 0.5700\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.8360 - acc: 0.5827 - val_loss: 1.5124 - val_acc: 0.6560\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.5537 - acc: 0.6451 - val_loss: 1.3473 - val_acc: 0.6940\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.4000 - acc: 0.6800 - val_loss: 1.2651 - val_acc: 0.7070\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.2813 - acc: 0.7051 - val_loss: 1.2007 - val_acc: 0.7160\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1936 - acc: 0.7190 - val_loss: 1.1471 - val_acc: 0.7330\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1085 - acc: 0.7325 - val_loss: 1.1088 - val_acc: 0.7470\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0443 - acc: 0.7444 - val_loss: 1.0751 - val_acc: 0.7670\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9789 - acc: 0.7679 - val_loss: 1.0481 - val_acc: 0.7730\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9065 - acc: 0.7784 - val_loss: 1.0221 - val_acc: 0.7820\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8602 - acc: 0.7939 - val_loss: 0.9933 - val_acc: 0.8020\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.8102 - acc: 0.8076 - val_loss: 0.9850 - val_acc: 0.8040\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7646 - acc: 0.8172 - val_loss: 0.9659 - val_acc: 0.8020\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7342 - acc: 0.8196 - val_loss: 0.9468 - val_acc: 0.8110\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7072 - acc: 0.8264 - val_loss: 0.9412 - val_acc: 0.8140\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6598 - acc: 0.8381 - val_loss: 0.9365 - val_acc: 0.8130\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6386 - acc: 0.8419 - val_loss: 0.9338 - val_acc: 0.8190\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.6031 - acc: 0.8507 - val_loss: 0.9290 - val_acc: 0.8250\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.5914 - acc: 0.8495 - val_loss: 0.9226 - val_acc: 0.8240\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.5723 - acc: 0.8584 - val_loss: 0.9244 - val_acc: 0.8250\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.5491 - acc: 0.8633 - val_loss: 0.9263 - val_acc: 0.8200\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.5217 - acc: 0.8661 - val_loss: 0.9283 - val_acc: 0.8190\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4976 - acc: 0.8737 - val_loss: 0.9411 - val_acc: 0.8210\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.5008 - acc: 0.8682 - val_loss: 0.9275 - val_acc: 0.8220\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4801 - acc: 0.8763 - val_loss: 0.9457 - val_acc: 0.8240\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4648 - acc: 0.8786 - val_loss: 0.9542 - val_acc: 0.8240\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4433 - acc: 0.8908 - val_loss: 0.9601 - val_acc: 0.8220\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4471 - acc: 0.8859 - val_loss: 0.9443 - val_acc: 0.8240\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4311 - acc: 0.8871 - val_loss: 0.9592 - val_acc: 0.8240\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4190 - acc: 0.8911 - val_loss: 0.9587 - val_acc: 0.8280\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.4172 - acc: 0.8926 - val_loss: 0.9540 - val_acc: 0.8250\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3939 - acc: 0.9004 - val_loss: 0.9666 - val_acc: 0.8270\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.3831 - acc: 0.9020 - val_loss: 0.9825 - val_acc: 0.8260\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.3878 - acc: 0.9008 - val_loss: 0.9820 - val_acc: 0.8260\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.3672 - acc: 0.9044 - val_loss: 0.9841 - val_acc: 0.8260\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.3624 - acc: 0.9065 - val_loss: 0.9928 - val_acc: 0.8240\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.3644 - acc: 0.9060 - val_loss: 1.0018 - val_acc: 0.8260\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3617 - acc: 0.9064 - val_loss: 0.9916 - val_acc: 0.8240\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.3542 - acc: 0.9090 - val_loss: 1.0010 - val_acc: 0.8280\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.3347 - acc: 0.9164 - val_loss: 1.0151 - val_acc: 0.8220\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3441 - acc: 0.9129 - val_loss: 1.0222 - val_acc: 0.8250\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.3277 - acc: 0.9116 - val_loss: 1.0388 - val_acc: 0.8260\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.3299 - acc: 0.9100 - val_loss: 1.0310 - val_acc: 0.8290\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3258 - acc: 0.9167 - val_loss: 1.0294 - val_acc: 0.8280\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.3099 - acc: 0.9162 - val_loss: 1.0456 - val_acc: 0.8240\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3120 - acc: 0.9182 - val_loss: 1.0486 - val_acc: 0.8260\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.3128 - acc: 0.9159 - val_loss: 1.0515 - val_acc: 0.8270\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3059 - acc: 0.9171 - val_loss: 1.0600 - val_acc: 0.8250\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2959 - acc: 0.9226 - val_loss: 1.0515 - val_acc: 0.8320\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2931 - acc: 0.9218 - val_loss: 1.0695 - val_acc: 0.8320\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2998 - acc: 0.9217 - val_loss: 1.0720 - val_acc: 0.8280\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2905 - acc: 0.9241 - val_loss: 1.0702 - val_acc: 0.8270\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2980 - acc: 0.9208 - val_loss: 1.0592 - val_acc: 0.8280\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2824 - acc: 0.9267 - val_loss: 1.0807 - val_acc: 0.8240\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2848 - acc: 0.9241 - val_loss: 1.0911 - val_acc: 0.8240\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2791 - acc: 0.9257 - val_loss: 1.0855 - val_acc: 0.8230\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2763 - acc: 0.9262 - val_loss: 1.0979 - val_acc: 0.8230\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.2771 - acc: 0.9268 - val_loss: 1.0996 - val_acc: 0.8200\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2687 - acc: 0.9313 - val_loss: 1.0901 - val_acc: 0.8210\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2678 - acc: 0.9306 - val_loss: 1.0934 - val_acc: 0.8250\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.2727 - acc: 0.9271 - val_loss: 1.0860 - val_acc: 0.8280\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2651 - acc: 0.9273 - val_loss: 1.0925 - val_acc: 0.8240\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2594 - acc: 0.9313 - val_loss: 1.0994 - val_acc: 0.8280\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2724 - acc: 0.9305 - val_loss: 1.0928 - val_acc: 0.8280\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2504 - acc: 0.9377 - val_loss: 1.0989 - val_acc: 0.8280\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2621 - acc: 0.9301 - val_loss: 1.1219 - val_acc: 0.8260\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2505 - acc: 0.9334 - val_loss: 1.1271 - val_acc: 0.8220\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2538 - acc: 0.9313 - val_loss: 1.1256 - val_acc: 0.8210\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2492 - acc: 0.9351 - val_loss: 1.1203 - val_acc: 0.8270\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2510 - acc: 0.9330 - val_loss: 1.1293 - val_acc: 0.8260\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2546 - acc: 0.9317 - val_loss: 1.1322 - val_acc: 0.8260\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2584 - acc: 0.9281 - val_loss: 1.1400 - val_acc: 0.8200\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.2494 - acc: 0.9336 - val_loss: 1.1385 - val_acc: 0.8230\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2472 - acc: 0.9340 - val_loss: 1.1542 - val_acc: 0.8250\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2494 - acc: 0.9326 - val_loss: 1.1493 - val_acc: 0.8210\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2328 - acc: 0.9394 - val_loss: 1.1673 - val_acc: 0.8220\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2489 - acc: 0.9339 - val_loss: 1.1705 - val_acc: 0.8210\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2410 - acc: 0.9344 - val_loss: 1.1560 - val_acc: 0.8190\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2409 - acc: 0.9366 - val_loss: 1.1558 - val_acc: 0.8220\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2436 - acc: 0.9351 - val_loss: 1.1889 - val_acc: 0.8160\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2492 - acc: 0.9303 - val_loss: 1.1713 - val_acc: 0.8170\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2344 - acc: 0.9382 - val_loss: 1.1753 - val_acc: 0.8200\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2395 - acc: 0.9359 - val_loss: 1.1676 - val_acc: 0.8190\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2259 - acc: 0.9399 - val_loss: 1.1819 - val_acc: 0.8200\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2416 - acc: 0.9365 - val_loss: 1.1763 - val_acc: 0.8230\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2351 - acc: 0.9362 - val_loss: 1.1678 - val_acc: 0.8160\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.2359 - acc: 0.9395 - val_loss: 1.1822 - val_acc: 0.8180\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.2319 - acc: 0.9386 - val_loss: 1.1686 - val_acc: 0.8140\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2393 - acc: 0.9344 - val_loss: 1.1613 - val_acc: 0.8190\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2377 - acc: 0.9357 - val_loss: 1.1849 - val_acc: 0.8180\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2302 - acc: 0.9414 - val_loss: 1.1862 - val_acc: 0.8180\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2197 - acc: 0.9397 - val_loss: 1.1997 - val_acc: 0.8170\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2326 - acc: 0.9354 - val_loss: 1.1910 - val_acc: 0.8190\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2283 - acc: 0.9395 - val_loss: 1.1846 - val_acc: 0.8230\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2239 - acc: 0.9397 - val_loss: 1.2018 - val_acc: 0.8250\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2173 - acc: 0.9422 - val_loss: 1.2283 - val_acc: 0.8170\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2371 - acc: 0.9342 - val_loss: 1.2099 - val_acc: 0.8130\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2186 - acc: 0.9397 - val_loss: 1.1990 - val_acc: 0.8110\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2288 - acc: 0.9386 - val_loss: 1.2041 - val_acc: 0.8190\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTV0LaGxjyvN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "3b0b8530-a2ea-4a21-de48-6cfa68e37eec"
      },
      "source": [
        "plt.scatter(dropout_rates[0:8], val_acc[0:8])\n",
        "plt.show()\n",
        "plt.scatter(l2_penalties[0:8], val_acc[0:8])\n",
        "plt.show()\n",
        "plt.scatter(num_units[0:8], val_acc[0:8])\n",
        "plt.show()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAJiZJcoBYGaWqP8Hbv1G+35yCAwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngJlZov4JpgSPjNOFQRIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD8CAYAAACYebj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAXcklEQVR4nO3df4xdZ53f8fenzi+DGpwQs0qcX0aY\nhJBt480lC0JFXUJigyh2t5TaogXaaCO0TbabIpdYIJamIC1KV1GpAitTQgBBvKmbGKubrMNusj+E\nkuDrdTaOnXUYEkE8zsLww1BtvPlhvv3jnjncjMee65mJ74z9fklXc89znvP4eebO3I/Pc547J1WF\nJEkA/2jYHZAkzR2GgiSpZShIklqGgiSpZShIklqGgiSpNVAoJFmZZE+SkSQ3TrL//CQPJNmR5NEk\n72rKL0xyIMkjzeMP+465PMnOps3PJsnsDUuSNB2Z6nMKSRYATwBXAXuBbcDaqtrdV2cDsKOqPp/k\nEuCeqrowyYXA/62qSydp99vA7wAPA/cAn62qe2dlVJKkaRnkTOEKYKSqnqyq54GNwKoJdQo4vXn+\nKmDfkRpMcjZwelU9VL1U+gqw+qh6LkmadScNUGcJ8HTf9l7g1yfU+SRwX5LrgVcC7+jbtzTJDuDn\nwMer6q+aNvdOaHPJVB0566yz6sILLxygy5Kkcdu3b/9RVS0epO4goTCItcDtVfUHSd4CfDXJpcAz\nwPlV9eMklwObk7zxaBpOci1wLcD5559Pt9udpS5L0okhyfcGrTvI9NEocF7f9rlNWb9rgDsBqupB\n4DTgrKp6rqp+3JRvB74LvL45/twp2qQ5bkNVdaqqs3jxQEEnSZqmQUJhG7AsydIkpwBrgC0T6nwf\nuBIgyRvohcJYksXNhWqSvBZYBjxZVc8AP0/y5mbV0QeAb8zKiCRJ0zbl9FFVvZjkOmArsAC4rap2\nJbkJ6FbVFuAjwBeS3EDvovOHqqqSvA24KckLwC+AD1fVT5qmfxu4HVgI3Ns8JElDNOWS1Lmk0+mU\n1xQk6egk2V5VnUHq+olmSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAk\ntQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktaa8R7M0XZt3jHLz1j3s23+AcxYt\nZN2Ki1i9fMmwuyXpCAwFvSw27xhl/V07OfDCQQBG9x9g/V07AQwGaQ5z+kgvi5u37mkDYdyBFw5y\n89Y9Q+qRpEEMFApJVibZk2QkyY2T7D8/yQNJdiR5NMm7mvKrkmxPsrP5+va+Y/68afOR5vGa2RuW\nhm3f/gNHVS5pbphy+ijJAuBW4CpgL7AtyZaq2t1X7ePAnVX1+SSXAPcAFwI/Av5FVe1LcimwFeif\nO3h/VXVnZyiaS85ZtJDRSQLgnEULh9AbSYMa5EzhCmCkqp6squeBjcCqCXUKOL15/ipgH0BV7aiq\nfU35LmBhklNn3m3NdetWXMTCkxe8pGzhyQtYt+KiIfVI0iAGudC8BHi6b3sv8OsT6nwSuC/J9cAr\ngXdM0s6/Av66qp7rK/tSkoPA/wE+VVU18aAk1wLXApx//vkDdFdzwfjFZFcfSfPLbK0+WgvcXlV/\nkOQtwFeTXFpVvwBI8kbgM8DVfce8v6pGk/xjeqHw74CvTGy4qjYAGwA6nc4hoaG5a/XyJYbALHOZ\nr15ug0wfjQLn9W2f25T1uwa4E6CqHgROA84CSHIucDfwgar67vgBVTXafP1/wNfpTVNJOozxZb6j\n+w9Q/HKZ7+YdE38dpekbJBS2AcuSLE1yCrAG2DKhzveBKwGSvIFeKIwlWQT8MXBjVX1rvHKSk5KM\nh8bJwLuBx2Y6GOl45jJfHQtTTh9V1YtJrqO3cmgBcFtV7UpyE9Ctqi3AR4AvJLmB3kXnD1VVNce9\nDvhEkk80TV4N/D2wtQmEBcCfAl+Y7cGBp9sz5fdv7nCZ74lnGL9/A11TqKp76C0z7S/7RN/z3cBb\nJznuU8CnDtPs5YN3c3r8VO3M+P2bW1zme2IZ1u/fcf2JZk+3Z8bv39ziMt8Ty7B+/47rv33k6fbM\n+P07vGGc1rvM98QyrN+/4zoUPN2eGb9/kxvmtJrLfE8cw/r9O66njzzdnhm/f5NzWk3HwrB+/47r\nMwVPt2fG79/knFbTsTCs379M8pcl5qxOp1Pdrn8/T8P11t+/f9LT+iWLFvKtG98+yRHScCXZXlWd\nQeoe19NH0svBaTUdz47r6SPp5eC0mo5nhoI0Da4C0vHK6SNJUstQkCS1DAVJUstQkCS1DAVJUstQ\nkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1BgqFJCuT7EkykuTGSfafn+SBJDuSPJrkXX37\n1jfH7UmyYtA2JUnH3pShkGQBcCvwTuASYG2SSyZU+zhwZ1UtB9YAn2uOvaTZfiOwEvhckgUDtilJ\nOsYGOVO4Ahipqier6nlgI7BqQp0CTm+evwrY1zxfBWysqueq6ilgpGlvkDYlScfYIKGwBHi6b3tv\nU9bvk8C/TbIXuAe4fopjB2kTgCTXJukm6Y6NjQ3QXUnSdM3Whea1wO1VdS7wLuCrSWal7araUFWd\nquosXrx4NpqUJB3GIHdeGwXO69s+tynrdw29awZU1YNJTgPOmuLYqdqUJB1jg/xvfhuwLMnSJKfQ\nu3C8ZUKd7wNXAiR5A3AaMNbUW5Pk1CRLgWXAtwdsU5J0jE15plBVLya5DtgKLABuq6pdSW4CulW1\nBfgI8IUkN9C76PyhqipgV5I7gd3Ai8B/rKqDAJO1+TKMT5J0FNJ7754fOp1OdbvdYXdDkuaVJNur\nqjNIXT/RLElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqS\npJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpNZAoZBkZZI9SUaS3DjJ/luSPNI8\nnkiyvyn/jb7yR5L8Q5LVzb7bkzzVt++y2R2aJOlonTRVhSQLgFuBq4C9wLYkW6pq93idqrqhr/71\nwPKm/AHgsqb8TGAEuK+v+XVVtWkWxiFJmgWDnClcAYxU1ZNV9TywEVh1hPprgTsmKX8vcG9VPXv0\n3ZQkHQuDhMIS4Om+7b1N2SGSXAAsBe6fZPcaDg2LTyd5tJl+OnWAvkiSXkazfaF5DbCpqg72FyY5\nG/hVYGtf8XrgYuBNwJnARydrMMm1SbpJumNjY7PcXUlSv0FCYRQ4r2/73KZsMpOdDQC8D7i7ql4Y\nL6iqZ6rnOeBL9KapDlFVG6qqU1WdxYsXD9BdSdJ0DRIK24BlSZYmOYXeG/+WiZWSXAycATw4SRuH\nXGdozh5IEmA18NjRdV2SNNumXH1UVS8muY7e1M8C4Laq2pXkJqBbVeMBsQbYWFXVf3ySC+mdafzF\nhKa/lmQxEOAR4MMzGYgkaeYy4T18Tut0OtXtdofdDUmaV5Jsr6rOIHX9RLMkqWUoSJJahoIkqWUo\nSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJa\nhoIkqWUoSJJahoIkqWUoSJJahoIkqTVQKCRZmWRPkpEkN06y/5YkjzSPJ5Ls79t3sG/flr7ypUke\nbtr8oySnzM6QJEnTNWUoJFkA3Aq8E7gEWJvkkv46VXVDVV1WVZcB/xO4q2/3gfF9VfWevvLPALdU\n1euAnwLXzHAskqQZGuRM4QpgpKqerKrngY3AqiPUXwvccaQGkwR4O7CpKfoysHqAvkiSXkaDhMIS\n4Om+7b1N2SGSXAAsBe7vKz4tSTfJQ0nG3/hfDeyvqhcHaPPa5vju2NjYAN2VJE3XSbPc3hpgU1Ud\n7Cu7oKpGk7wWuD/JTuBngzZYVRuADQCdTqdmtbeSpJcY5ExhFDivb/vcpmwya5gwdVRVo83XJ4E/\nB5YDPwYWJRkPpSO1KUk6RgYJhW3Asma10Cn03vi3TKyU5GLgDODBvrIzkpzaPD8LeCuwu6oKeAB4\nb1P1g8A3ZjIQSdLMTRkKzbz/dcBW4HHgzqraleSmJP2ridYAG5s3/HFvALpJ/oZeCPx+Ve1u9n0U\n+M9JRuhdY/jizIcjSZqJvPQ9fG7rdDrV7XaH3Q1JmleSbK+qziB1/USzJKllKEiSWoaCJKllKEiS\nWoaCJKllKEiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKllKEiSWoaC\nJKllKEiSWoaCJKllKEiSWoaCJKk1UCgkWZlkT5KRJDdOsv+WJI80jyeS7G/KL0vyYJJdSR5N8m/6\njrk9yVN9x102e8OSJE3HSVNVSLIAuBW4CtgLbEuypap2j9epqhv66l8PLG82nwU+UFXfSXIOsD3J\n1qra3+xfV1WbZmkskqQZmjIUgCuAkap6EiDJRmAVsPsw9dcCvwdQVU+MF1bVviQ/BBYD+w9zrHTM\nbN4xys1b97Bv/wHOWbSQdSsuYvXyJcPuljRUg0wfLQGe7tve25QdIskFwFLg/kn2XQGcAny3r/jT\nzbTSLUlOPUyb1ybpJumOjY0N0F1papt3jLL+rp2M7j9AAaP7D7D+rp1s3jE67K5JQzXbF5rXAJuq\n6mB/YZKzga8C/76qftEUrwcuBt4EnAl8dLIGq2pDVXWqqrN48eJZ7q5OVDdv3cOBF17yY8qBFw5y\n89Y9Q+qRNDcMEgqjwHl92+c2ZZNZA9zRX5DkdOCPgY9V1UPj5VX1TPU8B3yJ3jSVdEzs23/gqMql\nE8UgobANWJZkaZJT6L3xb5lYKcnFwBnAg31lpwB3A1+ZeEG5OXsgSYDVwGPTHYR0tM5ZtPCoyqUT\nxZShUFUvAtcBW4HHgTuraleSm5K8p6/qGmBjVVVf2fuAtwEfmmTp6deS7AR2AmcBn5qF8UgDWbfi\nIhaevOAlZQtPXsC6FRcNqUfS3JCXvofPbZ1Op7rd7rC7oeOEq490okiyvao6g9QdZEmqdFxavXyJ\nISBN4J+5kCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJ\nUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1BgqFJCuT7EkykuTGSfbfkuSR5vFEkv19\n+z6Y5DvN44N95Zcn2dm0+dkkmZ0hSZKm66SpKiRZANwKXAXsBbYl2VJVu8frVNUNffWvB5Y3z88E\nfg/oAAVsb479KfB54LeAh4F7gJXAvbM0LknSNAxypnAFMFJVT1bV88BGYNUR6q8F7mierwC+WVU/\naYLgm8DKJGcDp1fVQ1VVwFeA1dMehSRpVgwSCkuAp/u29zZlh0hyAbAUuH+KY5c0zwdp89ok3STd\nsbGxAborSZqu2b7QvAbYVFUHZ6vBqtpQVZ2q6ixevHi2mpUkTWKQUBgFzuvbPrcpm8wafjl1dKRj\nR5vng7QpSTpGBgmFbcCyJEuTnELvjX/LxEpJLgbOAB7sK94KXJ3kjCRnAFcDW6vqGeDnSd7crDr6\nAPCNGY5FkjRDU64+qqoXk1xH7w1+AXBbVe1KchPQrarxgFgDbGwuHI8f+5Mk/41esADcVFU/aZ7/\nNnA7sJDeqiNXHknSkKXvPXzO63Q61e12h90NSZpXkmyvqs4gdf1EsySpZShIklqGgiSpZShIklqG\ngiSpNeWSVEnavGOUm7fuYd/+A5yzaCHrVlzE6uWT/mUazXOGgqQj2rxjlPV37eTAC72/XjO6/wDr\n79oJYDAch5w+knREN2/d0wbCuAMvHOTmrXuG1CO9nAwFSUe0b/+BoyrX/GYoSDqicxYtPKpyzW+G\ngqQjWrfiIhaevOAlZQtPXsC6FRcNqUd6OXmhWdIRjV9MdvXRicFQkDSl1cuXGAInCKePJEktQ0GS\n1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEktQ0GS1BooFJKsTLInyUiSGw9T531JdifZleTrTdlvJHmk\n7/EPSVY3+25P8lTfvstmb1iSpOmY8hPNSRYAtwJXAXuBbUm2VNXuvjrLgPXAW6vqp0leA1BVDwCX\nNXXOBEaA+/qaX1dVm2ZrMJKkmRnkTOEKYKSqnqyq54GNwKoJdX4LuLWqfgpQVT+cpJ33AvdW1bMz\n6bAk6eUzSCgsAZ7u297blPV7PfD6JN9K8lCSlZO0swa4Y0LZp5M8muSWJKdO9o8nuTZJN0l3bGxs\ngO5KkqZrti40nwQsA/45sBb4QpJF4zuTnA38KrC175j1wMXAm4AzgY9O1nBVbaiqTlV1Fi9ePEvd\nlSRNZpBQGAXO69s+tynrtxfYUlUvVNVTwBP0QmLc+4C7q+qF8YKqeqZ6ngO+RG+aSpI0RIOEwjZg\nWZKlSU6hNw20ZUKdzfTOEkhyFr3ppCf79q9lwtRRc/ZAkgCrgcem0X9J0iyacvVRVb2Y5Dp6Uz8L\ngNuqaleSm4BuVW1p9l2dZDdwkN6qoh8DJLmQ3pnGX0xo+mtJFgMBHgE+PDtDkiRNV6pq2H0YWKfT\nqW63O+xuSNK8kmR7VXUGqesnmiVJLUNBktQyFCRJLUNBktSaVxeak4wB3xvCP30W8KMh/LuzzXHM\nLY5j7jgexgCHH8cFVTXQp3/nVSgMS5LuoFfu5zLHMbc4jrnjeBgDzM44nD6SJLUMBUlSy1AYzIZh\nd2CWOI65xXHMHcfDGGAWxuE1BUlSyzMFSVLLUJggyaIkm5L8bZLHk7wlyZlJvpnkO83XM4bdzyNJ\nctGEe2P/PMnvzrdxACS5obnv92NJ7khyWvMXex9u7hn+R81f753TkvynZgy7kvxuUzbnX48ktyX5\nYZLH+som7Xd6Ptu8Lo8m+bXh9fylDjOOf928Hr9I0plQf30zjj1JVhz7Hk/uMOO4uXm/ejTJ3RPu\nZXPU4zAUDvU/gD+pqouBfwo8DtwI/FlVLQP+rNmes6pqT1VdVlWXAZcDzwJ3M8/GkWQJ8DtAp6ou\npfdXetcAnwFuqarXAT8FrhleL6eW5FJ6t6y9gt7P1LuTvI758XrcDky8k+Lh+v1OevdRWQZcC3z+\nGPVxELdz6DgeA34T+Mv+wiSX0Ps5e2NzzOeae9XPBbdz6Di+CVxaVf+E3r1s1sP0x2Eo9EnyKuBt\nwBcBqur5qtpP757UX26qfZne/R/miyuB71bV95if4zgJWJjkJOAVwDPA24FNzf75MI43AA9X1bNV\n9SK9PyP/m8yD16Oq/hL4yYTiw/V7FfCV5uZZDwGLxu+bMmyTjaOqHq+qPZNUXwVsrKrnmpuGjTBH\nbgJ2mHHc1/xcATxE70ZoMM1xGAovtRQYA76UZEeS/5XklcCvVNUzTZ2/A35laD08ev33xp5X46iq\nUeC/A9+nFwY/A7YD+/t+CSa7Z/hc8xjwz5K8OskrgHfRu8fIvHo9+hyu34Pcz30+mM/j+A/Avc3z\naY3DUHipk4BfAz5fVcuBv2fCKX31lmvNiyVbzVz7e4D/PXHffBhHM1e9il5YnwO8kkNPnee8qnqc\n3pTXfcCf0Lup1MEJdeb86zGZ+drv41GSjwEvAl+bSTuGwkvtBfZW1cPN9iZ6IfGDvtuHng38cEj9\nO1rvBP66qn7QbM+3cbwDeKqqxpr7e98FvJXetMT4XQMnu2f4nFNVX6yqy6vqbfSugzzB/Hs9xh2u\n34Pcz30+mHfjSPIh4N3A++uXnzOY1jgMhT5V9XfA00kuaoquBHbTuyf1B5uyDwLfGEL3pmPivbHn\n2zi+D7w5ySuae3mPvx4PAO9t6syHcZDkNc3X8+ldT/g68+/1GHe4fm8BPtCsQnoz8LO+aab5ZAuw\nJsmpSZbSu3D+7SH36bCSrAT+C/Ceqnq2b9f0xlFVPvoewGVAF3gU2AycAbya3iqL7wB/Cpw57H4O\nMI5XAj8GXtVXNh/H8V+Bv6U3L/9V4FTgtc0P9wi9qbFTh93PAcbxV/QC7W+AK+fL60HvPxXPAC/Q\nO5O+5nD9pne/9VuB7wI76a0aG/oYjjCOf9k8fw74AbC1r/7HmnHsAd457P5PMY4RetcOHmkefziT\ncfiJZklSy+kjSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktf4/ViAAjHxa42QAAAAA\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr7Q2mVOTdsu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b8da787d-eaa9-437c-e670-49f083829b9b"
      },
      "source": [
        "np.random.seed(6473)\n",
        "dropout_rates = np.random.uniform(low = 0.0, high = 0.5, size = (8,))\n",
        "l2_penalties_exp = np.random.uniform(low = -6, high = -1, size = (8,))\n",
        "num_layers = np.random.uniform(low = 1, high = 5, size = (8,)).astype(int)\n",
        "l2_penalties = 10**l2_penalties_exp\n",
        "\n",
        "val_acc = np.zeros((8,))\n",
        "\n",
        "for (i, (dropout_rate, l2_penalty, num_layer)) in enumerate(zip(dropout_rates, l2_penalties, num_layers)):\n",
        "  model = models.Sequential()\n",
        "\n",
        "  model.add(layers.Dropout(rate = dropout_rate))\n",
        "\n",
        "  for j in range(num_layer):\n",
        "    model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l = l2_penalty), input_shape=(10000,)))\n",
        "    model.add(layers.Dropout(rate = dropout_rate))\n",
        "  \n",
        "  model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  history = model.fit(partial_x_train,\n",
        "                      partial_y_train,\n",
        "                      epochs=100,\n",
        "                      batch_size=512,\n",
        "                      validation_data=(x_val, y_val))\n",
        "  \n",
        "  val_acc[i] = history.history['val_acc'][-1]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "7982/7982 [==============================] - 11s 1ms/step - loss: 3.4302 - acc: 0.2088 - val_loss: 2.5174 - val_acc: 0.3540\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 2.4533 - acc: 0.3723 - val_loss: 1.9940 - val_acc: 0.5240\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.1067 - acc: 0.4540 - val_loss: 1.7437 - val_acc: 0.5440\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.9125 - acc: 0.5142 - val_loss: 1.6735 - val_acc: 0.5880\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.8397 - acc: 0.5385 - val_loss: 1.6187 - val_acc: 0.6050\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.7492 - acc: 0.5675 - val_loss: 1.5794 - val_acc: 0.6110\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.6844 - acc: 0.5844 - val_loss: 1.5434 - val_acc: 0.6320\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.6153 - acc: 0.6037 - val_loss: 1.4907 - val_acc: 0.6550\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.5629 - acc: 0.6131 - val_loss: 1.4644 - val_acc: 0.6730\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.5375 - acc: 0.6221 - val_loss: 1.4500 - val_acc: 0.6770\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.4710 - acc: 0.6432 - val_loss: 1.4134 - val_acc: 0.6880\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.4536 - acc: 0.6498 - val_loss: 1.4030 - val_acc: 0.6940\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.4235 - acc: 0.6570 - val_loss: 1.3858 - val_acc: 0.6970\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.3793 - acc: 0.6684 - val_loss: 1.3727 - val_acc: 0.6980\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.3652 - acc: 0.6775 - val_loss: 1.3692 - val_acc: 0.6990\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.3286 - acc: 0.6840 - val_loss: 1.3595 - val_acc: 0.7020\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.3102 - acc: 0.6847 - val_loss: 1.3562 - val_acc: 0.7050\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.2783 - acc: 0.6928 - val_loss: 1.3594 - val_acc: 0.6980\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.2569 - acc: 0.6977 - val_loss: 1.3557 - val_acc: 0.7030\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.2506 - acc: 0.6941 - val_loss: 1.3536 - val_acc: 0.7030\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.2253 - acc: 0.7028 - val_loss: 1.3409 - val_acc: 0.7090\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.2185 - acc: 0.7045 - val_loss: 1.3382 - val_acc: 0.7060\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.2052 - acc: 0.7077 - val_loss: 1.3406 - val_acc: 0.7090\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.1901 - acc: 0.7096 - val_loss: 1.3320 - val_acc: 0.7100\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.1701 - acc: 0.7139 - val_loss: 1.3301 - val_acc: 0.7210\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.1708 - acc: 0.7196 - val_loss: 1.3347 - val_acc: 0.7230\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.1402 - acc: 0.7221 - val_loss: 1.3323 - val_acc: 0.7280\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.1417 - acc: 0.7228 - val_loss: 1.3501 - val_acc: 0.7310\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.1396 - acc: 0.7235 - val_loss: 1.3267 - val_acc: 0.7310\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.1189 - acc: 0.7305 - val_loss: 1.3337 - val_acc: 0.7340\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.1199 - acc: 0.7281 - val_loss: 1.3270 - val_acc: 0.7310\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.1088 - acc: 0.7308 - val_loss: 1.3091 - val_acc: 0.7320\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.1112 - acc: 0.7355 - val_loss: 1.3181 - val_acc: 0.7350\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.0973 - acc: 0.7364 - val_loss: 1.3235 - val_acc: 0.7310\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.0929 - acc: 0.7397 - val_loss: 1.3092 - val_acc: 0.7440\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.0765 - acc: 0.7397 - val_loss: 1.3139 - val_acc: 0.7450\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.0744 - acc: 0.7440 - val_loss: 1.3214 - val_acc: 0.7350\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.0553 - acc: 0.7442 - val_loss: 1.3188 - val_acc: 0.7410\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.0484 - acc: 0.7466 - val_loss: 1.3173 - val_acc: 0.7470\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0549 - acc: 0.7473 - val_loss: 1.3211 - val_acc: 0.7460\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.0345 - acc: 0.7461 - val_loss: 1.3025 - val_acc: 0.7440\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.0392 - acc: 0.7498 - val_loss: 1.3222 - val_acc: 0.7420\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.0408 - acc: 0.7508 - val_loss: 1.3037 - val_acc: 0.7500\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.0175 - acc: 0.7544 - val_loss: 1.3025 - val_acc: 0.7440\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.0088 - acc: 0.7581 - val_loss: 1.3103 - val_acc: 0.7440\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.0065 - acc: 0.7554 - val_loss: 1.3190 - val_acc: 0.7430\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.0162 - acc: 0.7573 - val_loss: 1.3170 - val_acc: 0.7460\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.0034 - acc: 0.7580 - val_loss: 1.2967 - val_acc: 0.7500\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9933 - acc: 0.7607 - val_loss: 1.3065 - val_acc: 0.7430\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9824 - acc: 0.7618 - val_loss: 1.3018 - val_acc: 0.7450\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9996 - acc: 0.7617 - val_loss: 1.2943 - val_acc: 0.7470\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9830 - acc: 0.7627 - val_loss: 1.2944 - val_acc: 0.7430\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9852 - acc: 0.7601 - val_loss: 1.2858 - val_acc: 0.7480\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9907 - acc: 0.7626 - val_loss: 1.2852 - val_acc: 0.7500\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.9840 - acc: 0.7615 - val_loss: 1.2627 - val_acc: 0.7470\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.9742 - acc: 0.7643 - val_loss: 1.2687 - val_acc: 0.7490\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.9805 - acc: 0.7642 - val_loss: 1.2672 - val_acc: 0.7430\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.9748 - acc: 0.7650 - val_loss: 1.2778 - val_acc: 0.7500\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.9566 - acc: 0.7670 - val_loss: 1.2726 - val_acc: 0.7530\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9554 - acc: 0.7681 - val_loss: 1.2773 - val_acc: 0.7560\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.9398 - acc: 0.7750 - val_loss: 1.2805 - val_acc: 0.7470\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9596 - acc: 0.7684 - val_loss: 1.2675 - val_acc: 0.7540\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9494 - acc: 0.7736 - val_loss: 1.2736 - val_acc: 0.7540\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9551 - acc: 0.7734 - val_loss: 1.2760 - val_acc: 0.7520\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9548 - acc: 0.7686 - val_loss: 1.2700 - val_acc: 0.7520\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9494 - acc: 0.7729 - val_loss: 1.2716 - val_acc: 0.7500\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9313 - acc: 0.7776 - val_loss: 1.2739 - val_acc: 0.7510\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9177 - acc: 0.7767 - val_loss: 1.2706 - val_acc: 0.7510\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9350 - acc: 0.7740 - val_loss: 1.2786 - val_acc: 0.7520\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9368 - acc: 0.7724 - val_loss: 1.2629 - val_acc: 0.7560\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9285 - acc: 0.7749 - val_loss: 1.2764 - val_acc: 0.7590\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9211 - acc: 0.7780 - val_loss: 1.2643 - val_acc: 0.7550\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.9274 - acc: 0.7791 - val_loss: 1.2686 - val_acc: 0.7560\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9097 - acc: 0.7818 - val_loss: 1.2661 - val_acc: 0.7550\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.9060 - acc: 0.7830 - val_loss: 1.2760 - val_acc: 0.7540\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8983 - acc: 0.7843 - val_loss: 1.2587 - val_acc: 0.7560\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.9185 - acc: 0.7789 - val_loss: 1.2690 - val_acc: 0.7510\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9274 - acc: 0.7770 - val_loss: 1.2578 - val_acc: 0.7580\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9199 - acc: 0.7829 - val_loss: 1.2899 - val_acc: 0.7480\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8994 - acc: 0.7904 - val_loss: 1.2770 - val_acc: 0.7590\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9153 - acc: 0.7794 - val_loss: 1.2829 - val_acc: 0.7530\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.8890 - acc: 0.7840 - val_loss: 1.2749 - val_acc: 0.7550\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9078 - acc: 0.7878 - val_loss: 1.2994 - val_acc: 0.7500\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9148 - acc: 0.7829 - val_loss: 1.2699 - val_acc: 0.7500\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9025 - acc: 0.7876 - val_loss: 1.2785 - val_acc: 0.7510\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8826 - acc: 0.7890 - val_loss: 1.2852 - val_acc: 0.7490\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9132 - acc: 0.7816 - val_loss: 1.3073 - val_acc: 0.7540\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.8854 - acc: 0.7869 - val_loss: 1.2875 - val_acc: 0.7500\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8886 - acc: 0.7856 - val_loss: 1.2658 - val_acc: 0.7510\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.8745 - acc: 0.7910 - val_loss: 1.2686 - val_acc: 0.7550\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8942 - acc: 0.7843 - val_loss: 1.2626 - val_acc: 0.7540\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8943 - acc: 0.7903 - val_loss: 1.2530 - val_acc: 0.7520\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8908 - acc: 0.7907 - val_loss: 1.2539 - val_acc: 0.7580\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.8772 - acc: 0.7928 - val_loss: 1.2594 - val_acc: 0.7540\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8621 - acc: 0.7944 - val_loss: 1.2717 - val_acc: 0.7530\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8653 - acc: 0.7967 - val_loss: 1.2628 - val_acc: 0.7560\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8977 - acc: 0.7928 - val_loss: 1.2528 - val_acc: 0.7580\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8725 - acc: 0.7963 - val_loss: 1.2583 - val_acc: 0.7570\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.8786 - acc: 0.7963 - val_loss: 1.2657 - val_acc: 0.7590\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8894 - acc: 0.7879 - val_loss: 1.2388 - val_acc: 0.7630\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 1s 105us/step - loss: 2.9724 - acc: 0.3970 - val_loss: 1.9311 - val_acc: 0.5800\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.8465 - acc: 0.5926 - val_loss: 1.5695 - val_acc: 0.6810\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.5356 - acc: 0.6731 - val_loss: 1.4087 - val_acc: 0.7010\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.3823 - acc: 0.7056 - val_loss: 1.3523 - val_acc: 0.7070\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.2613 - acc: 0.7305 - val_loss: 1.2832 - val_acc: 0.7430\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.1551 - acc: 0.7538 - val_loss: 1.2429 - val_acc: 0.7580\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.0716 - acc: 0.7772 - val_loss: 1.2087 - val_acc: 0.7640\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.0037 - acc: 0.7960 - val_loss: 1.1781 - val_acc: 0.7850\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9380 - acc: 0.8071 - val_loss: 1.1878 - val_acc: 0.7660\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8879 - acc: 0.8222 - val_loss: 1.1558 - val_acc: 0.7880\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.8265 - acc: 0.8321 - val_loss: 1.1319 - val_acc: 0.8040\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7844 - acc: 0.8460 - val_loss: 1.1807 - val_acc: 0.7920\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7508 - acc: 0.8512 - val_loss: 1.1326 - val_acc: 0.8010\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7234 - acc: 0.8586 - val_loss: 1.1561 - val_acc: 0.7980\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.6984 - acc: 0.8632 - val_loss: 1.1212 - val_acc: 0.8130\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.6674 - acc: 0.8768 - val_loss: 1.1729 - val_acc: 0.7980\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.6445 - acc: 0.8825 - val_loss: 1.1285 - val_acc: 0.8180\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.6182 - acc: 0.8851 - val_loss: 1.1533 - val_acc: 0.8130\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.6081 - acc: 0.8894 - val_loss: 1.1915 - val_acc: 0.8100\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.5905 - acc: 0.8908 - val_loss: 1.1387 - val_acc: 0.8170\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.5702 - acc: 0.8963 - val_loss: 1.1742 - val_acc: 0.8130\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.5606 - acc: 0.9004 - val_loss: 1.1629 - val_acc: 0.8100\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.5469 - acc: 0.9059 - val_loss: 1.1477 - val_acc: 0.8140\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.5265 - acc: 0.9082 - val_loss: 1.1564 - val_acc: 0.8170\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.5307 - acc: 0.9107 - val_loss: 1.1371 - val_acc: 0.8240\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.5166 - acc: 0.9128 - val_loss: 1.1482 - val_acc: 0.8170\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.5106 - acc: 0.9142 - val_loss: 1.1515 - val_acc: 0.8200\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.4921 - acc: 0.9187 - val_loss: 1.1507 - val_acc: 0.8150\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.4869 - acc: 0.9178 - val_loss: 1.2041 - val_acc: 0.8030\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.4903 - acc: 0.9191 - val_loss: 1.1623 - val_acc: 0.8170\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.4785 - acc: 0.9201 - val_loss: 1.1854 - val_acc: 0.8140\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.4759 - acc: 0.9214 - val_loss: 1.1926 - val_acc: 0.8180\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.4767 - acc: 0.9217 - val_loss: 1.1837 - val_acc: 0.8120\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.4595 - acc: 0.9260 - val_loss: 1.1833 - val_acc: 0.8080\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.4545 - acc: 0.9256 - val_loss: 1.1779 - val_acc: 0.8210\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.4489 - acc: 0.9267 - val_loss: 1.1743 - val_acc: 0.8130\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.4506 - acc: 0.9280 - val_loss: 1.2009 - val_acc: 0.8140\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.4395 - acc: 0.9298 - val_loss: 1.1962 - val_acc: 0.8170\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.4287 - acc: 0.9330 - val_loss: 1.2014 - val_acc: 0.8200\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.4471 - acc: 0.9297 - val_loss: 1.1990 - val_acc: 0.8170\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.4354 - acc: 0.9307 - val_loss: 1.2342 - val_acc: 0.8130\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.4280 - acc: 0.9330 - val_loss: 1.1992 - val_acc: 0.8210\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.4296 - acc: 0.9339 - val_loss: 1.2025 - val_acc: 0.8120\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.4149 - acc: 0.9370 - val_loss: 1.2090 - val_acc: 0.8160\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.4240 - acc: 0.9337 - val_loss: 1.1812 - val_acc: 0.8180\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.4171 - acc: 0.9352 - val_loss: 1.2083 - val_acc: 0.8120\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.4131 - acc: 0.9344 - val_loss: 1.2687 - val_acc: 0.8110\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.4096 - acc: 0.9370 - val_loss: 1.2179 - val_acc: 0.8170\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.4173 - acc: 0.9364 - val_loss: 1.2056 - val_acc: 0.8220\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.4026 - acc: 0.9380 - val_loss: 1.2030 - val_acc: 0.8200\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.4057 - acc: 0.9351 - val_loss: 1.2390 - val_acc: 0.8120\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.4040 - acc: 0.9406 - val_loss: 1.2194 - val_acc: 0.8140\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3873 - acc: 0.9426 - val_loss: 1.2484 - val_acc: 0.8190\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3996 - acc: 0.9389 - val_loss: 1.2403 - val_acc: 0.8100\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3874 - acc: 0.9401 - val_loss: 1.2322 - val_acc: 0.8130\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.3905 - acc: 0.9411 - val_loss: 1.2226 - val_acc: 0.8160\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3879 - acc: 0.9396 - val_loss: 1.2393 - val_acc: 0.8150\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3807 - acc: 0.9416 - val_loss: 1.2374 - val_acc: 0.8250\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3934 - acc: 0.9377 - val_loss: 1.2301 - val_acc: 0.8120\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3851 - acc: 0.9411 - val_loss: 1.2548 - val_acc: 0.8060\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3770 - acc: 0.9415 - val_loss: 1.2274 - val_acc: 0.8140\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3836 - acc: 0.9402 - val_loss: 1.2211 - val_acc: 0.8110\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3746 - acc: 0.9407 - val_loss: 1.2468 - val_acc: 0.8060\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3819 - acc: 0.9422 - val_loss: 1.2144 - val_acc: 0.8190\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3819 - acc: 0.9425 - val_loss: 1.2215 - val_acc: 0.8180\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3730 - acc: 0.9409 - val_loss: 1.2246 - val_acc: 0.8120\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.3736 - acc: 0.9425 - val_loss: 1.2656 - val_acc: 0.8110\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.3785 - acc: 0.9417 - val_loss: 1.2720 - val_acc: 0.8160\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.3722 - acc: 0.9432 - val_loss: 1.2400 - val_acc: 0.8150\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3688 - acc: 0.9437 - val_loss: 1.2195 - val_acc: 0.8150\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3663 - acc: 0.9424 - val_loss: 1.2150 - val_acc: 0.8210\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3604 - acc: 0.9455 - val_loss: 1.2084 - val_acc: 0.8200\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3702 - acc: 0.9440 - val_loss: 1.2400 - val_acc: 0.8160\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3672 - acc: 0.9439 - val_loss: 1.2347 - val_acc: 0.8100\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3579 - acc: 0.9446 - val_loss: 1.2274 - val_acc: 0.8130\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3726 - acc: 0.9416 - val_loss: 1.2079 - val_acc: 0.8110\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3656 - acc: 0.9436 - val_loss: 1.2460 - val_acc: 0.8070\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3593 - acc: 0.9442 - val_loss: 1.2447 - val_acc: 0.8090\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3545 - acc: 0.9440 - val_loss: 1.2275 - val_acc: 0.8060\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3560 - acc: 0.9424 - val_loss: 1.2248 - val_acc: 0.8100\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3613 - acc: 0.9449 - val_loss: 1.2093 - val_acc: 0.8080\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3603 - acc: 0.9425 - val_loss: 1.1923 - val_acc: 0.8120\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3602 - acc: 0.9432 - val_loss: 1.2557 - val_acc: 0.8050\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3577 - acc: 0.9430 - val_loss: 1.2476 - val_acc: 0.8020\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3518 - acc: 0.9439 - val_loss: 1.2196 - val_acc: 0.8080\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3574 - acc: 0.9426 - val_loss: 1.2286 - val_acc: 0.8130\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3490 - acc: 0.9469 - val_loss: 1.2130 - val_acc: 0.8170\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3567 - acc: 0.9424 - val_loss: 1.2511 - val_acc: 0.8030\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3465 - acc: 0.9456 - val_loss: 1.2114 - val_acc: 0.8170\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.3523 - acc: 0.9468 - val_loss: 1.2384 - val_acc: 0.8090\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3467 - acc: 0.9463 - val_loss: 1.2240 - val_acc: 0.8110\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3527 - acc: 0.9441 - val_loss: 1.2528 - val_acc: 0.8090\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.3469 - acc: 0.9463 - val_loss: 1.2083 - val_acc: 0.8030\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.3428 - acc: 0.9471 - val_loss: 1.2102 - val_acc: 0.8070\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3461 - acc: 0.9422 - val_loss: 1.2457 - val_acc: 0.8120\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3406 - acc: 0.9465 - val_loss: 1.2285 - val_acc: 0.8150\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3459 - acc: 0.9446 - val_loss: 1.2733 - val_acc: 0.8050\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3334 - acc: 0.9470 - val_loss: 1.2378 - val_acc: 0.8100\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3444 - acc: 0.9458 - val_loss: 1.2495 - val_acc: 0.8090\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3401 - acc: 0.9466 - val_loss: 1.1889 - val_acc: 0.8010\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 1s 112us/step - loss: 3.8473 - acc: 0.4558 - val_loss: 2.6614 - val_acc: 0.5840\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.4599 - acc: 0.6262 - val_loss: 2.1898 - val_acc: 0.6630\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 2.1248 - acc: 0.6818 - val_loss: 2.0258 - val_acc: 0.6960\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.9574 - acc: 0.7081 - val_loss: 1.9441 - val_acc: 0.7020\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.8472 - acc: 0.7216 - val_loss: 1.8339 - val_acc: 0.7200\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.7455 - acc: 0.7423 - val_loss: 1.7936 - val_acc: 0.7250\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.6692 - acc: 0.7536 - val_loss: 1.7217 - val_acc: 0.7490\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.6235 - acc: 0.7645 - val_loss: 1.7406 - val_acc: 0.7310\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5638 - acc: 0.7741 - val_loss: 1.6847 - val_acc: 0.7520\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.5135 - acc: 0.7833 - val_loss: 1.6541 - val_acc: 0.7510\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.4756 - acc: 0.7886 - val_loss: 1.5945 - val_acc: 0.7700\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.4414 - acc: 0.7965 - val_loss: 1.6212 - val_acc: 0.7640\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.4066 - acc: 0.8007 - val_loss: 1.5789 - val_acc: 0.7710\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.3711 - acc: 0.8047 - val_loss: 1.5774 - val_acc: 0.7600\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.3569 - acc: 0.8076 - val_loss: 1.5432 - val_acc: 0.7640\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.3277 - acc: 0.8126 - val_loss: 1.4997 - val_acc: 0.7820\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.3065 - acc: 0.8172 - val_loss: 1.5138 - val_acc: 0.7840\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.2888 - acc: 0.8178 - val_loss: 1.4858 - val_acc: 0.7810\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.2721 - acc: 0.8218 - val_loss: 1.4821 - val_acc: 0.7860\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.2477 - acc: 0.8224 - val_loss: 1.4890 - val_acc: 0.7750\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.2400 - acc: 0.8257 - val_loss: 1.4565 - val_acc: 0.7800\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.2138 - acc: 0.8264 - val_loss: 1.4597 - val_acc: 0.7880\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.1949 - acc: 0.8331 - val_loss: 1.4787 - val_acc: 0.7830\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.1798 - acc: 0.8353 - val_loss: 1.4430 - val_acc: 0.7870\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.1809 - acc: 0.8356 - val_loss: 1.4359 - val_acc: 0.7840\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.1607 - acc: 0.8393 - val_loss: 1.4675 - val_acc: 0.7710\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.1548 - acc: 0.8393 - val_loss: 1.4383 - val_acc: 0.7820\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.1362 - acc: 0.8429 - val_loss: 1.4890 - val_acc: 0.7560\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.1270 - acc: 0.8450 - val_loss: 1.4181 - val_acc: 0.7850\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.1021 - acc: 0.8483 - val_loss: 1.4413 - val_acc: 0.7760\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.0983 - acc: 0.8487 - val_loss: 1.4195 - val_acc: 0.7810\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.0893 - acc: 0.8533 - val_loss: 1.4284 - val_acc: 0.7760\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.0900 - acc: 0.8524 - val_loss: 1.3966 - val_acc: 0.7840\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.0748 - acc: 0.8552 - val_loss: 1.3801 - val_acc: 0.7920\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.0600 - acc: 0.8598 - val_loss: 1.4271 - val_acc: 0.7800\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.0604 - acc: 0.8567 - val_loss: 1.4057 - val_acc: 0.7900\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.0445 - acc: 0.8621 - val_loss: 1.3856 - val_acc: 0.7850\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.0374 - acc: 0.8574 - val_loss: 1.3929 - val_acc: 0.7810\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.0309 - acc: 0.8664 - val_loss: 1.3614 - val_acc: 0.7930\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.0250 - acc: 0.8646 - val_loss: 1.3567 - val_acc: 0.7940\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.0054 - acc: 0.8710 - val_loss: 1.3772 - val_acc: 0.7860\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.0115 - acc: 0.8687 - val_loss: 1.3969 - val_acc: 0.7880\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.0043 - acc: 0.8698 - val_loss: 1.3552 - val_acc: 0.8060\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.0015 - acc: 0.8711 - val_loss: 1.3707 - val_acc: 0.7850\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.9873 - acc: 0.8743 - val_loss: 1.3572 - val_acc: 0.7970\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9847 - acc: 0.8713 - val_loss: 1.3436 - val_acc: 0.7930\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.9739 - acc: 0.8750 - val_loss: 1.3645 - val_acc: 0.8030\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.9809 - acc: 0.8702 - val_loss: 1.3615 - val_acc: 0.7970\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9576 - acc: 0.8810 - val_loss: 1.3741 - val_acc: 0.8000\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9597 - acc: 0.8768 - val_loss: 1.3411 - val_acc: 0.8020\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9557 - acc: 0.8796 - val_loss: 1.3282 - val_acc: 0.8010\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.9647 - acc: 0.8740 - val_loss: 1.3324 - val_acc: 0.7990\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9428 - acc: 0.8824 - val_loss: 1.3566 - val_acc: 0.7850\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9360 - acc: 0.8850 - val_loss: 1.3606 - val_acc: 0.7850\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.9226 - acc: 0.8889 - val_loss: 1.3979 - val_acc: 0.7840\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9430 - acc: 0.8790 - val_loss: 1.3704 - val_acc: 0.7810\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.9328 - acc: 0.8816 - val_loss: 1.3389 - val_acc: 0.7900\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9184 - acc: 0.8860 - val_loss: 1.3036 - val_acc: 0.8010\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.9115 - acc: 0.8857 - val_loss: 1.3221 - val_acc: 0.7980\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9236 - acc: 0.8802 - val_loss: 1.3116 - val_acc: 0.8030\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.9212 - acc: 0.8835 - val_loss: 1.3064 - val_acc: 0.8000\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.8964 - acc: 0.8918 - val_loss: 1.3304 - val_acc: 0.7980\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9003 - acc: 0.8882 - val_loss: 1.3319 - val_acc: 0.7850\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9017 - acc: 0.8884 - val_loss: 1.3388 - val_acc: 0.7880\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.8923 - acc: 0.8928 - val_loss: 1.3097 - val_acc: 0.8020\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.8842 - acc: 0.8929 - val_loss: 1.3012 - val_acc: 0.8000\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.8837 - acc: 0.8899 - val_loss: 1.3435 - val_acc: 0.7940\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8758 - acc: 0.8950 - val_loss: 1.2903 - val_acc: 0.8030\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8799 - acc: 0.8971 - val_loss: 1.3127 - val_acc: 0.7940\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8738 - acc: 0.8950 - val_loss: 1.2929 - val_acc: 0.8010\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8716 - acc: 0.8950 - val_loss: 1.3419 - val_acc: 0.7880\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8606 - acc: 0.8983 - val_loss: 1.3192 - val_acc: 0.8030\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.8591 - acc: 0.8998 - val_loss: 1.4069 - val_acc: 0.7700\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8683 - acc: 0.8930 - val_loss: 1.3129 - val_acc: 0.8030\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8599 - acc: 0.8958 - val_loss: 1.3487 - val_acc: 0.7940\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8504 - acc: 0.9003 - val_loss: 1.3035 - val_acc: 0.8030\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8539 - acc: 0.8975 - val_loss: 1.3420 - val_acc: 0.7820\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.8432 - acc: 0.8983 - val_loss: 1.3472 - val_acc: 0.7910\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8523 - acc: 0.9002 - val_loss: 1.3015 - val_acc: 0.8000\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.8276 - acc: 0.9049 - val_loss: 1.2833 - val_acc: 0.8050\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8339 - acc: 0.9057 - val_loss: 1.3031 - val_acc: 0.7990\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8418 - acc: 0.8994 - val_loss: 1.3060 - val_acc: 0.7910\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8250 - acc: 0.9028 - val_loss: 1.3700 - val_acc: 0.7790\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.8341 - acc: 0.8979 - val_loss: 1.3080 - val_acc: 0.7980\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8290 - acc: 0.9044 - val_loss: 1.2849 - val_acc: 0.7920\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8179 - acc: 0.9033 - val_loss: 1.3052 - val_acc: 0.8030\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8261 - acc: 0.9024 - val_loss: 1.2876 - val_acc: 0.8030\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8236 - acc: 0.9037 - val_loss: 1.2930 - val_acc: 0.7970\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8078 - acc: 0.9088 - val_loss: 1.2799 - val_acc: 0.8100\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8080 - acc: 0.9100 - val_loss: 1.3709 - val_acc: 0.7810\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.8055 - acc: 0.9053 - val_loss: 1.3122 - val_acc: 0.7910\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8177 - acc: 0.9043 - val_loss: 1.3035 - val_acc: 0.8080\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8064 - acc: 0.9013 - val_loss: 1.3017 - val_acc: 0.8070\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.8011 - acc: 0.9107 - val_loss: 1.2825 - val_acc: 0.8010\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7965 - acc: 0.9097 - val_loss: 1.3458 - val_acc: 0.7950\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8055 - acc: 0.9079 - val_loss: 1.2897 - val_acc: 0.7950\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7980 - acc: 0.9062 - val_loss: 1.3263 - val_acc: 0.7940\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7864 - acc: 0.9121 - val_loss: 1.3137 - val_acc: 0.7910\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.7963 - acc: 0.9079 - val_loss: 1.2613 - val_acc: 0.8070\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7805 - acc: 0.9126 - val_loss: 1.2986 - val_acc: 0.7980\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 1s 111us/step - loss: 2.8542 - acc: 0.3925 - val_loss: 1.9133 - val_acc: 0.5800\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.8831 - acc: 0.5649 - val_loss: 1.5175 - val_acc: 0.6530\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.5951 - acc: 0.6318 - val_loss: 1.3496 - val_acc: 0.6850\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.4177 - acc: 0.6748 - val_loss: 1.2478 - val_acc: 0.7190\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.2978 - acc: 0.7055 - val_loss: 1.1831 - val_acc: 0.7270\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.2004 - acc: 0.7206 - val_loss: 1.1267 - val_acc: 0.7500\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.1155 - acc: 0.7395 - val_loss: 1.0884 - val_acc: 0.7610\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.0585 - acc: 0.7543 - val_loss: 1.0544 - val_acc: 0.7780\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.9960 - acc: 0.7647 - val_loss: 1.0260 - val_acc: 0.7770\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9392 - acc: 0.7805 - val_loss: 1.0159 - val_acc: 0.7830\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8759 - acc: 0.7925 - val_loss: 0.9831 - val_acc: 0.7900\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8395 - acc: 0.8017 - val_loss: 0.9769 - val_acc: 0.7910\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8165 - acc: 0.8004 - val_loss: 0.9525 - val_acc: 0.8050\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7717 - acc: 0.8133 - val_loss: 0.9476 - val_acc: 0.7970\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7220 - acc: 0.8232 - val_loss: 0.9468 - val_acc: 0.7950\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.6982 - acc: 0.8290 - val_loss: 0.9242 - val_acc: 0.8110\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.6690 - acc: 0.8385 - val_loss: 0.9255 - val_acc: 0.8160\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.6453 - acc: 0.8457 - val_loss: 0.9284 - val_acc: 0.8160\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.6227 - acc: 0.8431 - val_loss: 0.9120 - val_acc: 0.8180\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.5944 - acc: 0.8558 - val_loss: 0.9097 - val_acc: 0.8240\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.5648 - acc: 0.8617 - val_loss: 0.9092 - val_acc: 0.8250\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.5460 - acc: 0.8621 - val_loss: 0.9072 - val_acc: 0.8260\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.5281 - acc: 0.8690 - val_loss: 0.9024 - val_acc: 0.8260\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.5067 - acc: 0.8741 - val_loss: 0.9153 - val_acc: 0.8240\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.5076 - acc: 0.8705 - val_loss: 0.9109 - val_acc: 0.8270\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.4855 - acc: 0.8765 - val_loss: 0.8991 - val_acc: 0.8300\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.4707 - acc: 0.8790 - val_loss: 0.9066 - val_acc: 0.8310\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.4437 - acc: 0.8865 - val_loss: 0.9277 - val_acc: 0.8250\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.4502 - acc: 0.8825 - val_loss: 0.9139 - val_acc: 0.8320\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.4450 - acc: 0.8886 - val_loss: 0.9276 - val_acc: 0.8240\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.4177 - acc: 0.8913 - val_loss: 0.9211 - val_acc: 0.8250\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.4115 - acc: 0.8934 - val_loss: 0.9215 - val_acc: 0.8270\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.4080 - acc: 0.8928 - val_loss: 0.9153 - val_acc: 0.8320\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3853 - acc: 0.8989 - val_loss: 0.9382 - val_acc: 0.8290\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3892 - acc: 0.8995 - val_loss: 0.9315 - val_acc: 0.8250\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3749 - acc: 0.8993 - val_loss: 0.9428 - val_acc: 0.8300\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3673 - acc: 0.9027 - val_loss: 0.9372 - val_acc: 0.8300\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3662 - acc: 0.9040 - val_loss: 0.9524 - val_acc: 0.8300\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.3664 - acc: 0.9065 - val_loss: 0.9403 - val_acc: 0.8300\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3500 - acc: 0.9084 - val_loss: 0.9430 - val_acc: 0.8320\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3444 - acc: 0.9080 - val_loss: 0.9604 - val_acc: 0.8280\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3352 - acc: 0.9121 - val_loss: 0.9529 - val_acc: 0.8320\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3425 - acc: 0.9097 - val_loss: 0.9584 - val_acc: 0.8310\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3276 - acc: 0.9157 - val_loss: 0.9621 - val_acc: 0.8290\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3174 - acc: 0.9146 - val_loss: 0.9882 - val_acc: 0.8250\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3106 - acc: 0.9149 - val_loss: 0.9921 - val_acc: 0.8210\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3070 - acc: 0.9169 - val_loss: 0.9946 - val_acc: 0.8290\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3137 - acc: 0.9133 - val_loss: 0.9881 - val_acc: 0.8290\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3120 - acc: 0.9166 - val_loss: 1.0127 - val_acc: 0.8250\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3097 - acc: 0.9137 - val_loss: 0.9996 - val_acc: 0.8280\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3087 - acc: 0.9179 - val_loss: 1.0078 - val_acc: 0.8260\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2949 - acc: 0.9183 - val_loss: 1.0058 - val_acc: 0.8260\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.2976 - acc: 0.9183 - val_loss: 1.0042 - val_acc: 0.8240\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2965 - acc: 0.9197 - val_loss: 1.0188 - val_acc: 0.8300\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.3005 - acc: 0.9193 - val_loss: 1.0162 - val_acc: 0.8300\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.2924 - acc: 0.9242 - val_loss: 1.0234 - val_acc: 0.8250\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.2893 - acc: 0.9201 - val_loss: 1.0117 - val_acc: 0.8220\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.2723 - acc: 0.9270 - val_loss: 1.0291 - val_acc: 0.8140\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2752 - acc: 0.9250 - val_loss: 1.0448 - val_acc: 0.8270\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2688 - acc: 0.9251 - val_loss: 1.0465 - val_acc: 0.8270\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2743 - acc: 0.9253 - val_loss: 1.0409 - val_acc: 0.8280\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2653 - acc: 0.9256 - val_loss: 1.0529 - val_acc: 0.8220\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2701 - acc: 0.9240 - val_loss: 1.0485 - val_acc: 0.8290\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.2702 - acc: 0.9257 - val_loss: 1.0528 - val_acc: 0.8230\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.2597 - acc: 0.9288 - val_loss: 1.0534 - val_acc: 0.8280\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.2722 - acc: 0.9228 - val_loss: 1.0567 - val_acc: 0.8240\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2577 - acc: 0.9295 - val_loss: 1.0648 - val_acc: 0.8220\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2570 - acc: 0.9280 - val_loss: 1.0608 - val_acc: 0.8280\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2682 - acc: 0.9260 - val_loss: 1.0677 - val_acc: 0.8200\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.2585 - acc: 0.9278 - val_loss: 1.0699 - val_acc: 0.8280\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2622 - acc: 0.9280 - val_loss: 1.0670 - val_acc: 0.8270\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2513 - acc: 0.9267 - val_loss: 1.0866 - val_acc: 0.8200\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2632 - acc: 0.9288 - val_loss: 1.0935 - val_acc: 0.8240\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2558 - acc: 0.9296 - val_loss: 1.1024 - val_acc: 0.8300\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2571 - acc: 0.9287 - val_loss: 1.0861 - val_acc: 0.8260\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2527 - acc: 0.9266 - val_loss: 1.1011 - val_acc: 0.8260\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2483 - acc: 0.9326 - val_loss: 1.1109 - val_acc: 0.8240\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2431 - acc: 0.9331 - val_loss: 1.1197 - val_acc: 0.8190\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2417 - acc: 0.9317 - val_loss: 1.1037 - val_acc: 0.8250\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2481 - acc: 0.9306 - val_loss: 1.1035 - val_acc: 0.8220\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2526 - acc: 0.9282 - val_loss: 1.1066 - val_acc: 0.8210\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2349 - acc: 0.9355 - val_loss: 1.1364 - val_acc: 0.8220\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2490 - acc: 0.9291 - val_loss: 1.1171 - val_acc: 0.8230\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.2550 - acc: 0.9282 - val_loss: 1.1286 - val_acc: 0.8210\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2360 - acc: 0.9349 - val_loss: 1.1232 - val_acc: 0.8230\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2477 - acc: 0.9308 - val_loss: 1.1300 - val_acc: 0.8190\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2335 - acc: 0.9321 - val_loss: 1.1314 - val_acc: 0.8230\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2401 - acc: 0.9352 - val_loss: 1.1415 - val_acc: 0.8150\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2289 - acc: 0.9351 - val_loss: 1.1328 - val_acc: 0.8180\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2309 - acc: 0.9371 - val_loss: 1.1386 - val_acc: 0.8250\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2451 - acc: 0.9335 - val_loss: 1.1299 - val_acc: 0.8210\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2408 - acc: 0.9317 - val_loss: 1.1166 - val_acc: 0.8210\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2301 - acc: 0.9369 - val_loss: 1.1397 - val_acc: 0.8180\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2311 - acc: 0.9356 - val_loss: 1.1337 - val_acc: 0.8220\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2402 - acc: 0.9315 - val_loss: 1.1302 - val_acc: 0.8210\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2435 - acc: 0.9336 - val_loss: 1.1331 - val_acc: 0.8260\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2240 - acc: 0.9364 - val_loss: 1.1471 - val_acc: 0.8260\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2258 - acc: 0.9397 - val_loss: 1.1488 - val_acc: 0.8270\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2425 - acc: 0.9320 - val_loss: 1.1456 - val_acc: 0.8250\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2318 - acc: 0.9332 - val_loss: 1.1606 - val_acc: 0.8210\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 1s 122us/step - loss: 6.1323 - acc: 0.3656 - val_loss: 4.1819 - val_acc: 0.5280\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 3.9380 - acc: 0.5063 - val_loss: 3.3929 - val_acc: 0.5820\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 3.4025 - acc: 0.5365 - val_loss: 3.0085 - val_acc: 0.6120\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 3.0400 - acc: 0.5674 - val_loss: 2.7301 - val_acc: 0.6270\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 2.8031 - acc: 0.5834 - val_loss: 2.5045 - val_acc: 0.6260\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 2.5737 - acc: 0.5966 - val_loss: 2.3220 - val_acc: 0.6380\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 2.4113 - acc: 0.6040 - val_loss: 2.1773 - val_acc: 0.6490\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 88us/step - loss: 2.2796 - acc: 0.6096 - val_loss: 2.0768 - val_acc: 0.6610\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 2.1719 - acc: 0.6169 - val_loss: 2.0004 - val_acc: 0.6720\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0842 - acc: 0.6327 - val_loss: 1.9242 - val_acc: 0.6580\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 2.0214 - acc: 0.6310 - val_loss: 1.8976 - val_acc: 0.6550\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.9687 - acc: 0.6409 - val_loss: 1.8461 - val_acc: 0.6700\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.9413 - acc: 0.6482 - val_loss: 1.8212 - val_acc: 0.6660\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.9075 - acc: 0.6446 - val_loss: 1.7962 - val_acc: 0.6700\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.8968 - acc: 0.6456 - val_loss: 1.7737 - val_acc: 0.6840\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.8602 - acc: 0.6602 - val_loss: 1.7467 - val_acc: 0.6830\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.8552 - acc: 0.6515 - val_loss: 1.7321 - val_acc: 0.6860\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.8361 - acc: 0.6575 - val_loss: 1.7381 - val_acc: 0.6820\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.8249 - acc: 0.6589 - val_loss: 1.7394 - val_acc: 0.6670\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.8179 - acc: 0.6592 - val_loss: 1.7066 - val_acc: 0.6860\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.8116 - acc: 0.6595 - val_loss: 1.6971 - val_acc: 0.6840\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.7963 - acc: 0.6646 - val_loss: 1.6854 - val_acc: 0.6870\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.7778 - acc: 0.6655 - val_loss: 1.6783 - val_acc: 0.6840\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.7784 - acc: 0.6674 - val_loss: 1.6846 - val_acc: 0.6920\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.7652 - acc: 0.6706 - val_loss: 1.6574 - val_acc: 0.6920\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.7557 - acc: 0.6700 - val_loss: 1.6559 - val_acc: 0.6820\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.7574 - acc: 0.6661 - val_loss: 1.6443 - val_acc: 0.6920\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.7380 - acc: 0.6710 - val_loss: 1.6347 - val_acc: 0.6890\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.7395 - acc: 0.6708 - val_loss: 1.6319 - val_acc: 0.6980\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.7303 - acc: 0.6709 - val_loss: 1.6303 - val_acc: 0.6940\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.7236 - acc: 0.6739 - val_loss: 1.6152 - val_acc: 0.6880\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.7187 - acc: 0.6721 - val_loss: 1.6158 - val_acc: 0.6950\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.7161 - acc: 0.6721 - val_loss: 1.6246 - val_acc: 0.6820\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.7034 - acc: 0.6739 - val_loss: 1.6215 - val_acc: 0.6830\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.6882 - acc: 0.6787 - val_loss: 1.5900 - val_acc: 0.6910\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.6959 - acc: 0.6724 - val_loss: 1.5937 - val_acc: 0.6940\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.6836 - acc: 0.6764 - val_loss: 1.5953 - val_acc: 0.7020\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.6798 - acc: 0.6763 - val_loss: 1.5888 - val_acc: 0.6900\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.6706 - acc: 0.6798 - val_loss: 1.5848 - val_acc: 0.6910\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.6813 - acc: 0.6760 - val_loss: 1.5795 - val_acc: 0.6930\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.6634 - acc: 0.6787 - val_loss: 1.5600 - val_acc: 0.6990\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.6719 - acc: 0.6744 - val_loss: 1.5731 - val_acc: 0.6890\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.6650 - acc: 0.6785 - val_loss: 1.5654 - val_acc: 0.6940\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.6583 - acc: 0.6818 - val_loss: 1.5596 - val_acc: 0.7010\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 1.6572 - acc: 0.6822 - val_loss: 1.5640 - val_acc: 0.6970\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.6498 - acc: 0.6848 - val_loss: 1.5500 - val_acc: 0.6970\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.6432 - acc: 0.6804 - val_loss: 1.5861 - val_acc: 0.6920\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.6525 - acc: 0.6804 - val_loss: 1.5416 - val_acc: 0.6980\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.6311 - acc: 0.6825 - val_loss: 1.5315 - val_acc: 0.7010\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.6462 - acc: 0.6773 - val_loss: 1.5339 - val_acc: 0.7010\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.6438 - acc: 0.6783 - val_loss: 1.5286 - val_acc: 0.7030\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.6277 - acc: 0.6840 - val_loss: 1.5346 - val_acc: 0.6990\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.6195 - acc: 0.6820 - val_loss: 1.5241 - val_acc: 0.6990\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.6191 - acc: 0.6882 - val_loss: 1.5359 - val_acc: 0.6930\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.6186 - acc: 0.6839 - val_loss: 1.5314 - val_acc: 0.6920\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.6251 - acc: 0.6827 - val_loss: 1.5467 - val_acc: 0.6880\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.6151 - acc: 0.6870 - val_loss: 1.5331 - val_acc: 0.7010\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.5985 - acc: 0.6887 - val_loss: 1.5168 - val_acc: 0.7000\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.6186 - acc: 0.6829 - val_loss: 1.5211 - val_acc: 0.7030\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.6110 - acc: 0.6854 - val_loss: 1.5347 - val_acc: 0.6990\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.6085 - acc: 0.6830 - val_loss: 1.5021 - val_acc: 0.7070\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5951 - acc: 0.6927 - val_loss: 1.5004 - val_acc: 0.7030\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.5938 - acc: 0.6869 - val_loss: 1.5139 - val_acc: 0.6910\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.5878 - acc: 0.6865 - val_loss: 1.4970 - val_acc: 0.7030\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5877 - acc: 0.6903 - val_loss: 1.5066 - val_acc: 0.7010\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.5870 - acc: 0.6899 - val_loss: 1.4913 - val_acc: 0.7100\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 1.5880 - acc: 0.6885 - val_loss: 1.4852 - val_acc: 0.7020\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.5819 - acc: 0.6857 - val_loss: 1.4912 - val_acc: 0.6970\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5842 - acc: 0.6862 - val_loss: 1.5106 - val_acc: 0.6960\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5799 - acc: 0.6916 - val_loss: 1.4889 - val_acc: 0.7080\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.5878 - acc: 0.6874 - val_loss: 1.4841 - val_acc: 0.7020\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.5789 - acc: 0.6904 - val_loss: 1.4744 - val_acc: 0.7120\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.5788 - acc: 0.6914 - val_loss: 1.5091 - val_acc: 0.7020\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.5681 - acc: 0.6917 - val_loss: 1.4994 - val_acc: 0.7020\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.5700 - acc: 0.6909 - val_loss: 1.4903 - val_acc: 0.7060\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5729 - acc: 0.6923 - val_loss: 1.4806 - val_acc: 0.7180\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.5608 - acc: 0.6952 - val_loss: 1.4769 - val_acc: 0.7020\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.5613 - acc: 0.6937 - val_loss: 1.4671 - val_acc: 0.7140\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.5681 - acc: 0.6891 - val_loss: 1.4721 - val_acc: 0.7090\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5467 - acc: 0.6951 - val_loss: 1.5051 - val_acc: 0.7020\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.5448 - acc: 0.6963 - val_loss: 1.4679 - val_acc: 0.7070\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.5607 - acc: 0.6936 - val_loss: 1.4707 - val_acc: 0.7040\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.5510 - acc: 0.6953 - val_loss: 1.4475 - val_acc: 0.7260\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5583 - acc: 0.6967 - val_loss: 1.4751 - val_acc: 0.7070\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5665 - acc: 0.6949 - val_loss: 1.4768 - val_acc: 0.7180\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5384 - acc: 0.6994 - val_loss: 1.4458 - val_acc: 0.7090\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5431 - acc: 0.6936 - val_loss: 1.4723 - val_acc: 0.7110\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.5382 - acc: 0.7013 - val_loss: 1.4700 - val_acc: 0.6990\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.5338 - acc: 0.6974 - val_loss: 1.4517 - val_acc: 0.7110\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.5354 - acc: 0.6989 - val_loss: 1.4459 - val_acc: 0.7180\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.5503 - acc: 0.6958 - val_loss: 1.4638 - val_acc: 0.7120\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.5377 - acc: 0.6993 - val_loss: 1.4585 - val_acc: 0.7140\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.5277 - acc: 0.6993 - val_loss: 1.4618 - val_acc: 0.7070\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5302 - acc: 0.7061 - val_loss: 1.4605 - val_acc: 0.7110\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5181 - acc: 0.7011 - val_loss: 1.4417 - val_acc: 0.7250\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5390 - acc: 0.6964 - val_loss: 1.4399 - val_acc: 0.7260\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5327 - acc: 0.6989 - val_loss: 1.4480 - val_acc: 0.7230\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.5198 - acc: 0.7035 - val_loss: 1.4403 - val_acc: 0.7200\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.5394 - acc: 0.6989 - val_loss: 1.4349 - val_acc: 0.7330\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.5127 - acc: 0.7036 - val_loss: 1.4360 - val_acc: 0.7290\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 1s 134us/step - loss: 12.1076 - acc: 0.2937 - val_loss: 8.7146 - val_acc: 0.5250\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 7.9113 - acc: 0.3971 - val_loss: 6.8665 - val_acc: 0.4930\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 6.5279 - acc: 0.4331 - val_loss: 5.7262 - val_acc: 0.5230\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 5.5013 - acc: 0.4538 - val_loss: 4.8385 - val_acc: 0.5360\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 4.6811 - acc: 0.4721 - val_loss: 4.1343 - val_acc: 0.5450\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 4.0174 - acc: 0.4842 - val_loss: 3.5843 - val_acc: 0.5490\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 3.5073 - acc: 0.4932 - val_loss: 3.1438 - val_acc: 0.5440\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 3.0953 - acc: 0.4987 - val_loss: 2.7833 - val_acc: 0.5330\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 2.7886 - acc: 0.5003 - val_loss: 2.5334 - val_acc: 0.5380\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.5791 - acc: 0.5054 - val_loss: 2.3725 - val_acc: 0.5460\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 2.4315 - acc: 0.5108 - val_loss: 2.2501 - val_acc: 0.5370\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.3332 - acc: 0.5163 - val_loss: 2.1880 - val_acc: 0.5400\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.2801 - acc: 0.5122 - val_loss: 2.1392 - val_acc: 0.5390\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.2458 - acc: 0.5249 - val_loss: 2.1232 - val_acc: 0.5650\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.2183 - acc: 0.5257 - val_loss: 2.0890 - val_acc: 0.5500\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 2.1886 - acc: 0.5346 - val_loss: 2.1271 - val_acc: 0.5400\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.1842 - acc: 0.5351 - val_loss: 2.0677 - val_acc: 0.5650\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.1767 - acc: 0.5397 - val_loss: 2.0723 - val_acc: 0.5500\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.1582 - acc: 0.5426 - val_loss: 2.0382 - val_acc: 0.5720\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.1671 - acc: 0.5426 - val_loss: 2.0416 - val_acc: 0.5770\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.1340 - acc: 0.5454 - val_loss: 2.0257 - val_acc: 0.5710\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 2.1400 - acc: 0.5470 - val_loss: 2.0410 - val_acc: 0.5780\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 2.1298 - acc: 0.5469 - val_loss: 2.0287 - val_acc: 0.5670\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.1198 - acc: 0.5509 - val_loss: 2.0114 - val_acc: 0.5760\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.1255 - acc: 0.5512 - val_loss: 2.0103 - val_acc: 0.5740\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.1039 - acc: 0.5539 - val_loss: 2.0068 - val_acc: 0.5650\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 2.1029 - acc: 0.5522 - val_loss: 2.0030 - val_acc: 0.5670\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 2.0983 - acc: 0.5542 - val_loss: 1.9842 - val_acc: 0.5740\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0995 - acc: 0.5496 - val_loss: 1.9980 - val_acc: 0.5780\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 2.0868 - acc: 0.5591 - val_loss: 1.9810 - val_acc: 0.5660\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 2.0918 - acc: 0.5507 - val_loss: 1.9788 - val_acc: 0.5830\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0847 - acc: 0.5554 - val_loss: 1.9851 - val_acc: 0.5650\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0815 - acc: 0.5532 - val_loss: 2.0329 - val_acc: 0.5810\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0831 - acc: 0.5534 - val_loss: 1.9636 - val_acc: 0.5820\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 2.0719 - acc: 0.5561 - val_loss: 1.9732 - val_acc: 0.5680\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0844 - acc: 0.5556 - val_loss: 1.9699 - val_acc: 0.5740\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0653 - acc: 0.5580 - val_loss: 1.9753 - val_acc: 0.5810\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0665 - acc: 0.5570 - val_loss: 1.9609 - val_acc: 0.5860\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0708 - acc: 0.5545 - val_loss: 1.9480 - val_acc: 0.5780\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0666 - acc: 0.5575 - val_loss: 1.9710 - val_acc: 0.5850\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0570 - acc: 0.5571 - val_loss: 1.9624 - val_acc: 0.5710\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0523 - acc: 0.5547 - val_loss: 1.9390 - val_acc: 0.5800\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0629 - acc: 0.5581 - val_loss: 1.9497 - val_acc: 0.5840\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0586 - acc: 0.5620 - val_loss: 1.9403 - val_acc: 0.5740\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0440 - acc: 0.5551 - val_loss: 1.9298 - val_acc: 0.5840\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0380 - acc: 0.5599 - val_loss: 1.9579 - val_acc: 0.5830\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0402 - acc: 0.5584 - val_loss: 1.9301 - val_acc: 0.5800\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0582 - acc: 0.5570 - val_loss: 1.9228 - val_acc: 0.5840\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 2.0438 - acc: 0.5576 - val_loss: 1.9177 - val_acc: 0.5830\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0431 - acc: 0.5585 - val_loss: 1.9195 - val_acc: 0.5760\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 2.0348 - acc: 0.5586 - val_loss: 1.9310 - val_acc: 0.5760\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 2.0319 - acc: 0.5603 - val_loss: 1.9645 - val_acc: 0.5840\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 2.0480 - acc: 0.5584 - val_loss: 1.9387 - val_acc: 0.5740\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 2.0249 - acc: 0.5586 - val_loss: 1.9610 - val_acc: 0.5800\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0421 - acc: 0.5536 - val_loss: 1.9248 - val_acc: 0.5790\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0190 - acc: 0.5649 - val_loss: 1.9432 - val_acc: 0.5710\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0424 - acc: 0.5568 - val_loss: 1.9231 - val_acc: 0.5770\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0294 - acc: 0.5648 - val_loss: 1.9185 - val_acc: 0.5760\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 2.0294 - acc: 0.5641 - val_loss: 1.9169 - val_acc: 0.5750\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 2.0306 - acc: 0.5616 - val_loss: 1.9057 - val_acc: 0.5830\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0191 - acc: 0.5663 - val_loss: 1.9139 - val_acc: 0.5850\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0178 - acc: 0.5603 - val_loss: 1.9158 - val_acc: 0.5780\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0191 - acc: 0.5618 - val_loss: 1.9400 - val_acc: 0.5850\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0130 - acc: 0.5695 - val_loss: 1.8893 - val_acc: 0.5800\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0240 - acc: 0.5633 - val_loss: 1.9059 - val_acc: 0.5840\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0264 - acc: 0.5594 - val_loss: 1.9192 - val_acc: 0.5860\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0183 - acc: 0.5669 - val_loss: 1.8960 - val_acc: 0.5840\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0148 - acc: 0.5653 - val_loss: 1.9019 - val_acc: 0.5810\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 2.0108 - acc: 0.5638 - val_loss: 1.9263 - val_acc: 0.5800\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0232 - acc: 0.5614 - val_loss: 1.9049 - val_acc: 0.5820\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 2.0038 - acc: 0.5664 - val_loss: 1.8899 - val_acc: 0.5860\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 2.0191 - acc: 0.5625 - val_loss: 1.8830 - val_acc: 0.5820\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0110 - acc: 0.5648 - val_loss: 1.8832 - val_acc: 0.5810\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0023 - acc: 0.5603 - val_loss: 1.8862 - val_acc: 0.5850\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.9986 - acc: 0.5650 - val_loss: 1.9439 - val_acc: 0.5670\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0132 - acc: 0.5649 - val_loss: 1.9079 - val_acc: 0.5740\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0063 - acc: 0.5656 - val_loss: 1.9283 - val_acc: 0.5770\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.0089 - acc: 0.5623 - val_loss: 1.8877 - val_acc: 0.5780\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0078 - acc: 0.5620 - val_loss: 1.8763 - val_acc: 0.5890\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0100 - acc: 0.5614 - val_loss: 1.8667 - val_acc: 0.5880\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.9990 - acc: 0.5640 - val_loss: 1.8818 - val_acc: 0.5810\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.9968 - acc: 0.5616 - val_loss: 1.8646 - val_acc: 0.5850\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 2.0044 - acc: 0.5620 - val_loss: 1.8772 - val_acc: 0.5840\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.9981 - acc: 0.5663 - val_loss: 1.8814 - val_acc: 0.5820\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.9873 - acc: 0.5631 - val_loss: 1.8661 - val_acc: 0.5830\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.9946 - acc: 0.5653 - val_loss: 1.8874 - val_acc: 0.5820\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.9933 - acc: 0.5659 - val_loss: 1.9209 - val_acc: 0.5760\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.9944 - acc: 0.5664 - val_loss: 1.8806 - val_acc: 0.5850\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.9970 - acc: 0.5643 - val_loss: 1.8999 - val_acc: 0.5800\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.9838 - acc: 0.5687 - val_loss: 1.8632 - val_acc: 0.5850\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.9927 - acc: 0.5679 - val_loss: 1.8874 - val_acc: 0.5820\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.9951 - acc: 0.5649 - val_loss: 1.8689 - val_acc: 0.5820\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.9831 - acc: 0.5670 - val_loss: 1.8586 - val_acc: 0.5920\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.9811 - acc: 0.5663 - val_loss: 1.8638 - val_acc: 0.5870\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.9835 - acc: 0.5678 - val_loss: 1.8586 - val_acc: 0.5820\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.9920 - acc: 0.5591 - val_loss: 1.8597 - val_acc: 0.5810\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.9795 - acc: 0.5656 - val_loss: 1.8800 - val_acc: 0.5750\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.9957 - acc: 0.5683 - val_loss: 1.8901 - val_acc: 0.5700\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.9907 - acc: 0.5666 - val_loss: 1.8789 - val_acc: 0.5850\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.9799 - acc: 0.5700 - val_loss: 1.8884 - val_acc: 0.5850\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 1s 134us/step - loss: 3.0349 - acc: 0.3549 - val_loss: 2.1020 - val_acc: 0.5400\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 2.1493 - acc: 0.5178 - val_loss: 1.6559 - val_acc: 0.6410\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.8236 - acc: 0.5890 - val_loss: 1.4844 - val_acc: 0.6760\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.6632 - acc: 0.6298 - val_loss: 1.4052 - val_acc: 0.6950\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.5545 - acc: 0.6562 - val_loss: 1.3416 - val_acc: 0.7000\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.4589 - acc: 0.6746 - val_loss: 1.2955 - val_acc: 0.7150\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.4027 - acc: 0.6883 - val_loss: 1.2665 - val_acc: 0.7240\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.3552 - acc: 0.6998 - val_loss: 1.2247 - val_acc: 0.7400\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.3098 - acc: 0.7075 - val_loss: 1.2038 - val_acc: 0.7450\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.2885 - acc: 0.7175 - val_loss: 1.1866 - val_acc: 0.7540\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.2492 - acc: 0.7230 - val_loss: 1.1639 - val_acc: 0.7590\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.2177 - acc: 0.7303 - val_loss: 1.1546 - val_acc: 0.7610\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.2011 - acc: 0.7384 - val_loss: 1.1296 - val_acc: 0.7720\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.1507 - acc: 0.7430 - val_loss: 1.1214 - val_acc: 0.7750\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.1337 - acc: 0.7471 - val_loss: 1.1082 - val_acc: 0.7760\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.1191 - acc: 0.7532 - val_loss: 1.0926 - val_acc: 0.7790\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.0950 - acc: 0.7612 - val_loss: 1.0854 - val_acc: 0.7800\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.0763 - acc: 0.7637 - val_loss: 1.0782 - val_acc: 0.7860\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.0600 - acc: 0.7677 - val_loss: 1.0714 - val_acc: 0.7820\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.0435 - acc: 0.7701 - val_loss: 1.0681 - val_acc: 0.7860\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.0175 - acc: 0.7801 - val_loss: 1.0621 - val_acc: 0.7880\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.0081 - acc: 0.7803 - val_loss: 1.0530 - val_acc: 0.7930\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9812 - acc: 0.7854 - val_loss: 1.0616 - val_acc: 0.7890\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9844 - acc: 0.7897 - val_loss: 1.0546 - val_acc: 0.7970\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.9896 - acc: 0.7833 - val_loss: 1.0443 - val_acc: 0.7930\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.9632 - acc: 0.7879 - val_loss: 1.0313 - val_acc: 0.7930\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9670 - acc: 0.7846 - val_loss: 1.0339 - val_acc: 0.7920\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9533 - acc: 0.7900 - val_loss: 1.0303 - val_acc: 0.7960\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9396 - acc: 0.7942 - val_loss: 1.0237 - val_acc: 0.7930\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9311 - acc: 0.7967 - val_loss: 1.0281 - val_acc: 0.7910\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9211 - acc: 0.7953 - val_loss: 1.0154 - val_acc: 0.8010\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9234 - acc: 0.7959 - val_loss: 1.0199 - val_acc: 0.8020\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9047 - acc: 0.7989 - val_loss: 1.0132 - val_acc: 0.8010\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9026 - acc: 0.8028 - val_loss: 1.0078 - val_acc: 0.8030\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.9100 - acc: 0.8018 - val_loss: 1.0135 - val_acc: 0.7980\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.8875 - acc: 0.8078 - val_loss: 1.0143 - val_acc: 0.8030\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.8734 - acc: 0.8034 - val_loss: 1.0123 - val_acc: 0.8010\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8906 - acc: 0.7995 - val_loss: 0.9942 - val_acc: 0.8030\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8871 - acc: 0.8103 - val_loss: 0.9945 - val_acc: 0.8110\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8785 - acc: 0.8079 - val_loss: 0.9957 - val_acc: 0.8120\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8529 - acc: 0.8120 - val_loss: 1.0032 - val_acc: 0.8110\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.8694 - acc: 0.8077 - val_loss: 0.9911 - val_acc: 0.8110\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.8577 - acc: 0.8127 - val_loss: 0.9996 - val_acc: 0.8100\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8567 - acc: 0.8122 - val_loss: 0.9866 - val_acc: 0.8100\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8420 - acc: 0.8138 - val_loss: 1.0026 - val_acc: 0.8110\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.8433 - acc: 0.8176 - val_loss: 0.9869 - val_acc: 0.8150\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8328 - acc: 0.8176 - val_loss: 0.9764 - val_acc: 0.8180\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.8261 - acc: 0.8185 - val_loss: 0.9850 - val_acc: 0.8120\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.8265 - acc: 0.8236 - val_loss: 0.9882 - val_acc: 0.8090\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8263 - acc: 0.8198 - val_loss: 0.9808 - val_acc: 0.8140\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8294 - acc: 0.8203 - val_loss: 0.9808 - val_acc: 0.8170\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.8153 - acc: 0.8246 - val_loss: 0.9813 - val_acc: 0.8140\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8193 - acc: 0.8250 - val_loss: 0.9743 - val_acc: 0.8170\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7976 - acc: 0.8211 - val_loss: 0.9838 - val_acc: 0.8160\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.8023 - acc: 0.8271 - val_loss: 0.9720 - val_acc: 0.8160\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.8083 - acc: 0.8255 - val_loss: 0.9795 - val_acc: 0.8090\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7996 - acc: 0.8247 - val_loss: 0.9644 - val_acc: 0.8130\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7925 - acc: 0.8287 - val_loss: 0.9679 - val_acc: 0.8140\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7897 - acc: 0.8297 - val_loss: 0.9720 - val_acc: 0.8160\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.7878 - acc: 0.8282 - val_loss: 0.9703 - val_acc: 0.8120\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8035 - acc: 0.8267 - val_loss: 0.9708 - val_acc: 0.8100\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7768 - acc: 0.8350 - val_loss: 0.9569 - val_acc: 0.8130\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.8031 - acc: 0.8299 - val_loss: 0.9619 - val_acc: 0.8090\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7969 - acc: 0.8267 - val_loss: 0.9726 - val_acc: 0.8070\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7731 - acc: 0.8325 - val_loss: 0.9609 - val_acc: 0.8120\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7762 - acc: 0.8295 - val_loss: 0.9631 - val_acc: 0.8110\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7877 - acc: 0.8315 - val_loss: 0.9410 - val_acc: 0.8210\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7771 - acc: 0.8307 - val_loss: 0.9471 - val_acc: 0.8180\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.7585 - acc: 0.8343 - val_loss: 0.9587 - val_acc: 0.8200\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7719 - acc: 0.8332 - val_loss: 0.9538 - val_acc: 0.8200\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7672 - acc: 0.8344 - val_loss: 0.9455 - val_acc: 0.8260\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7651 - acc: 0.8299 - val_loss: 0.9529 - val_acc: 0.8270\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7699 - acc: 0.8295 - val_loss: 0.9485 - val_acc: 0.8240\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7627 - acc: 0.8363 - val_loss: 0.9531 - val_acc: 0.8240\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7477 - acc: 0.8389 - val_loss: 0.9524 - val_acc: 0.8170\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7709 - acc: 0.8269 - val_loss: 0.9572 - val_acc: 0.8190\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7602 - acc: 0.8314 - val_loss: 0.9567 - val_acc: 0.8190\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.7525 - acc: 0.8360 - val_loss: 0.9529 - val_acc: 0.8250\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7466 - acc: 0.8369 - val_loss: 0.9407 - val_acc: 0.8260\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.7594 - acc: 0.8356 - val_loss: 0.9560 - val_acc: 0.8200\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.7466 - acc: 0.8346 - val_loss: 0.9468 - val_acc: 0.8260\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.7596 - acc: 0.8335 - val_loss: 0.9497 - val_acc: 0.8250\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7341 - acc: 0.8365 - val_loss: 0.9450 - val_acc: 0.8230\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7347 - acc: 0.8416 - val_loss: 0.9478 - val_acc: 0.8210\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7449 - acc: 0.8378 - val_loss: 0.9413 - val_acc: 0.8340\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.7168 - acc: 0.8436 - val_loss: 0.9440 - val_acc: 0.8290\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7470 - acc: 0.8363 - val_loss: 0.9344 - val_acc: 0.8230\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7406 - acc: 0.8393 - val_loss: 0.9361 - val_acc: 0.8260\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7374 - acc: 0.8378 - val_loss: 0.9436 - val_acc: 0.8240\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7445 - acc: 0.8389 - val_loss: 0.9404 - val_acc: 0.8210\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7338 - acc: 0.8380 - val_loss: 0.9332 - val_acc: 0.8210\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7516 - acc: 0.8388 - val_loss: 0.9356 - val_acc: 0.8190\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7343 - acc: 0.8463 - val_loss: 0.9406 - val_acc: 0.8270\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.7269 - acc: 0.8452 - val_loss: 0.9546 - val_acc: 0.8250\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.7248 - acc: 0.8395 - val_loss: 0.9434 - val_acc: 0.8250\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7309 - acc: 0.8413 - val_loss: 0.9464 - val_acc: 0.8210\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7318 - acc: 0.8415 - val_loss: 0.9433 - val_acc: 0.8260\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7267 - acc: 0.8429 - val_loss: 0.9334 - val_acc: 0.8230\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.7318 - acc: 0.8472 - val_loss: 0.9410 - val_acc: 0.8230\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.7114 - acc: 0.8477 - val_loss: 0.9289 - val_acc: 0.8290\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 1s 144us/step - loss: 2.7809 - acc: 0.5060 - val_loss: 1.9587 - val_acc: 0.6390\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.7703 - acc: 0.6576 - val_loss: 1.4850 - val_acc: 0.6880\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 1.4006 - acc: 0.7136 - val_loss: 1.2586 - val_acc: 0.7290\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.1890 - acc: 0.7464 - val_loss: 1.1338 - val_acc: 0.7510\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.0470 - acc: 0.7779 - val_loss: 1.0633 - val_acc: 0.7790\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.9262 - acc: 0.8006 - val_loss: 0.9988 - val_acc: 0.7970\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.8372 - acc: 0.8208 - val_loss: 0.9621 - val_acc: 0.8030\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.7784 - acc: 0.8316 - val_loss: 0.9300 - val_acc: 0.8130\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.7197 - acc: 0.8459 - val_loss: 0.9023 - val_acc: 0.8230\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.6493 - acc: 0.8563 - val_loss: 0.8882 - val_acc: 0.8230\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.6197 - acc: 0.8658 - val_loss: 0.8663 - val_acc: 0.8250\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.5623 - acc: 0.8812 - val_loss: 0.8481 - val_acc: 0.8260\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.5263 - acc: 0.8900 - val_loss: 0.8443 - val_acc: 0.8290\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.5021 - acc: 0.8910 - val_loss: 0.8382 - val_acc: 0.8290\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.4583 - acc: 0.9003 - val_loss: 0.8291 - val_acc: 0.8300\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.4348 - acc: 0.9058 - val_loss: 0.8268 - val_acc: 0.8350\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.4108 - acc: 0.9136 - val_loss: 0.8251 - val_acc: 0.8350\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.3857 - acc: 0.9188 - val_loss: 0.8239 - val_acc: 0.8320\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.3731 - acc: 0.9194 - val_loss: 0.8238 - val_acc: 0.8310\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.3555 - acc: 0.9231 - val_loss: 0.8381 - val_acc: 0.8320\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.3480 - acc: 0.9225 - val_loss: 0.8326 - val_acc: 0.8380\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3344 - acc: 0.9265 - val_loss: 0.8383 - val_acc: 0.8290\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.3192 - acc: 0.9271 - val_loss: 0.8402 - val_acc: 0.8350\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.3042 - acc: 0.9300 - val_loss: 0.8464 - val_acc: 0.8360\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2939 - acc: 0.9331 - val_loss: 0.8473 - val_acc: 0.8360\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2925 - acc: 0.9317 - val_loss: 0.8541 - val_acc: 0.8340\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2788 - acc: 0.9362 - val_loss: 0.8517 - val_acc: 0.8370\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2679 - acc: 0.9381 - val_loss: 0.8603 - val_acc: 0.8350\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2673 - acc: 0.9359 - val_loss: 0.8636 - val_acc: 0.8360\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2528 - acc: 0.9399 - val_loss: 0.8808 - val_acc: 0.8330\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2535 - acc: 0.9412 - val_loss: 0.8846 - val_acc: 0.8330\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.2408 - acc: 0.9439 - val_loss: 0.8880 - val_acc: 0.8330\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2359 - acc: 0.9436 - val_loss: 0.8969 - val_acc: 0.8330\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2273 - acc: 0.9463 - val_loss: 0.8899 - val_acc: 0.8360\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2314 - acc: 0.9449 - val_loss: 0.9050 - val_acc: 0.8330\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2265 - acc: 0.9449 - val_loss: 0.8994 - val_acc: 0.8290\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2250 - acc: 0.9469 - val_loss: 0.9029 - val_acc: 0.8330\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2217 - acc: 0.9459 - val_loss: 0.9099 - val_acc: 0.8310\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2225 - acc: 0.9455 - val_loss: 0.9186 - val_acc: 0.8280\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.2202 - acc: 0.9465 - val_loss: 0.9153 - val_acc: 0.8350\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2115 - acc: 0.9481 - val_loss: 0.9224 - val_acc: 0.8320\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2106 - acc: 0.9478 - val_loss: 0.9355 - val_acc: 0.8290\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.2082 - acc: 0.9490 - val_loss: 0.9317 - val_acc: 0.8290\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.2070 - acc: 0.9474 - val_loss: 0.9335 - val_acc: 0.8310\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.2012 - acc: 0.9470 - val_loss: 0.9279 - val_acc: 0.8360\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1956 - acc: 0.9493 - val_loss: 0.9349 - val_acc: 0.8290\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1911 - acc: 0.9500 - val_loss: 0.9410 - val_acc: 0.8280\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1947 - acc: 0.9508 - val_loss: 0.9413 - val_acc: 0.8320\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1873 - acc: 0.9514 - val_loss: 0.9478 - val_acc: 0.8300\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1950 - acc: 0.9503 - val_loss: 0.9518 - val_acc: 0.8280\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1832 - acc: 0.9506 - val_loss: 0.9626 - val_acc: 0.8310\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1914 - acc: 0.9510 - val_loss: 0.9736 - val_acc: 0.8260\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1909 - acc: 0.9490 - val_loss: 0.9722 - val_acc: 0.8270\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1877 - acc: 0.9471 - val_loss: 0.9787 - val_acc: 0.8280\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1795 - acc: 0.9524 - val_loss: 0.9651 - val_acc: 0.8280\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1848 - acc: 0.9528 - val_loss: 0.9798 - val_acc: 0.8260\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1793 - acc: 0.9528 - val_loss: 0.9871 - val_acc: 0.8290\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1806 - acc: 0.9534 - val_loss: 0.9996 - val_acc: 0.8290\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1768 - acc: 0.9515 - val_loss: 1.0016 - val_acc: 0.8290\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1767 - acc: 0.9528 - val_loss: 1.0005 - val_acc: 0.8300\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1817 - acc: 0.9518 - val_loss: 1.0011 - val_acc: 0.8280\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1763 - acc: 0.9516 - val_loss: 1.0009 - val_acc: 0.8320\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1714 - acc: 0.9534 - val_loss: 1.0102 - val_acc: 0.8310\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1734 - acc: 0.9544 - val_loss: 1.0162 - val_acc: 0.8250\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1745 - acc: 0.9533 - val_loss: 1.0080 - val_acc: 0.8340\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1717 - acc: 0.9545 - val_loss: 1.0134 - val_acc: 0.8260\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1681 - acc: 0.9543 - val_loss: 1.0158 - val_acc: 0.8280\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1667 - acc: 0.9553 - val_loss: 1.0285 - val_acc: 0.8260\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1651 - acc: 0.9544 - val_loss: 1.0340 - val_acc: 0.8250\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1773 - acc: 0.9518 - val_loss: 1.0201 - val_acc: 0.8240\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1667 - acc: 0.9534 - val_loss: 1.0273 - val_acc: 0.8250\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1722 - acc: 0.9515 - val_loss: 1.0343 - val_acc: 0.8250\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1721 - acc: 0.9535 - val_loss: 1.0319 - val_acc: 0.8240\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1631 - acc: 0.9543 - val_loss: 1.0339 - val_acc: 0.8220\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1669 - acc: 0.9548 - val_loss: 1.0430 - val_acc: 0.8260\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1674 - acc: 0.9521 - val_loss: 1.0370 - val_acc: 0.8290\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1652 - acc: 0.9533 - val_loss: 1.0427 - val_acc: 0.8300\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1647 - acc: 0.9526 - val_loss: 1.0477 - val_acc: 0.8250\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1641 - acc: 0.9518 - val_loss: 1.0618 - val_acc: 0.8230\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1617 - acc: 0.9557 - val_loss: 1.0617 - val_acc: 0.8220\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1632 - acc: 0.9551 - val_loss: 1.0667 - val_acc: 0.8230\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1680 - acc: 0.9525 - val_loss: 1.0530 - val_acc: 0.8260\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1656 - acc: 0.9554 - val_loss: 1.0701 - val_acc: 0.8210\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1649 - acc: 0.9541 - val_loss: 1.0690 - val_acc: 0.8230\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1626 - acc: 0.9536 - val_loss: 1.0611 - val_acc: 0.8240\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.1570 - acc: 0.9546 - val_loss: 1.0767 - val_acc: 0.8250\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1561 - acc: 0.9555 - val_loss: 1.0763 - val_acc: 0.8250\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1560 - acc: 0.9553 - val_loss: 1.0817 - val_acc: 0.8250\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1583 - acc: 0.9544 - val_loss: 1.0837 - val_acc: 0.8210\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1644 - acc: 0.9548 - val_loss: 1.0952 - val_acc: 0.8170\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1666 - acc: 0.9506 - val_loss: 1.1013 - val_acc: 0.8210\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1542 - acc: 0.9565 - val_loss: 1.1047 - val_acc: 0.8220\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1543 - acc: 0.9572 - val_loss: 1.1045 - val_acc: 0.8190\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1594 - acc: 0.9544 - val_loss: 1.1027 - val_acc: 0.8260\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1573 - acc: 0.9553 - val_loss: 1.1092 - val_acc: 0.8200\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1576 - acc: 0.9524 - val_loss: 1.1004 - val_acc: 0.8230\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1546 - acc: 0.9546 - val_loss: 1.1038 - val_acc: 0.8270\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1529 - acc: 0.9560 - val_loss: 1.1026 - val_acc: 0.8230\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1539 - acc: 0.9551 - val_loss: 1.1064 - val_acc: 0.8230\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1567 - acc: 0.9545 - val_loss: 1.1100 - val_acc: 0.8230\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrD9xC3kVTBt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "outputId": "e0a430b6-2981-4aa3-ff41-98c88c2ffa34"
      },
      "source": [
        "plt.scatter(dropout_rates[0:8], val_acc[0:8])\n",
        "plt.show()\n",
        "plt.scatter(l2_penalties[0:8], val_acc[0:8])\n",
        "plt.show()\n",
        "plt.scatter(num_layers[0:8], val_acc[0:8])\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL0mN+/8q3y0qMo24MwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl6SOM+glqeMMeknquP8PWZzjgswyfvEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASm0lEQVR4nO3df2xd533f8fentBwL6BK5E4vFlG2p\ng6wmbbYouTCyGSuCBbYEb4iEpijkIps9tDH2w9mWFhqsoVg69Y8GELAW24ymbme0y5aoRiIIXJZO\ncOF0BYokFVW5UaVMrqKstegAZm0r3TAilpTv/uCRfUVT5qV0yUs+fL+AC53znOeQ30dH+tzD8xye\nm6pCktSu7xt1AZKk5WXQS1LjDHpJapxBL0mNM+glqXG3jLqA+TZv3lxbt24ddRmStKacOHHiL6pq\nfKFtqy7ot27dytTU1KjLkKQ1JcmfXW/bQJdukuxOcjbJuSSPL7D9riRfTnIyydeTPNi1b00ym+S5\n7vXpGx+GJOlGLHpGn2QMeAK4H7gAHE8yWVVn+rr9PPB0Vf1qkncDXwK2dtu+WVXvHW7ZkqRBDXJG\nfy9wrqrOV9VrwGFgz7w+Bby9W34H8OLwSpQk3YxBgn4CeKFv/ULX1u8XgI8mucDc2fzH+7Zt6y7p\n/M8kf2ehb5Dk0SRTSaZmZmYGr16StKhh3V75EPCbVbUFeBD4TJLvA74N3FVVO4GfBT6b5O3zd66q\nJ6uqV1W98fEFJ40lSTdokKCfBu7sW9/StfX7aeBpgKr6CnAbsLmqvltVL3ftJ4BvAvfcbNGSpMEN\nEvTHge1JtiW5FdgHTM7r8+fAhwCSvIu5oJ9JMt5N5pLkh4DtwPlhFS9JWtyid91U1eUkjwHHgDHg\nqao6neQgMFVVk8DPAb+e5BPMTcw+UlWV5MeAg0kuAd8D/nFVvbJso5EkvUlW2/Poe71e+QtTkrQ0\nSU5UVW+hbT7rRpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDVu1T2PXu04enKaQ8fO8uLFWe7YtJH9\nu3awd+f8xyRJWm4GvZbF0ZPTHDhyitlLVwCYvjjLgSOnAAx7aYV56UbL4tCxs6+H/FWzl65w6NjZ\nEVUkrV8GvZbFixdnl9QuafkY9FoWd2zauKR2ScunqaA/enKa+z71LNse/+/c96lnOXpy/tOUtVL2\n79rBxg1j17Rt3DDG/l07RlSRtH41E/RXJ/+mL85SvDH5Z9iPxt6dE3zk/ROMJQCMJXzk/RNOxEoj\n0EzQO/m3uhw9Oc0XTkxzpXs66pUqvnBi2jdeaQSaCXon/1YX33il1aOZoHfyb3XxjVdaPZoJeif/\nVhffeKXVo5mg37tzgl/68fcwsWkjASY2beSXfvw9Tv6NiG+80urR1CMQ9u70ro7V4upx8Fk30ug1\nFfRaXXzjlVaHZi7dSJIWZtBLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxg0U9El2Jzmb5FySxxfY\nfleSLyc5meTrSR7s23ag2+9skl3DLF7S0viZDevTor8wlWQMeAK4H7gAHE8yWVVn+rr9PPB0Vf1q\nkncDXwK2dsv7gB8B7gB+N8k9VXXtYw0lLTs/sH39GuSM/l7gXFWdr6rXgMPAnnl9Cnh7t/wO4MVu\neQ9wuKq+W1XfAs51X0/SCvPR0evXIEE/AbzQt36ha+v3C8BHk1xg7mz+40vYlySPJplKMjUzMzNg\n6ZKWYvo6j4i+XrvaMazJ2IeA36yqLcCDwGeSDPy1q+rJqupVVW98fHxIJUnqd/VjHQdtVzsGeajZ\nNHBn3/qWrq3fTwO7AarqK0luAzYPuK+kFXD1Yx0HbVc7BjnrPg5sT7Itya3MTa5Ozuvz58CHAJK8\nC7gNmOn67UvytiTbgO3AHw6reEmDm7jOh75cr13tWDToq+oy8BhwDPgGc3fXnE5yMMmHu24/B3ws\nyR8DnwMeqTmngaeBM8D/AP6Zd9xIo+GHwaxfqVX2Y1uv16upqalRlyE16ejJaT8MplFJTlRVb6Ft\nfvCItI74YTDrk49AkKTGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4\ng16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMb5CVOSNEIr8fGOBr0kjcjRk9McOHKK2UtX\nAJi+OMuBI6cAhhr2XrqRpBE5dOzs6yF/1eylKxw6dnao38egl6QRefHi7JLab5RBL0kjcsemjUtq\nv1EGvSSNyP5dO9i4Yeyato0bxti/a8dQv4+TsZI0IlcnXL3rRpIatnfnxNCDfT4v3UhS4wx6SWrc\nQEGfZHeSs0nOJXl8ge2/nOS57vV8kot92670bZscZvGSpMUteo0+yRjwBHA/cAE4nmSyqs5c7VNV\nn+jr/3FgZ9+XmK2q9w6vZEnSUgxyRn8vcK6qzlfVa8BhYM9b9H8I+NwwipMk3bxBgn4CeKFv/ULX\n9iZJ7ga2Ac/2Nd+WZCrJV5Psvc5+j3Z9pmZmZgYsXZI0iGFPxu4DPl9V/Q9vuLuqesBPAb+S5K/P\n36mqnqyqXlX1xsfHh1ySJK1vgwT9NHBn3/qWrm0h+5h32aaqprs/zwO/x7XX7yVJy2yQoD8ObE+y\nLcmtzIX5m+6eSfLDwO3AV/rabk/ytm55M3AfcGb+vpKk5bPoXTdVdTnJY8AxYAx4qqpOJzkITFXV\n1dDfBxyuqurb/V3AryX5HnNvKp/qv1tHkrT8cm0uj16v16upqalRlyFJa0qSE9186Jv4m7GS1DiD\nXpIaZ9BLUuMMeklqnEEvSY0z6CWpcX7ClJbN0ZPTy/4RaZIWZ9BrWRw9Oc2BI6eYvTT32KPpi7Mc\nOHIKwLCXVpiXbrQsDh07+3rIXzV76QqHjp0dUUXS+mXQa1m8eHF2Se2Slo9Br2Vxx6aNS2qXtHwM\nei2L/bt2sHHD2DVtGzeMsX/XjhFVJK1fTsZqWVydcPWuG2n0DHotm707Jwx2aRXw0o0kNc6gl6TG\nGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjBgr6JLuT\nnE1yLsnjC2z/5STPda/nk1zs2/Zwkj/tXg8Ps3hJ0uIWfUxxkjHgCeB+4AJwPMlkVZ252qeqPtHX\n/+PAzm75B4BPAj2ggBPdvq8OdRSSpOsa5Iz+XuBcVZ2vqteAw8Cet+j/EPC5bnkX8ExVvdKF+zPA\n7pspWJK0NIME/QTwQt/6ha7tTZLcDWwDnl3KvkkeTTKVZGpmZmaQuiVJAxr2ZOw+4PNVdWUpO1XV\nk1XVq6re+Pj4kEuSpPVtkKCfBu7sW9/StS1kH29ctlnqvpKkZTBI0B8HtifZluRW5sJ8cn6nJD8M\n3A58pa/5GPBAktuT3A480LVJklbIonfdVNXlJI8xF9BjwFNVdTrJQWCqqq6G/j7gcFVV376vJPlF\n5t4sAA5W1SvDHYIk6a2kL5dXhV6vV1NTU6MuQ5LWlCQnqqq30DZ/M1aSGmfQS1LjDHpJapxBL0mN\nM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiD\nXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJatxAQZ9kd5Kz\nSc4lefw6fX4yyZkkp5N8tq/9SpLnutfksAqXJA3mlsU6JBkDngDuBy4Ax5NMVtWZvj7bgQPAfVX1\napIf7PsSs1X13iHXLUka0CBn9PcC56rqfFW9BhwG9szr8zHgiap6FaCqXhpumZKkGzVI0E8AL/St\nX+ja+t0D3JPkD5J8Ncnuvm23JZnq2vfeZL2SpCVa9NLNEr7OduCDwBbg95O8p6ouAndX1XSSHwKe\nTXKqqr7Zv3OSR4FHAe66664hlSRJgsHO6KeBO/vWt3Rt/S4Ak1V1qaq+BTzPXPBTVdPdn+eB3wN2\nzv8GVfVkVfWqqjc+Pr7kQUiSrm+QoD8ObE+yLcmtwD5g/t0zR5k7myfJZuYu5ZxPcnuSt/W13wec\nQZK0Yha9dFNVl5M8BhwDxoCnqup0koPAVFVNdtseSHIGuALsr6qXk/xt4NeSfI+5N5VP9d+tI0la\nfqmqUddwjV6vV1NTU6MuQ5LWlCQnqqq30DZ/M1aSGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCX\npMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklq\nnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNGyjok+xOcjbJuSSPX6fP\nTyY5k+R0ks/2tT+c5E+718PDKlySNJhbFuuQZAx4ArgfuAAcTzJZVWf6+mwHDgD3VdWrSX6wa/8B\n4JNADyjgRLfvq8MfiiRpIYOc0d8LnKuq81X1GnAY2DOvz8eAJ64GeFW91LXvAp6pqle6bc8Au4dT\nuiRpEIME/QTwQt/6ha6t3z3APUn+IMlXk+xewr4keTTJVJKpmZmZwauXJC1qWJOxtwDbgQ8CDwG/\nnmTToDtX1ZNV1auq3vj4+JBKkiTBYEE/DdzZt76la+t3AZisqktV9S3geeaCf5B9JUnLaJCgPw5s\nT7Itya3APmByXp+jzJ3Nk2Qzc5dyzgPHgAeS3J7kduCBrk2StEIWveumqi4neYy5gB4Dnqqq00kO\nAlNVNckbgX4GuALsr6qXAZL8InNvFgAHq+qV5RiIJGlhqapR13CNXq9XU1NToy5DktaUJCeqqrfQ\nNn8zVpIaZ9BLUuMWvUYvqR1HT05z6NhZXrw4yx2bNrJ/1w727nzTr7aoMQa9tE4cPTnNgSOnmL10\nBYDpi7McOHIKwLBvnJdupHXi0LGzr4f8VbOXrnDo2NkRVaSVYtBL68SLF2eX1K52GPTSOnHHpo1L\nalc7DHppndi/awcbN4xd07Zxwxj7d+0YUUVaKU7GSuvE1QlX77pZfwx6aR3Zu3PCYF+HvHQjSY0z\n6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINe\nkhpn0EtS41JVo67hGklmgD+7yS+zGfiLIZQzao5j9WllLK2MA9oZy82O4+6qGl9ow6oL+mFIMlVV\nvVHXcbMcx+rTylhaGQe0M5blHIeXbiSpcQa9JDWu1aB/ctQFDInjWH1aGUsr44B2xrJs42jyGr0k\n6Q2tntFLkjoGvSQ1bs0GfZKnkryU5E+usz1J/n2Sc0m+nuR9K13jIAYYxweTfCfJc93r36x0jYNI\ncmeSLyc5k+R0kn+xQJ+1ckwGGcuqPy5Jbkvyh0n+uBvHv12gz9uS/HZ3TL6WZOvKV/rWBhzHI0lm\n+o7Hz4yi1kElGUtyMskXF9g2/GNSVWvyBfwY8D7gT66z/UHgd4AAHwC+Nuqab3AcHwS+OOo6BxjH\nO4H3dct/BXgeePcaPSaDjGXVH5fu7/n7u+UNwNeAD8zr80+BT3fL+4DfHnXdNziOR4D/OOpalzCm\nnwU+u9C/oeU4Jmv2jL6qfh945S267AH+c835KrApyTtXprrBDTCONaGqvl1Vf9Qt/x/gG8DEvG5r\n5ZgMMpZVr/t7/r/d6obuNf/uiz3Ab3XLnwc+lCQrVOJABhzHmpFkC/D3gN+4TpehH5M1G/QDmABe\n6Fu/wBr8z9r5W92Prb+T5EdGXcxiuh81dzJ35tVvzR2TtxgLrIHj0l0ieA54CXimqq57TKrqMvAd\n4K+ubJWLG2AcAB/pLgl+PsmdK1ziUvwK8K+A711n+9CPSctB34o/Yu4ZFn8T+A/A0RHX85aSfD/w\nBeBfVtVfjrqem7HIWNbEcamqK1X1XmALcG+SHx11TTdigHH8N2BrVf0N4BneOCNeVZL8feClqjqx\nkt+35aCfBvrf1bd0bWtKVf3l1R9bq+pLwIYkm0dc1oKSbGAuGP9rVR1ZoMuaOSaLjWUtHReAqroI\nfBnYPW/T68ckyS3AO4CXV7a6wV1vHFX1clV9t1v9DeD9K13bgO4DPpzkfwOHgb+b5L/M6zP0Y9Jy\n0E8C/7C70+MDwHeq6tujLmqpkvy1q9fnktzL3DFbdf8Ruxr/E/CNqvp31+m2Jo7JIGNZC8clyXiS\nTd3yRuB+4H/N6zYJPNwt/wTwbHWzgKvFIOOYN9fzYebmVVadqjpQVVuqaitzE63PVtVH53Ub+jG5\n5WZ2HqUkn2PuzofNSS4An2Rukoaq+jTwJebu8jgH/D/gH42m0rc2wDh+AvgnSS4Ds8C+1fYfsXMf\n8A+AU921VIB/DdwFa+uYMNhY1sJxeSfwW0nGmHsjerqqvpjkIDBVVZPMvaF9Jsk55m4K2De6cq9r\nkHH88yQfBi4zN45HRlbtDVjuY+IjECSpcS1fupEkYdBLUvMMeklqnEEvSY0z6CWpcQa9JDXOoJek\nxv1/K2QSKJ9BlbMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNQIUuV2YEuY",
        "colab_type": "text"
      },
      "source": [
        "## Refit models to the combined training and validation set data\n",
        "When we are making test set predictions, we want to do so with a model that was fit using as much data as possible.\n",
        "\n",
        "### Chollet's Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf4WT59OYMyc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "9e64804e-ec77-4dd9-84ee-23e9acd17adc"
      },
      "source": [
        "history = chollet_model.fit(x_train_and_val,\n",
        "                    y_train_and_val,\n",
        "                    epochs=20,\n",
        "                    batch_size=512)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ac58a92360a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = chollet_model.fit(x_train_and_val,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0my_train_and_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     batch_size=512)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'chollet_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsbUROQ02N5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c109faa7-2482-4510-b8ec-4b09642c6f39"
      },
      "source": [
        "final_model = models.Sequential()\n",
        "\n",
        "final_model.add(layers.Dropout(rate = 0.45))\n",
        "final_model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "final_model.add(layers.Dropout(rate = 0.45))\n",
        "final_model.add(layers.Dense(64, activation='relu'))\n",
        "final_model.add(layers.Dropout(rate = 0.45))\n",
        "final_model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "final_model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = final_model.fit(x_train_and_val,\n",
        "                    y_train_and_val,\n",
        "                    epochs=100,\n",
        "                    batch_size=512)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 3.2783 - acc: 0.3582 - val_loss: 2.4726 - val_acc: 0.5140\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 2.1864 - acc: 0.4977 - val_loss: 1.7071 - val_acc: 0.5990\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.7090 - acc: 0.6050 - val_loss: 1.4593 - val_acc: 0.6710\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4711 - acc: 0.6646 - val_loss: 1.3192 - val_acc: 0.7050\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.3203 - acc: 0.6917 - val_loss: 1.2166 - val_acc: 0.7220\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1977 - acc: 0.7172 - val_loss: 1.1405 - val_acc: 0.7400\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.0911 - acc: 0.7392 - val_loss: 1.0815 - val_acc: 0.7610\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.0093 - acc: 0.7611 - val_loss: 1.0309 - val_acc: 0.7690\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9261 - acc: 0.7747 - val_loss: 0.9897 - val_acc: 0.7750\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.8562 - acc: 0.7899 - val_loss: 0.9616 - val_acc: 0.7910\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7852 - acc: 0.8086 - val_loss: 0.9415 - val_acc: 0.7950\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7424 - acc: 0.8178 - val_loss: 0.9222 - val_acc: 0.8010\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6845 - acc: 0.8300 - val_loss: 0.9071 - val_acc: 0.8100\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6355 - acc: 0.8416 - val_loss: 0.8935 - val_acc: 0.8200\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.5902 - acc: 0.8522 - val_loss: 0.8957 - val_acc: 0.8110\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.5653 - acc: 0.8607 - val_loss: 0.8929 - val_acc: 0.8200\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.5188 - acc: 0.8730 - val_loss: 0.8824 - val_acc: 0.8240\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.5036 - acc: 0.8728 - val_loss: 0.8905 - val_acc: 0.8210\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.4745 - acc: 0.8810 - val_loss: 0.8785 - val_acc: 0.8240\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4448 - acc: 0.8859 - val_loss: 0.8881 - val_acc: 0.8210\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4283 - acc: 0.8872 - val_loss: 0.8820 - val_acc: 0.8230\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.4064 - acc: 0.8926 - val_loss: 0.8800 - val_acc: 0.8310\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3872 - acc: 0.8995 - val_loss: 0.8838 - val_acc: 0.8310\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.3700 - acc: 0.9032 - val_loss: 0.9092 - val_acc: 0.8250\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3510 - acc: 0.9074 - val_loss: 0.8898 - val_acc: 0.8330\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3523 - acc: 0.9069 - val_loss: 0.9134 - val_acc: 0.8270\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.3516 - acc: 0.9105 - val_loss: 0.9121 - val_acc: 0.8320\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3306 - acc: 0.9103 - val_loss: 0.9239 - val_acc: 0.8290\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.3123 - acc: 0.9194 - val_loss: 0.9244 - val_acc: 0.8310\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.3159 - acc: 0.9162 - val_loss: 0.9234 - val_acc: 0.8320\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.3090 - acc: 0.9163 - val_loss: 0.9136 - val_acc: 0.8310\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2878 - acc: 0.9230 - val_loss: 0.9226 - val_acc: 0.8320\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2865 - acc: 0.9238 - val_loss: 0.9379 - val_acc: 0.8280\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2748 - acc: 0.9245 - val_loss: 0.9558 - val_acc: 0.8210\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2758 - acc: 0.9252 - val_loss: 0.9470 - val_acc: 0.8270\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2482 - acc: 0.9323 - val_loss: 0.9720 - val_acc: 0.8270\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2512 - acc: 0.9321 - val_loss: 0.9790 - val_acc: 0.8240\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2597 - acc: 0.9296 - val_loss: 0.9670 - val_acc: 0.8250\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2407 - acc: 0.9336 - val_loss: 0.9886 - val_acc: 0.8220\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2337 - acc: 0.9345 - val_loss: 0.9997 - val_acc: 0.8270\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2468 - acc: 0.9320 - val_loss: 0.9947 - val_acc: 0.8170\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2332 - acc: 0.9332 - val_loss: 1.0112 - val_acc: 0.8190\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2255 - acc: 0.9366 - val_loss: 1.0132 - val_acc: 0.8180\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2216 - acc: 0.9362 - val_loss: 1.0126 - val_acc: 0.8170\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.2178 - acc: 0.9359 - val_loss: 1.0126 - val_acc: 0.8230\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2197 - acc: 0.9386 - val_loss: 1.0123 - val_acc: 0.8210\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2157 - acc: 0.9410 - val_loss: 1.0202 - val_acc: 0.8230\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.2102 - acc: 0.9392 - val_loss: 1.0442 - val_acc: 0.8220\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2079 - acc: 0.9375 - val_loss: 1.0283 - val_acc: 0.8150\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2094 - acc: 0.9380 - val_loss: 1.0339 - val_acc: 0.8220\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.2063 - acc: 0.9410 - val_loss: 1.0472 - val_acc: 0.8220\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2017 - acc: 0.9432 - val_loss: 1.0394 - val_acc: 0.8170\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2051 - acc: 0.9405 - val_loss: 1.0404 - val_acc: 0.8180\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2006 - acc: 0.9406 - val_loss: 1.0523 - val_acc: 0.8190\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1892 - acc: 0.9449 - val_loss: 1.0726 - val_acc: 0.8250\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1956 - acc: 0.9420 - val_loss: 1.0660 - val_acc: 0.8230\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1873 - acc: 0.9437 - val_loss: 1.0644 - val_acc: 0.8190\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1897 - acc: 0.9442 - val_loss: 1.0781 - val_acc: 0.8200\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1811 - acc: 0.9473 - val_loss: 1.0795 - val_acc: 0.8220\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1880 - acc: 0.9430 - val_loss: 1.0913 - val_acc: 0.8190\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1972 - acc: 0.9412 - val_loss: 1.0766 - val_acc: 0.8220\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1712 - acc: 0.9464 - val_loss: 1.0781 - val_acc: 0.8190\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1806 - acc: 0.9474 - val_loss: 1.0809 - val_acc: 0.8270\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1779 - acc: 0.9425 - val_loss: 1.0870 - val_acc: 0.8210\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1672 - acc: 0.9493 - val_loss: 1.1184 - val_acc: 0.8250\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1707 - acc: 0.9461 - val_loss: 1.1220 - val_acc: 0.8170\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.1728 - acc: 0.9454 - val_loss: 1.0954 - val_acc: 0.8270\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1740 - acc: 0.9480 - val_loss: 1.1071 - val_acc: 0.8260\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1703 - acc: 0.9465 - val_loss: 1.1190 - val_acc: 0.8200\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1645 - acc: 0.9461 - val_loss: 1.1215 - val_acc: 0.8190\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1711 - acc: 0.9480 - val_loss: 1.1233 - val_acc: 0.8230\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1652 - acc: 0.9460 - val_loss: 1.1288 - val_acc: 0.8270\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1689 - acc: 0.9483 - val_loss: 1.1329 - val_acc: 0.8180\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1648 - acc: 0.9498 - val_loss: 1.1416 - val_acc: 0.8200\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1571 - acc: 0.9500 - val_loss: 1.1515 - val_acc: 0.8160\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1562 - acc: 0.9511 - val_loss: 1.1642 - val_acc: 0.8170\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1547 - acc: 0.9531 - val_loss: 1.1597 - val_acc: 0.8190\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1576 - acc: 0.9514 - val_loss: 1.1639 - val_acc: 0.8210\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1613 - acc: 0.9500 - val_loss: 1.1593 - val_acc: 0.8210\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1642 - acc: 0.9486 - val_loss: 1.1566 - val_acc: 0.8220\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1589 - acc: 0.9505 - val_loss: 1.1555 - val_acc: 0.8180\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1569 - acc: 0.9514 - val_loss: 1.1533 - val_acc: 0.8220\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1631 - acc: 0.9490 - val_loss: 1.1443 - val_acc: 0.8200\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1579 - acc: 0.9513 - val_loss: 1.1587 - val_acc: 0.8160\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1531 - acc: 0.9510 - val_loss: 1.1534 - val_acc: 0.8210\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1469 - acc: 0.9535 - val_loss: 1.1740 - val_acc: 0.8260\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1508 - acc: 0.9493 - val_loss: 1.1743 - val_acc: 0.8190\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.1539 - acc: 0.9530 - val_loss: 1.1790 - val_acc: 0.8170\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1456 - acc: 0.9518 - val_loss: 1.1714 - val_acc: 0.8190\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1487 - acc: 0.9534 - val_loss: 1.1691 - val_acc: 0.8220\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1497 - acc: 0.9496 - val_loss: 1.1689 - val_acc: 0.8220\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1486 - acc: 0.9529 - val_loss: 1.1725 - val_acc: 0.8230\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1397 - acc: 0.9541 - val_loss: 1.1790 - val_acc: 0.8180\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1397 - acc: 0.9533 - val_loss: 1.1754 - val_acc: 0.8200\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1399 - acc: 0.9541 - val_loss: 1.1899 - val_acc: 0.8160\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1461 - acc: 0.9518 - val_loss: 1.2006 - val_acc: 0.8170\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1425 - acc: 0.9538 - val_loss: 1.1966 - val_acc: 0.8180\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1424 - acc: 0.9523 - val_loss: 1.1926 - val_acc: 0.8180\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1485 - acc: 0.9514 - val_loss: 1.2038 - val_acc: 0.8180\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1433 - acc: 0.9533 - val_loss: 1.1957 - val_acc: 0.8160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WucnaK6g2pAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "d848432a-1dcf-41d0-8e68-abdb0e7e6fb1"
      },
      "source": [
        "print(chollet_model.evaluate(x_test, y_test))\n",
        "print(final_model.evaluate(x_test, y_test))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2246/2246 [==============================] - 0s 113us/step\n",
            "[1.2420880563112635, 0.7760463045944832]\n",
            "2246/2246 [==============================] - 0s 107us/step\n",
            "[1.4232519968003963, 0.8023152271234235]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l963rFnQqnvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}