{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSYV3U4JZdzt",
        "colab_type": "text"
      },
      "source": [
        "## Goals\n",
        "\n",
        "The goals of the coding part of this homework assignment are to:\n",
        " * Practice the process of finding a good neural network model\n",
        " * Practice tuning hyperparameters for a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO0Kphe9bPJk",
        "colab_type": "text"
      },
      "source": [
        "## Module Imports\n",
        "Please add code below to import numpy and any keras submodules you need.  Set a seed for random number generation from numpy before importing keras.  I've already imported pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FzuhTFCZHoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "np.random.seed(987442)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiN8hvUMaHO6",
        "colab_type": "text"
      },
      "source": [
        "## Problem 1: Forest cover type prediction\n",
        "We have a data set from with characteristics of land in national forests; our goal is to predict the type of forest on that land.  There are 7 possible forest types: Spruce/Fir, Lodgepole Pine, Ponderosa Pine, Cottonwood/Willow, Aspen, Douglas-fir, or Krummholz.  Our features are things like elevation, slope of land, distance to water, soil type, and so on.  (Original data source: https://archive.ics.uci.edu/ml/datasets/Covertype)\n",
        "\n",
        "I didn't trust their train/validation/test split, so I have re-split the data using a random split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNp5PN3ubNcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read in data\n",
        "forest_cover = pd.read_csv(\"http://www.evanlray.com/data/UCIML/forest_cover/covtype.data\")\n",
        "\n",
        "# drop a couple of columns of all 0's\n",
        "to_drop = [20, 28]\n",
        "forest_cover.drop(forest_cover.columns[to_drop], axis = 1, inplace=True)\n",
        "\n",
        "# extract X and y (train, validaton, and test all together)\n",
        "X = forest_cover.iloc[:, 0:52].to_numpy()\n",
        "y = forest_cover.iloc[:, 52].to_numpy() - 1\n",
        "\n",
        "# perform a train/validation/test split\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "  X, y, test_size=0.5, random_state=5435)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "  X_train_val, y_train_val, test_size=0.5, random_state=208963)\n",
        "\n",
        "# normalize features\n",
        "X_train_mean = np.mean(X_train, axis = 0)\n",
        "X_train = X_train - X_train_mean\n",
        "X_train_std = np.std(X_train, axis = 0)\n",
        "X_train = X_train / X_train_std\n",
        "\n",
        "# normalize X_val, but using X_train_mean and X_train_std to do the normalization\n",
        "X_val = X_val - X_train_mean\n",
        "X_val = X_val / X_train_std\n",
        "\n",
        "# normalize X_test, but using X_train_mean and X_train_std to do the normalization\n",
        "X_test = X_test - X_train_mean\n",
        "X_test = X_test / X_train_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC2-YMnQsarO",
        "colab_type": "text"
      },
      "source": [
        "#### Here's a starting model that we used in problem set 2.\n",
        "\n",
        "The model takes several minutes to fit, so you don't have to rerun this code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JZjbKGte8Gj",
        "colab_type": "code",
        "outputId": "2684b2d0-9b15-49b7-a3b6-86b6432edaea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# define a multinomial logistic regression model: one layer, with a sigmoid activation and 2 inputs\n",
        "multi_logistic_model = models.Sequential()\n",
        "multi_logistic_model.add(layers.Dense(\n",
        "    7,\n",
        "    activation = 'softmax',\n",
        "    input_shape = (52,)))\n",
        "\n",
        "# compile the model using stochastic gradient descent for optimization,\n",
        "# binary cross-entropy loss, and measuring performance by classification accuracy\n",
        "multi_logistic_model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics = ['accuracy'])\n",
        "\n",
        "# Estimate the model parameters\n",
        "history = multi_logistic_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data = (X_val, y_val),\n",
        "    epochs = 50,\n",
        "    batch_size = 512)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 145252 samples, validate on 145253 samples\n",
            "Epoch 1/50\n",
            "145252/145252 [==============================] - 2s 12us/step - loss: 1.7926 - acc: 0.3868 - val_loss: 1.3132 - val_acc: 0.5815\n",
            "Epoch 2/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 1.1455 - acc: 0.6244 - val_loss: 1.0307 - val_acc: 0.6550\n",
            "Epoch 3/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.9660 - acc: 0.6731 - val_loss: 0.9138 - val_acc: 0.6834\n",
            "Epoch 4/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.8772 - acc: 0.6915 - val_loss: 0.8474 - val_acc: 0.6974\n",
            "Epoch 5/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.8243 - acc: 0.7029 - val_loss: 0.8052 - val_acc: 0.7051\n",
            "Epoch 6/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.7898 - acc: 0.7089 - val_loss: 0.7763 - val_acc: 0.7091\n",
            "Epoch 7/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.7655 - acc: 0.7108 - val_loss: 0.7558 - val_acc: 0.7109\n",
            "Epoch 8/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.7490 - acc: 0.7121 - val_loss: 0.7421 - val_acc: 0.7116\n",
            "Epoch 9/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.7371 - acc: 0.7131 - val_loss: 0.7318 - val_acc: 0.7121\n",
            "Epoch 10/50\n",
            "145252/145252 [==============================] - 1s 9us/step - loss: 0.7278 - acc: 0.7139 - val_loss: 0.7234 - val_acc: 0.7115\n",
            "Epoch 11/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.7200 - acc: 0.7132 - val_loss: 0.7163 - val_acc: 0.7112\n",
            "Epoch 12/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.7135 - acc: 0.7138 - val_loss: 0.7099 - val_acc: 0.7120\n",
            "Epoch 13/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.7077 - acc: 0.7139 - val_loss: 0.7050 - val_acc: 0.7127\n",
            "Epoch 14/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.7028 - acc: 0.7142 - val_loss: 0.7001 - val_acc: 0.7139\n",
            "Epoch 15/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6985 - acc: 0.7147 - val_loss: 0.6958 - val_acc: 0.7140\n",
            "Epoch 16/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6944 - acc: 0.7153 - val_loss: 0.6921 - val_acc: 0.7130\n",
            "Epoch 17/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6908 - acc: 0.7154 - val_loss: 0.6886 - val_acc: 0.7133\n",
            "Epoch 18/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6874 - acc: 0.7157 - val_loss: 0.6854 - val_acc: 0.7151\n",
            "Epoch 19/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6844 - acc: 0.7168 - val_loss: 0.6829 - val_acc: 0.7140\n",
            "Epoch 20/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6819 - acc: 0.7175 - val_loss: 0.6805 - val_acc: 0.7153\n",
            "Epoch 21/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6797 - acc: 0.7177 - val_loss: 0.6783 - val_acc: 0.7165\n",
            "Epoch 22/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6779 - acc: 0.7177 - val_loss: 0.6768 - val_acc: 0.7155\n",
            "Epoch 23/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6764 - acc: 0.7184 - val_loss: 0.6754 - val_acc: 0.7173\n",
            "Epoch 24/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6750 - acc: 0.7189 - val_loss: 0.6739 - val_acc: 0.7169\n",
            "Epoch 25/50\n",
            "145252/145252 [==============================] - 1s 9us/step - loss: 0.6738 - acc: 0.7187 - val_loss: 0.6726 - val_acc: 0.7178\n",
            "Epoch 26/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6728 - acc: 0.7191 - val_loss: 0.6717 - val_acc: 0.7178\n",
            "Epoch 27/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6718 - acc: 0.7190 - val_loss: 0.6709 - val_acc: 0.7170\n",
            "Epoch 28/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6709 - acc: 0.7201 - val_loss: 0.6700 - val_acc: 0.7167\n",
            "Epoch 29/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6700 - acc: 0.7195 - val_loss: 0.6690 - val_acc: 0.7178\n",
            "Epoch 30/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6692 - acc: 0.7201 - val_loss: 0.6682 - val_acc: 0.7195\n",
            "Epoch 31/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6684 - acc: 0.7205 - val_loss: 0.6675 - val_acc: 0.7190\n",
            "Epoch 32/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6677 - acc: 0.7202 - val_loss: 0.6668 - val_acc: 0.7191\n",
            "Epoch 33/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6671 - acc: 0.7205 - val_loss: 0.6661 - val_acc: 0.7183\n",
            "Epoch 34/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6665 - acc: 0.7207 - val_loss: 0.6658 - val_acc: 0.7186\n",
            "Epoch 35/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6660 - acc: 0.7209 - val_loss: 0.6650 - val_acc: 0.7191\n",
            "Epoch 36/50\n",
            "145252/145252 [==============================] - 1s 9us/step - loss: 0.6655 - acc: 0.7215 - val_loss: 0.6646 - val_acc: 0.7190\n",
            "Epoch 37/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6649 - acc: 0.7211 - val_loss: 0.6639 - val_acc: 0.7183\n",
            "Epoch 38/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6645 - acc: 0.7210 - val_loss: 0.6638 - val_acc: 0.7208\n",
            "Epoch 39/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6641 - acc: 0.7217 - val_loss: 0.6630 - val_acc: 0.7203\n",
            "Epoch 40/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6637 - acc: 0.7214 - val_loss: 0.6627 - val_acc: 0.7196\n",
            "Epoch 41/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6633 - acc: 0.7217 - val_loss: 0.6626 - val_acc: 0.7207\n",
            "Epoch 42/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6629 - acc: 0.7214 - val_loss: 0.6620 - val_acc: 0.7216\n",
            "Epoch 43/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6626 - acc: 0.7223 - val_loss: 0.6617 - val_acc: 0.7207\n",
            "Epoch 44/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6622 - acc: 0.7222 - val_loss: 0.6614 - val_acc: 0.7203\n",
            "Epoch 45/50\n",
            "145252/145252 [==============================] - 1s 9us/step - loss: 0.6619 - acc: 0.7224 - val_loss: 0.6611 - val_acc: 0.7203\n",
            "Epoch 46/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6616 - acc: 0.7220 - val_loss: 0.6609 - val_acc: 0.7198\n",
            "Epoch 47/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6614 - acc: 0.7221 - val_loss: 0.6604 - val_acc: 0.7213\n",
            "Epoch 48/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6611 - acc: 0.7226 - val_loss: 0.6604 - val_acc: 0.7208\n",
            "Epoch 49/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6609 - acc: 0.7224 - val_loss: 0.6602 - val_acc: 0.7203\n",
            "Epoch 50/50\n",
            "145252/145252 [==============================] - 1s 8us/step - loss: 0.6606 - acc: 0.7222 - val_loss: 0.6596 - val_acc: 0.7202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O5VUmsFEwlo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3e03a1d9-8bbf-4bfa-85a9-ecfbbf8eb2a8"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, label='Training acc')\n",
        "plt.plot(epochs, val_acc, label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSer/B6U4xZy9PCwCAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl_1PJeVtnYQ",
        "colab_type": "text"
      },
      "source": [
        "#### (a) Create a neural network model that overfits the training data.\n",
        "For your convenience I have copied the code above and pasted it in below, renaming the model to \"overfit_model\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccaxm7eD-qOc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42d2b7dc-bf39-4c48-ac80-fb6b195d3779"
      },
      "source": [
        "overfit_model = models.Sequential()\n",
        "\n",
        "overfit_model.add(layers.Dense(\n",
        "    1024,\n",
        "    activation = 'relu',\n",
        "    input_shape = (52,)))\n",
        "\n",
        "overfit_model.add(layers.Dense(\n",
        "    1024,\n",
        "    activation = 'relu'))\n",
        "\n",
        "overfit_model.add(layers.Dense(\n",
        "    1024,\n",
        "    activation = 'relu'))\n",
        "\n",
        "overfit_model.add(layers.Dense(\n",
        "    1024,\n",
        "    activation = 'relu'))\n",
        "\n",
        "overfit_model.add(layers.Dense(\n",
        "    1024,\n",
        "    activation = 'relu'))\n",
        "\n",
        "overfit_model.add(layers.Dense(\n",
        "    1024,\n",
        "    activation = 'relu'))\n",
        "\n",
        "overfit_model.add(layers.Dense(\n",
        "    1024,\n",
        "    activation = 'relu'))\n",
        "\n",
        "overfit_model.add(layers.Dense(\n",
        "    1024,\n",
        "    activation = 'relu'))\n",
        "\n",
        "overfit_model.add(layers.Dense(\n",
        "    1024,\n",
        "    activation = 'relu'))\n",
        "\n",
        "overfit_model.add(layers.Dense(\n",
        "    1024,\n",
        "    activation = 'relu'))\n",
        "\n",
        "overfit_model.add(layers.Dense(\n",
        "    1024,\n",
        "    activation = 'relu'))\n",
        "\n",
        "overfit_model.add(layers.Dense(\n",
        "    7,\n",
        "    activation = 'softmax'))\n",
        "\n",
        "# compile the model using stochastic gradient descent for optimization,\n",
        "# binary cross-entropy loss, and measuring performance by classification accuracy\n",
        "overfit_model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics = ['accuracy'])\n",
        "\n",
        "# Estimate the model parameters\n",
        "history = overfit_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data = (X_val, y_val),\n",
        "    epochs = 50,\n",
        "    batch_size = 512)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 145252 samples, validate on 145253 samples\n",
            "Epoch 1/50\n",
            "145252/145252 [==============================] - 10s 67us/step - loss: 0.7402 - acc: 0.6753 - val_loss: 0.5648 - val_acc: 0.7729\n",
            "Epoch 2/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.5090 - acc: 0.7895 - val_loss: 0.4633 - val_acc: 0.8076\n",
            "Epoch 3/50\n",
            "145252/145252 [==============================] - 8s 58us/step - loss: 0.4344 - acc: 0.8221 - val_loss: 0.4183 - val_acc: 0.8280\n",
            "Epoch 4/50\n",
            "145252/145252 [==============================] - 8s 58us/step - loss: 0.3780 - acc: 0.8460 - val_loss: 0.3787 - val_acc: 0.8480\n",
            "Epoch 5/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.3388 - acc: 0.8634 - val_loss: 0.3444 - val_acc: 0.8626\n",
            "Epoch 6/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.3080 - acc: 0.8764 - val_loss: 0.3090 - val_acc: 0.8777\n",
            "Epoch 7/50\n",
            "145252/145252 [==============================] - 8s 58us/step - loss: 0.2784 - acc: 0.8891 - val_loss: 0.2941 - val_acc: 0.8835\n",
            "Epoch 8/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.2587 - acc: 0.8962 - val_loss: 0.2792 - val_acc: 0.8897\n",
            "Epoch 9/50\n",
            "145252/145252 [==============================] - 8s 58us/step - loss: 0.2427 - acc: 0.9023 - val_loss: 0.2606 - val_acc: 0.8947\n",
            "Epoch 10/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.2285 - acc: 0.9076 - val_loss: 0.2474 - val_acc: 0.9027\n",
            "Epoch 11/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.2140 - acc: 0.9143 - val_loss: 0.2431 - val_acc: 0.9069\n",
            "Epoch 12/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.2046 - acc: 0.9178 - val_loss: 0.2452 - val_acc: 0.9058\n",
            "Epoch 13/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1916 - acc: 0.9232 - val_loss: 0.2353 - val_acc: 0.9078\n",
            "Epoch 14/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1845 - acc: 0.9255 - val_loss: 0.2329 - val_acc: 0.9124\n",
            "Epoch 15/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1784 - acc: 0.9283 - val_loss: 0.2167 - val_acc: 0.9182\n",
            "Epoch 16/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1668 - acc: 0.9326 - val_loss: 0.2109 - val_acc: 0.9206\n",
            "Epoch 17/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1652 - acc: 0.9333 - val_loss: 0.2156 - val_acc: 0.9188\n",
            "Epoch 18/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1620 - acc: 0.9346 - val_loss: 0.2090 - val_acc: 0.9211\n",
            "Epoch 19/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1533 - acc: 0.9380 - val_loss: 0.2224 - val_acc: 0.9209\n",
            "Epoch 20/50\n",
            "145252/145252 [==============================] - 8s 58us/step - loss: 0.1478 - acc: 0.9395 - val_loss: 0.2068 - val_acc: 0.9241\n",
            "Epoch 21/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1457 - acc: 0.9419 - val_loss: 0.2200 - val_acc: 0.9201\n",
            "Epoch 22/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1386 - acc: 0.9433 - val_loss: 0.2036 - val_acc: 0.9288\n",
            "Epoch 23/50\n",
            "145252/145252 [==============================] - 8s 58us/step - loss: 0.1394 - acc: 0.9443 - val_loss: 0.2034 - val_acc: 0.9285\n",
            "Epoch 24/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1307 - acc: 0.9468 - val_loss: 0.2062 - val_acc: 0.9266\n",
            "Epoch 25/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1267 - acc: 0.9484 - val_loss: 0.2054 - val_acc: 0.9259\n",
            "Epoch 26/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1242 - acc: 0.9495 - val_loss: 0.2051 - val_acc: 0.9301\n",
            "Epoch 27/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1219 - acc: 0.9505 - val_loss: 0.2085 - val_acc: 0.9289\n",
            "Epoch 28/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1202 - acc: 0.9513 - val_loss: 0.2118 - val_acc: 0.9282\n",
            "Epoch 29/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1132 - acc: 0.9537 - val_loss: 0.2154 - val_acc: 0.9295\n",
            "Epoch 30/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1109 - acc: 0.9549 - val_loss: 0.2016 - val_acc: 0.9323\n",
            "Epoch 31/50\n",
            "145252/145252 [==============================] - 8s 58us/step - loss: 0.1131 - acc: 0.9542 - val_loss: 0.2307 - val_acc: 0.9255\n",
            "Epoch 32/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1196 - acc: 0.9526 - val_loss: 0.2047 - val_acc: 0.9328\n",
            "Epoch 33/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1044 - acc: 0.9576 - val_loss: 0.2010 - val_acc: 0.9342\n",
            "Epoch 34/50\n",
            "145252/145252 [==============================] - 8s 58us/step - loss: 0.1020 - acc: 0.9588 - val_loss: 0.1986 - val_acc: 0.9331\n",
            "Epoch 35/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1003 - acc: 0.9594 - val_loss: 0.2103 - val_acc: 0.9307\n",
            "Epoch 36/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.1002 - acc: 0.9593 - val_loss: 0.2055 - val_acc: 0.9331\n",
            "Epoch 37/50\n",
            "145252/145252 [==============================] - 8s 58us/step - loss: 0.0964 - acc: 0.9606 - val_loss: 0.2113 - val_acc: 0.9350\n",
            "Epoch 38/50\n",
            "145252/145252 [==============================] - 8s 58us/step - loss: 0.0946 - acc: 0.9614 - val_loss: 0.1996 - val_acc: 0.9354\n",
            "Epoch 39/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.0930 - acc: 0.9622 - val_loss: 0.2186 - val_acc: 0.9338\n",
            "Epoch 40/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.0929 - acc: 0.9628 - val_loss: 0.2028 - val_acc: 0.9350\n",
            "Epoch 41/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.0917 - acc: 0.9627 - val_loss: 0.2040 - val_acc: 0.9320\n",
            "Epoch 42/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.0907 - acc: 0.9638 - val_loss: 0.2080 - val_acc: 0.9369\n",
            "Epoch 43/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.0894 - acc: 0.9638 - val_loss: 0.2002 - val_acc: 0.9354\n",
            "Epoch 44/50\n",
            "145252/145252 [==============================] - 8s 58us/step - loss: 0.0879 - acc: 0.9652 - val_loss: 0.2131 - val_acc: 0.9356\n",
            "Epoch 45/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.0870 - acc: 0.9651 - val_loss: 0.2031 - val_acc: 0.9378\n",
            "Epoch 46/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.0823 - acc: 0.9672 - val_loss: 0.2101 - val_acc: 0.9363\n",
            "Epoch 47/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.0804 - acc: 0.9674 - val_loss: 0.2098 - val_acc: 0.9359\n",
            "Epoch 48/50\n",
            "145252/145252 [==============================] - 8s 58us/step - loss: 0.0815 - acc: 0.9672 - val_loss: 0.2207 - val_acc: 0.9354\n",
            "Epoch 49/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.0796 - acc: 0.9686 - val_loss: 0.2123 - val_acc: 0.9397\n",
            "Epoch 50/50\n",
            "145252/145252 [==============================] - 9s 59us/step - loss: 0.0763 - acc: 0.9695 - val_loss: 0.1994 - val_acc: 0.9373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr_5yHaMPRzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ionosphere = pd.read_csv(\"http://www.evanlray.com/data/UCIML/ionosphere/ionosphere.data\", header = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvYlrgDCPf74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ce103a99-dcdb-48b6-d8c4-742924315351"
      },
      "source": [
        "np.all(ionosphere.iloc[:, 1] == 0)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiDamyySQKou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5101ac88-8e9b-4de5-f89b-c2fd8721ac2e"
      },
      "source": [
        "ionosphere.columns"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "            34],\n",
              "           dtype='int64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOFFsy-cF0K1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c1cb73d0-cc94-49dd-a46f-f05099e234e2"
      },
      "source": [
        "# read in data\n",
        "ionosphere = pd.read_csv(\"http://www.evanlray.com/data/UCIML/ionosphere/ionosphere.data\")\n",
        "\n",
        "# drop a couple of columns of all 0's\n",
        "to_drop = [2]\n",
        "ionosphere.drop(ionosphere.columns[to_drop], axis = 1, inplace=True)\n",
        "\n",
        "# extract X and y (train, validaton, and test all together)\n",
        "X = ionosphere.iloc[:, 0:-1].to_numpy()\n",
        "y = pd.get_dummies(ionosphere.iloc[:, -1]).to_numpy()[:,1]\n",
        "\n",
        "# perform a train/validation/test split\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "  X, y, test_size=0.05, random_state=5435)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "  X_train_val, y_train_val, test_size=0.05, random_state=208963)\n",
        "\n",
        "# normalize features\n",
        "X_train_mean = np.mean(X_train, axis = 0)\n",
        "X_train = X_train - X_train_mean\n",
        "X_train_std = np.std(X_train, axis = 0)\n",
        "X_train = X_train / X_train_std\n",
        "\n",
        "# normalize X_val, but using X_train_mean and X_train_std to do the normalization\n",
        "X_val = X_val - X_train_mean\n",
        "X_val = X_val / X_train_std\n",
        "\n",
        "# normalize X_test, but using X_train_mean and X_train_std to do the normalization\n",
        "X_test = X_test - X_train_mean\n",
        "X_test = X_test / X_train_std"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybD0Nc-xVTwB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "965837ff-0e73-4adf-875f-abd159136891"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(315, 33)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGtcMKKuUhGA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4bc6e015-e4e7-42e7-eda1-03ccc5f71822"
      },
      "source": [
        "# define a multinomial logistic regression model: one layer, with a sigmoid activation and 2 inputs\n",
        "baseline_model = models.Sequential()\n",
        "baseline_model.add(layers.Dense(\n",
        "    128,\n",
        "    activation = 'relu',\n",
        "    input_shape = (33,)))\n",
        "\n",
        "baseline_model.add(layers.Dense(\n",
        "    128,\n",
        "    activation = 'relu',\n",
        "    input_shape = (33,)))\n",
        "baseline_model.add(layers.Dense(\n",
        "    128,\n",
        "    activation = 'relu',\n",
        "    input_shape = (33,)))\n",
        "\n",
        "baseline_model.add(layers.Dense(\n",
        "    1,\n",
        "    activation = 'sigmoid'))\n",
        "\n",
        "# compile the model using stochastic gradient descent for optimization,\n",
        "# binary cross-entropy loss, and measuring performance by classification accuracy\n",
        "baseline_model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy'])\n",
        "\n",
        "# Estimate the model parameters\n",
        "history = baseline_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data = (X_val, y_val),\n",
        "    epochs = 500,\n",
        "    batch_size = X_train.shape[0])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 315 samples, validate on 17 samples\n",
            "Epoch 1/500\n",
            "315/315 [==============================] - 1s 3ms/step - loss: 0.6931 - acc: 0.3492 - val_loss: 0.6932 - val_acc: 0.4706\n",
            "Epoch 2/500\n",
            "315/315 [==============================] - 0s 37us/step - loss: 0.6930 - acc: 0.6508 - val_loss: 0.6932 - val_acc: 0.4706\n",
            "Epoch 3/500\n",
            "315/315 [==============================] - 0s 32us/step - loss: 0.6929 - acc: 0.6508 - val_loss: 0.6932 - val_acc: 0.4706\n",
            "Epoch 4/500\n",
            "315/315 [==============================] - 0s 43us/step - loss: 0.6928 - acc: 0.6508 - val_loss: 0.6933 - val_acc: 0.4706\n",
            "Epoch 5/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6926 - acc: 0.6508 - val_loss: 0.6933 - val_acc: 0.4706\n",
            "Epoch 6/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6925 - acc: 0.6508 - val_loss: 0.6933 - val_acc: 0.4706\n",
            "Epoch 7/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6923 - acc: 0.6508 - val_loss: 0.6933 - val_acc: 0.4706\n",
            "Epoch 8/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6922 - acc: 0.6508 - val_loss: 0.6934 - val_acc: 0.4706\n",
            "Epoch 9/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6920 - acc: 0.6508 - val_loss: 0.6934 - val_acc: 0.4706\n",
            "Epoch 10/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6919 - acc: 0.6508 - val_loss: 0.6934 - val_acc: 0.4706\n",
            "Epoch 11/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6918 - acc: 0.6508 - val_loss: 0.6935 - val_acc: 0.4706\n",
            "Epoch 12/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6916 - acc: 0.6508 - val_loss: 0.6935 - val_acc: 0.4706\n",
            "Epoch 13/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6915 - acc: 0.6508 - val_loss: 0.6935 - val_acc: 0.4706\n",
            "Epoch 14/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6913 - acc: 0.6508 - val_loss: 0.6936 - val_acc: 0.4706\n",
            "Epoch 15/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6912 - acc: 0.6508 - val_loss: 0.6936 - val_acc: 0.4706\n",
            "Epoch 16/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6910 - acc: 0.6508 - val_loss: 0.6936 - val_acc: 0.4706\n",
            "Epoch 17/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6909 - acc: 0.6508 - val_loss: 0.6937 - val_acc: 0.4706\n",
            "Epoch 18/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6907 - acc: 0.6508 - val_loss: 0.6937 - val_acc: 0.4706\n",
            "Epoch 19/500\n",
            "315/315 [==============================] - 0s 17us/step - loss: 0.6906 - acc: 0.6508 - val_loss: 0.6937 - val_acc: 0.4706\n",
            "Epoch 20/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6904 - acc: 0.6508 - val_loss: 0.6938 - val_acc: 0.4706\n",
            "Epoch 21/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6903 - acc: 0.6508 - val_loss: 0.6938 - val_acc: 0.4706\n",
            "Epoch 22/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6901 - acc: 0.6508 - val_loss: 0.6938 - val_acc: 0.4706\n",
            "Epoch 23/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6900 - acc: 0.6508 - val_loss: 0.6939 - val_acc: 0.4706\n",
            "Epoch 24/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6898 - acc: 0.6508 - val_loss: 0.6939 - val_acc: 0.4706\n",
            "Epoch 25/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6897 - acc: 0.6508 - val_loss: 0.6939 - val_acc: 0.4706\n",
            "Epoch 26/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6895 - acc: 0.6508 - val_loss: 0.6940 - val_acc: 0.4706\n",
            "Epoch 27/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6894 - acc: 0.6508 - val_loss: 0.6940 - val_acc: 0.4706\n",
            "Epoch 28/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6893 - acc: 0.6508 - val_loss: 0.6940 - val_acc: 0.4706\n",
            "Epoch 29/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6891 - acc: 0.6508 - val_loss: 0.6941 - val_acc: 0.4706\n",
            "Epoch 30/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6890 - acc: 0.6508 - val_loss: 0.6941 - val_acc: 0.4706\n",
            "Epoch 31/500\n",
            "315/315 [==============================] - 0s 32us/step - loss: 0.6888 - acc: 0.6508 - val_loss: 0.6942 - val_acc: 0.4706\n",
            "Epoch 32/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6887 - acc: 0.6508 - val_loss: 0.6942 - val_acc: 0.4706\n",
            "Epoch 33/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6885 - acc: 0.6508 - val_loss: 0.6942 - val_acc: 0.4706\n",
            "Epoch 34/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6884 - acc: 0.6508 - val_loss: 0.6943 - val_acc: 0.4706\n",
            "Epoch 35/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6883 - acc: 0.6508 - val_loss: 0.6943 - val_acc: 0.4706\n",
            "Epoch 36/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6881 - acc: 0.6508 - val_loss: 0.6943 - val_acc: 0.4706\n",
            "Epoch 37/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6880 - acc: 0.6508 - val_loss: 0.6944 - val_acc: 0.4706\n",
            "Epoch 38/500\n",
            "315/315 [==============================] - 0s 35us/step - loss: 0.6878 - acc: 0.6508 - val_loss: 0.6944 - val_acc: 0.4706\n",
            "Epoch 39/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6877 - acc: 0.6508 - val_loss: 0.6945 - val_acc: 0.4706\n",
            "Epoch 40/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6875 - acc: 0.6508 - val_loss: 0.6945 - val_acc: 0.4706\n",
            "Epoch 41/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6874 - acc: 0.6508 - val_loss: 0.6945 - val_acc: 0.4706\n",
            "Epoch 42/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6873 - acc: 0.6508 - val_loss: 0.6946 - val_acc: 0.4706\n",
            "Epoch 43/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6871 - acc: 0.6508 - val_loss: 0.6946 - val_acc: 0.4706\n",
            "Epoch 44/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6870 - acc: 0.6508 - val_loss: 0.6947 - val_acc: 0.4706\n",
            "Epoch 45/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6869 - acc: 0.6508 - val_loss: 0.6947 - val_acc: 0.4706\n",
            "Epoch 46/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6867 - acc: 0.6508 - val_loss: 0.6947 - val_acc: 0.4706\n",
            "Epoch 47/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6866 - acc: 0.6508 - val_loss: 0.6948 - val_acc: 0.4706\n",
            "Epoch 48/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6864 - acc: 0.6508 - val_loss: 0.6948 - val_acc: 0.4706\n",
            "Epoch 49/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6863 - acc: 0.6508 - val_loss: 0.6949 - val_acc: 0.4706\n",
            "Epoch 50/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6862 - acc: 0.6508 - val_loss: 0.6949 - val_acc: 0.4706\n",
            "Epoch 51/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6860 - acc: 0.6508 - val_loss: 0.6949 - val_acc: 0.4706\n",
            "Epoch 52/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6859 - acc: 0.6508 - val_loss: 0.6950 - val_acc: 0.4706\n",
            "Epoch 53/500\n",
            "315/315 [==============================] - 0s 18us/step - loss: 0.6858 - acc: 0.6508 - val_loss: 0.6950 - val_acc: 0.4706\n",
            "Epoch 54/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6856 - acc: 0.6508 - val_loss: 0.6951 - val_acc: 0.4706\n",
            "Epoch 55/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6855 - acc: 0.6508 - val_loss: 0.6951 - val_acc: 0.4706\n",
            "Epoch 56/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6854 - acc: 0.6508 - val_loss: 0.6951 - val_acc: 0.4706\n",
            "Epoch 57/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6852 - acc: 0.6508 - val_loss: 0.6952 - val_acc: 0.4706\n",
            "Epoch 58/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6851 - acc: 0.6508 - val_loss: 0.6952 - val_acc: 0.4706\n",
            "Epoch 59/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6850 - acc: 0.6508 - val_loss: 0.6953 - val_acc: 0.4706\n",
            "Epoch 60/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6848 - acc: 0.6508 - val_loss: 0.6953 - val_acc: 0.4706\n",
            "Epoch 61/500\n",
            "315/315 [==============================] - 0s 18us/step - loss: 0.6847 - acc: 0.6508 - val_loss: 0.6954 - val_acc: 0.4706\n",
            "Epoch 62/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6846 - acc: 0.6508 - val_loss: 0.6954 - val_acc: 0.4706\n",
            "Epoch 63/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6844 - acc: 0.6508 - val_loss: 0.6954 - val_acc: 0.4706\n",
            "Epoch 64/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6843 - acc: 0.6508 - val_loss: 0.6955 - val_acc: 0.4706\n",
            "Epoch 65/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6842 - acc: 0.6508 - val_loss: 0.6955 - val_acc: 0.4706\n",
            "Epoch 66/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6840 - acc: 0.6508 - val_loss: 0.6956 - val_acc: 0.4706\n",
            "Epoch 67/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6839 - acc: 0.6508 - val_loss: 0.6956 - val_acc: 0.4706\n",
            "Epoch 68/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6838 - acc: 0.6508 - val_loss: 0.6957 - val_acc: 0.4706\n",
            "Epoch 69/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6836 - acc: 0.6508 - val_loss: 0.6957 - val_acc: 0.4706\n",
            "Epoch 70/500\n",
            "315/315 [==============================] - 0s 31us/step - loss: 0.6835 - acc: 0.6508 - val_loss: 0.6958 - val_acc: 0.4706\n",
            "Epoch 71/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6834 - acc: 0.6508 - val_loss: 0.6958 - val_acc: 0.4706\n",
            "Epoch 72/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6833 - acc: 0.6508 - val_loss: 0.6958 - val_acc: 0.4706\n",
            "Epoch 73/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6831 - acc: 0.6508 - val_loss: 0.6959 - val_acc: 0.4706\n",
            "Epoch 74/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6830 - acc: 0.6508 - val_loss: 0.6959 - val_acc: 0.4706\n",
            "Epoch 75/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6829 - acc: 0.6508 - val_loss: 0.6960 - val_acc: 0.4706\n",
            "Epoch 76/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6827 - acc: 0.6508 - val_loss: 0.6960 - val_acc: 0.4706\n",
            "Epoch 77/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6826 - acc: 0.6508 - val_loss: 0.6961 - val_acc: 0.4706\n",
            "Epoch 78/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6825 - acc: 0.6508 - val_loss: 0.6961 - val_acc: 0.4706\n",
            "Epoch 79/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6824 - acc: 0.6508 - val_loss: 0.6962 - val_acc: 0.4706\n",
            "Epoch 80/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6822 - acc: 0.6508 - val_loss: 0.6962 - val_acc: 0.4706\n",
            "Epoch 81/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6821 - acc: 0.6508 - val_loss: 0.6963 - val_acc: 0.4706\n",
            "Epoch 82/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6820 - acc: 0.6508 - val_loss: 0.6963 - val_acc: 0.4706\n",
            "Epoch 83/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6819 - acc: 0.6508 - val_loss: 0.6964 - val_acc: 0.4706\n",
            "Epoch 84/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6817 - acc: 0.6508 - val_loss: 0.6964 - val_acc: 0.4706\n",
            "Epoch 85/500\n",
            "315/315 [==============================] - 0s 18us/step - loss: 0.6816 - acc: 0.6508 - val_loss: 0.6964 - val_acc: 0.4706\n",
            "Epoch 86/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6815 - acc: 0.6508 - val_loss: 0.6965 - val_acc: 0.4706\n",
            "Epoch 87/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6814 - acc: 0.6508 - val_loss: 0.6965 - val_acc: 0.4706\n",
            "Epoch 88/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6813 - acc: 0.6508 - val_loss: 0.6966 - val_acc: 0.4706\n",
            "Epoch 89/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6811 - acc: 0.6508 - val_loss: 0.6966 - val_acc: 0.4706\n",
            "Epoch 90/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6810 - acc: 0.6508 - val_loss: 0.6967 - val_acc: 0.4706\n",
            "Epoch 91/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6809 - acc: 0.6508 - val_loss: 0.6967 - val_acc: 0.4706\n",
            "Epoch 92/500\n",
            "315/315 [==============================] - 0s 16us/step - loss: 0.6808 - acc: 0.6508 - val_loss: 0.6968 - val_acc: 0.4706\n",
            "Epoch 93/500\n",
            "315/315 [==============================] - 0s 34us/step - loss: 0.6806 - acc: 0.6508 - val_loss: 0.6968 - val_acc: 0.4706\n",
            "Epoch 94/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6805 - acc: 0.6508 - val_loss: 0.6969 - val_acc: 0.4706\n",
            "Epoch 95/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6804 - acc: 0.6508 - val_loss: 0.6969 - val_acc: 0.4706\n",
            "Epoch 96/500\n",
            "315/315 [==============================] - 0s 35us/step - loss: 0.6803 - acc: 0.6508 - val_loss: 0.6970 - val_acc: 0.4706\n",
            "Epoch 97/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6802 - acc: 0.6508 - val_loss: 0.6970 - val_acc: 0.4706\n",
            "Epoch 98/500\n",
            "315/315 [==============================] - 0s 36us/step - loss: 0.6800 - acc: 0.6508 - val_loss: 0.6971 - val_acc: 0.4706\n",
            "Epoch 99/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6799 - acc: 0.6508 - val_loss: 0.6971 - val_acc: 0.4706\n",
            "Epoch 100/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6798 - acc: 0.6508 - val_loss: 0.6972 - val_acc: 0.4706\n",
            "Epoch 101/500\n",
            "315/315 [==============================] - 0s 18us/step - loss: 0.6797 - acc: 0.6508 - val_loss: 0.6972 - val_acc: 0.4706\n",
            "Epoch 102/500\n",
            "315/315 [==============================] - 0s 35us/step - loss: 0.6796 - acc: 0.6508 - val_loss: 0.6973 - val_acc: 0.4706\n",
            "Epoch 103/500\n",
            "315/315 [==============================] - 0s 36us/step - loss: 0.6795 - acc: 0.6508 - val_loss: 0.6973 - val_acc: 0.4706\n",
            "Epoch 104/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6793 - acc: 0.6508 - val_loss: 0.6974 - val_acc: 0.4706\n",
            "Epoch 105/500\n",
            "315/315 [==============================] - 0s 34us/step - loss: 0.6792 - acc: 0.6508 - val_loss: 0.6974 - val_acc: 0.4706\n",
            "Epoch 106/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6791 - acc: 0.6508 - val_loss: 0.6975 - val_acc: 0.4706\n",
            "Epoch 107/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6790 - acc: 0.6508 - val_loss: 0.6975 - val_acc: 0.4706\n",
            "Epoch 108/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6789 - acc: 0.6508 - val_loss: 0.6976 - val_acc: 0.4706\n",
            "Epoch 109/500\n",
            "315/315 [==============================] - 0s 32us/step - loss: 0.6788 - acc: 0.6508 - val_loss: 0.6976 - val_acc: 0.4706\n",
            "Epoch 110/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6786 - acc: 0.6508 - val_loss: 0.6977 - val_acc: 0.4706\n",
            "Epoch 111/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6785 - acc: 0.6508 - val_loss: 0.6977 - val_acc: 0.4706\n",
            "Epoch 112/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6784 - acc: 0.6508 - val_loss: 0.6978 - val_acc: 0.4706\n",
            "Epoch 113/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6783 - acc: 0.6508 - val_loss: 0.6978 - val_acc: 0.4706\n",
            "Epoch 114/500\n",
            "315/315 [==============================] - 0s 37us/step - loss: 0.6782 - acc: 0.6508 - val_loss: 0.6979 - val_acc: 0.4706\n",
            "Epoch 115/500\n",
            "315/315 [==============================] - 0s 37us/step - loss: 0.6781 - acc: 0.6508 - val_loss: 0.6979 - val_acc: 0.4706\n",
            "Epoch 116/500\n",
            "315/315 [==============================] - 0s 32us/step - loss: 0.6780 - acc: 0.6508 - val_loss: 0.6980 - val_acc: 0.4706\n",
            "Epoch 117/500\n",
            "315/315 [==============================] - 0s 39us/step - loss: 0.6778 - acc: 0.6508 - val_loss: 0.6981 - val_acc: 0.4706\n",
            "Epoch 118/500\n",
            "315/315 [==============================] - 0s 31us/step - loss: 0.6777 - acc: 0.6508 - val_loss: 0.6981 - val_acc: 0.4706\n",
            "Epoch 119/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6776 - acc: 0.6508 - val_loss: 0.6982 - val_acc: 0.4706\n",
            "Epoch 120/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6775 - acc: 0.6508 - val_loss: 0.6982 - val_acc: 0.4706\n",
            "Epoch 121/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6774 - acc: 0.6508 - val_loss: 0.6983 - val_acc: 0.4706\n",
            "Epoch 122/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6773 - acc: 0.6508 - val_loss: 0.6983 - val_acc: 0.4706\n",
            "Epoch 123/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6772 - acc: 0.6508 - val_loss: 0.6984 - val_acc: 0.4706\n",
            "Epoch 124/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6771 - acc: 0.6508 - val_loss: 0.6984 - val_acc: 0.4706\n",
            "Epoch 125/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6769 - acc: 0.6508 - val_loss: 0.6985 - val_acc: 0.4706\n",
            "Epoch 126/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6768 - acc: 0.6508 - val_loss: 0.6985 - val_acc: 0.4706\n",
            "Epoch 127/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6767 - acc: 0.6508 - val_loss: 0.6986 - val_acc: 0.4706\n",
            "Epoch 128/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6766 - acc: 0.6508 - val_loss: 0.6986 - val_acc: 0.4706\n",
            "Epoch 129/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6765 - acc: 0.6508 - val_loss: 0.6987 - val_acc: 0.4706\n",
            "Epoch 130/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6764 - acc: 0.6508 - val_loss: 0.6988 - val_acc: 0.4706\n",
            "Epoch 131/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6763 - acc: 0.6508 - val_loss: 0.6988 - val_acc: 0.4706\n",
            "Epoch 132/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6762 - acc: 0.6508 - val_loss: 0.6989 - val_acc: 0.4706\n",
            "Epoch 133/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6761 - acc: 0.6508 - val_loss: 0.6989 - val_acc: 0.4706\n",
            "Epoch 134/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6760 - acc: 0.6508 - val_loss: 0.6990 - val_acc: 0.4706\n",
            "Epoch 135/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6759 - acc: 0.6508 - val_loss: 0.6990 - val_acc: 0.4706\n",
            "Epoch 136/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6758 - acc: 0.6508 - val_loss: 0.6991 - val_acc: 0.4706\n",
            "Epoch 137/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6756 - acc: 0.6508 - val_loss: 0.6991 - val_acc: 0.4706\n",
            "Epoch 138/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6755 - acc: 0.6508 - val_loss: 0.6992 - val_acc: 0.4706\n",
            "Epoch 139/500\n",
            "315/315 [==============================] - 0s 36us/step - loss: 0.6754 - acc: 0.6508 - val_loss: 0.6993 - val_acc: 0.4706\n",
            "Epoch 140/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6753 - acc: 0.6508 - val_loss: 0.6993 - val_acc: 0.4706\n",
            "Epoch 141/500\n",
            "315/315 [==============================] - 0s 31us/step - loss: 0.6752 - acc: 0.6508 - val_loss: 0.6994 - val_acc: 0.4706\n",
            "Epoch 142/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6751 - acc: 0.6508 - val_loss: 0.6994 - val_acc: 0.4706\n",
            "Epoch 143/500\n",
            "315/315 [==============================] - 0s 18us/step - loss: 0.6750 - acc: 0.6508 - val_loss: 0.6995 - val_acc: 0.4706\n",
            "Epoch 144/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6749 - acc: 0.6508 - val_loss: 0.6995 - val_acc: 0.4706\n",
            "Epoch 145/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6748 - acc: 0.6508 - val_loss: 0.6996 - val_acc: 0.4706\n",
            "Epoch 146/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6747 - acc: 0.6508 - val_loss: 0.6996 - val_acc: 0.4706\n",
            "Epoch 147/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6746 - acc: 0.6508 - val_loss: 0.6997 - val_acc: 0.4706\n",
            "Epoch 148/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6745 - acc: 0.6508 - val_loss: 0.6998 - val_acc: 0.4706\n",
            "Epoch 149/500\n",
            "315/315 [==============================] - 0s 31us/step - loss: 0.6744 - acc: 0.6508 - val_loss: 0.6998 - val_acc: 0.4706\n",
            "Epoch 150/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6743 - acc: 0.6508 - val_loss: 0.6999 - val_acc: 0.4706\n",
            "Epoch 151/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6742 - acc: 0.6508 - val_loss: 0.6999 - val_acc: 0.4706\n",
            "Epoch 152/500\n",
            "315/315 [==============================] - 0s 38us/step - loss: 0.6741 - acc: 0.6508 - val_loss: 0.7000 - val_acc: 0.4706\n",
            "Epoch 153/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6740 - acc: 0.6508 - val_loss: 0.7001 - val_acc: 0.4706\n",
            "Epoch 154/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6739 - acc: 0.6508 - val_loss: 0.7001 - val_acc: 0.4706\n",
            "Epoch 155/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6738 - acc: 0.6508 - val_loss: 0.7002 - val_acc: 0.4706\n",
            "Epoch 156/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6737 - acc: 0.6508 - val_loss: 0.7002 - val_acc: 0.4706\n",
            "Epoch 157/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6736 - acc: 0.6508 - val_loss: 0.7003 - val_acc: 0.4706\n",
            "Epoch 158/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6735 - acc: 0.6508 - val_loss: 0.7003 - val_acc: 0.4706\n",
            "Epoch 159/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6734 - acc: 0.6508 - val_loss: 0.7004 - val_acc: 0.4706\n",
            "Epoch 160/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6733 - acc: 0.6508 - val_loss: 0.7005 - val_acc: 0.4706\n",
            "Epoch 161/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6732 - acc: 0.6508 - val_loss: 0.7005 - val_acc: 0.4706\n",
            "Epoch 162/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6731 - acc: 0.6508 - val_loss: 0.7006 - val_acc: 0.4706\n",
            "Epoch 163/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6730 - acc: 0.6508 - val_loss: 0.7006 - val_acc: 0.4706\n",
            "Epoch 164/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6729 - acc: 0.6508 - val_loss: 0.7007 - val_acc: 0.4706\n",
            "Epoch 165/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6728 - acc: 0.6508 - val_loss: 0.7008 - val_acc: 0.4706\n",
            "Epoch 166/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6727 - acc: 0.6508 - val_loss: 0.7008 - val_acc: 0.4706\n",
            "Epoch 167/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6726 - acc: 0.6508 - val_loss: 0.7009 - val_acc: 0.4706\n",
            "Epoch 168/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6725 - acc: 0.6508 - val_loss: 0.7009 - val_acc: 0.4706\n",
            "Epoch 169/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6724 - acc: 0.6508 - val_loss: 0.7010 - val_acc: 0.4706\n",
            "Epoch 170/500\n",
            "315/315 [==============================] - 0s 34us/step - loss: 0.6723 - acc: 0.6508 - val_loss: 0.7011 - val_acc: 0.4706\n",
            "Epoch 171/500\n",
            "315/315 [==============================] - 0s 34us/step - loss: 0.6722 - acc: 0.6508 - val_loss: 0.7011 - val_acc: 0.4706\n",
            "Epoch 172/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6721 - acc: 0.6508 - val_loss: 0.7012 - val_acc: 0.4706\n",
            "Epoch 173/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6720 - acc: 0.6508 - val_loss: 0.7012 - val_acc: 0.4706\n",
            "Epoch 174/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6719 - acc: 0.6508 - val_loss: 0.7013 - val_acc: 0.4706\n",
            "Epoch 175/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6718 - acc: 0.6508 - val_loss: 0.7014 - val_acc: 0.4706\n",
            "Epoch 176/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6717 - acc: 0.6508 - val_loss: 0.7014 - val_acc: 0.4706\n",
            "Epoch 177/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6716 - acc: 0.6508 - val_loss: 0.7015 - val_acc: 0.4706\n",
            "Epoch 178/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6715 - acc: 0.6508 - val_loss: 0.7015 - val_acc: 0.4706\n",
            "Epoch 179/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6714 - acc: 0.6508 - val_loss: 0.7016 - val_acc: 0.4706\n",
            "Epoch 180/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6713 - acc: 0.6508 - val_loss: 0.7017 - val_acc: 0.4706\n",
            "Epoch 181/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6713 - acc: 0.6508 - val_loss: 0.7017 - val_acc: 0.4706\n",
            "Epoch 182/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6712 - acc: 0.6508 - val_loss: 0.7018 - val_acc: 0.4706\n",
            "Epoch 183/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6711 - acc: 0.6508 - val_loss: 0.7019 - val_acc: 0.4706\n",
            "Epoch 184/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6710 - acc: 0.6508 - val_loss: 0.7019 - val_acc: 0.4706\n",
            "Epoch 185/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6709 - acc: 0.6508 - val_loss: 0.7020 - val_acc: 0.4706\n",
            "Epoch 186/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6708 - acc: 0.6508 - val_loss: 0.7020 - val_acc: 0.4706\n",
            "Epoch 187/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6707 - acc: 0.6508 - val_loss: 0.7021 - val_acc: 0.4706\n",
            "Epoch 188/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6706 - acc: 0.6508 - val_loss: 0.7022 - val_acc: 0.4706\n",
            "Epoch 189/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6705 - acc: 0.6508 - val_loss: 0.7022 - val_acc: 0.4706\n",
            "Epoch 190/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6704 - acc: 0.6508 - val_loss: 0.7023 - val_acc: 0.4706\n",
            "Epoch 191/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6703 - acc: 0.6508 - val_loss: 0.7023 - val_acc: 0.4706\n",
            "Epoch 192/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6702 - acc: 0.6508 - val_loss: 0.7024 - val_acc: 0.4706\n",
            "Epoch 193/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6702 - acc: 0.6508 - val_loss: 0.7025 - val_acc: 0.4706\n",
            "Epoch 194/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6701 - acc: 0.6508 - val_loss: 0.7025 - val_acc: 0.4706\n",
            "Epoch 195/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6700 - acc: 0.6508 - val_loss: 0.7026 - val_acc: 0.4706\n",
            "Epoch 196/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6699 - acc: 0.6508 - val_loss: 0.7027 - val_acc: 0.4706\n",
            "Epoch 197/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6698 - acc: 0.6508 - val_loss: 0.7027 - val_acc: 0.4706\n",
            "Epoch 198/500\n",
            "315/315 [==============================] - 0s 18us/step - loss: 0.6697 - acc: 0.6508 - val_loss: 0.7028 - val_acc: 0.4706\n",
            "Epoch 199/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6696 - acc: 0.6508 - val_loss: 0.7029 - val_acc: 0.4706\n",
            "Epoch 200/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6695 - acc: 0.6508 - val_loss: 0.7029 - val_acc: 0.4706\n",
            "Epoch 201/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6694 - acc: 0.6508 - val_loss: 0.7030 - val_acc: 0.4706\n",
            "Epoch 202/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6694 - acc: 0.6508 - val_loss: 0.7030 - val_acc: 0.4706\n",
            "Epoch 203/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6693 - acc: 0.6508 - val_loss: 0.7031 - val_acc: 0.4706\n",
            "Epoch 204/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6692 - acc: 0.6508 - val_loss: 0.7032 - val_acc: 0.4706\n",
            "Epoch 205/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6691 - acc: 0.6508 - val_loss: 0.7032 - val_acc: 0.4706\n",
            "Epoch 206/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6690 - acc: 0.6508 - val_loss: 0.7033 - val_acc: 0.4706\n",
            "Epoch 207/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6689 - acc: 0.6508 - val_loss: 0.7034 - val_acc: 0.4706\n",
            "Epoch 208/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6688 - acc: 0.6508 - val_loss: 0.7034 - val_acc: 0.4706\n",
            "Epoch 209/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6687 - acc: 0.6508 - val_loss: 0.7035 - val_acc: 0.4706\n",
            "Epoch 210/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6687 - acc: 0.6508 - val_loss: 0.7036 - val_acc: 0.4706\n",
            "Epoch 211/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6686 - acc: 0.6508 - val_loss: 0.7036 - val_acc: 0.4706\n",
            "Epoch 212/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6685 - acc: 0.6508 - val_loss: 0.7037 - val_acc: 0.4706\n",
            "Epoch 213/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6684 - acc: 0.6508 - val_loss: 0.7038 - val_acc: 0.4706\n",
            "Epoch 214/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6683 - acc: 0.6508 - val_loss: 0.7038 - val_acc: 0.4706\n",
            "Epoch 215/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6682 - acc: 0.6508 - val_loss: 0.7039 - val_acc: 0.4706\n",
            "Epoch 216/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6682 - acc: 0.6508 - val_loss: 0.7039 - val_acc: 0.4706\n",
            "Epoch 217/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6681 - acc: 0.6508 - val_loss: 0.7040 - val_acc: 0.4706\n",
            "Epoch 218/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6680 - acc: 0.6508 - val_loss: 0.7041 - val_acc: 0.4706\n",
            "Epoch 219/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6679 - acc: 0.6508 - val_loss: 0.7041 - val_acc: 0.4706\n",
            "Epoch 220/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6678 - acc: 0.6508 - val_loss: 0.7042 - val_acc: 0.4706\n",
            "Epoch 221/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6677 - acc: 0.6508 - val_loss: 0.7043 - val_acc: 0.4706\n",
            "Epoch 222/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6677 - acc: 0.6508 - val_loss: 0.7043 - val_acc: 0.4706\n",
            "Epoch 223/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6676 - acc: 0.6508 - val_loss: 0.7044 - val_acc: 0.4706\n",
            "Epoch 224/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6675 - acc: 0.6508 - val_loss: 0.7045 - val_acc: 0.4706\n",
            "Epoch 225/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6674 - acc: 0.6508 - val_loss: 0.7045 - val_acc: 0.4706\n",
            "Epoch 226/500\n",
            "315/315 [==============================] - 0s 45us/step - loss: 0.6673 - acc: 0.6508 - val_loss: 0.7046 - val_acc: 0.4706\n",
            "Epoch 227/500\n",
            "315/315 [==============================] - 0s 39us/step - loss: 0.6672 - acc: 0.6508 - val_loss: 0.7047 - val_acc: 0.4706\n",
            "Epoch 228/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6672 - acc: 0.6508 - val_loss: 0.7047 - val_acc: 0.4706\n",
            "Epoch 229/500\n",
            "315/315 [==============================] - 0s 31us/step - loss: 0.6671 - acc: 0.6508 - val_loss: 0.7048 - val_acc: 0.4706\n",
            "Epoch 230/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6670 - acc: 0.6508 - val_loss: 0.7049 - val_acc: 0.4706\n",
            "Epoch 231/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6669 - acc: 0.6508 - val_loss: 0.7049 - val_acc: 0.4706\n",
            "Epoch 232/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6668 - acc: 0.6508 - val_loss: 0.7050 - val_acc: 0.4706\n",
            "Epoch 233/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6668 - acc: 0.6508 - val_loss: 0.7051 - val_acc: 0.4706\n",
            "Epoch 234/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6667 - acc: 0.6508 - val_loss: 0.7051 - val_acc: 0.4706\n",
            "Epoch 235/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6666 - acc: 0.6508 - val_loss: 0.7052 - val_acc: 0.4706\n",
            "Epoch 236/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6665 - acc: 0.6508 - val_loss: 0.7053 - val_acc: 0.4706\n",
            "Epoch 237/500\n",
            "315/315 [==============================] - 0s 18us/step - loss: 0.6665 - acc: 0.6508 - val_loss: 0.7053 - val_acc: 0.4706\n",
            "Epoch 238/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6664 - acc: 0.6508 - val_loss: 0.7054 - val_acc: 0.4706\n",
            "Epoch 239/500\n",
            "315/315 [==============================] - 0s 37us/step - loss: 0.6663 - acc: 0.6508 - val_loss: 0.7055 - val_acc: 0.4706\n",
            "Epoch 240/500\n",
            "315/315 [==============================] - 0s 36us/step - loss: 0.6662 - acc: 0.6508 - val_loss: 0.7055 - val_acc: 0.4706\n",
            "Epoch 241/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6661 - acc: 0.6508 - val_loss: 0.7056 - val_acc: 0.4706\n",
            "Epoch 242/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6661 - acc: 0.6508 - val_loss: 0.7057 - val_acc: 0.4706\n",
            "Epoch 243/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6660 - acc: 0.6508 - val_loss: 0.7057 - val_acc: 0.4706\n",
            "Epoch 244/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6659 - acc: 0.6508 - val_loss: 0.7058 - val_acc: 0.4706\n",
            "Epoch 245/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6658 - acc: 0.6508 - val_loss: 0.7059 - val_acc: 0.4706\n",
            "Epoch 246/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6658 - acc: 0.6508 - val_loss: 0.7059 - val_acc: 0.4706\n",
            "Epoch 247/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6657 - acc: 0.6508 - val_loss: 0.7060 - val_acc: 0.4706\n",
            "Epoch 248/500\n",
            "315/315 [==============================] - 0s 42us/step - loss: 0.6656 - acc: 0.6508 - val_loss: 0.7061 - val_acc: 0.4706\n",
            "Epoch 249/500\n",
            "315/315 [==============================] - 0s 37us/step - loss: 0.6655 - acc: 0.6508 - val_loss: 0.7062 - val_acc: 0.4706\n",
            "Epoch 250/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6655 - acc: 0.6508 - val_loss: 0.7062 - val_acc: 0.4706\n",
            "Epoch 251/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6654 - acc: 0.6508 - val_loss: 0.7063 - val_acc: 0.4706\n",
            "Epoch 252/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6653 - acc: 0.6508 - val_loss: 0.7064 - val_acc: 0.4706\n",
            "Epoch 253/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6652 - acc: 0.6508 - val_loss: 0.7064 - val_acc: 0.4706\n",
            "Epoch 254/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6652 - acc: 0.6508 - val_loss: 0.7065 - val_acc: 0.4706\n",
            "Epoch 255/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6651 - acc: 0.6508 - val_loss: 0.7066 - val_acc: 0.4706\n",
            "Epoch 256/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6650 - acc: 0.6508 - val_loss: 0.7066 - val_acc: 0.4706\n",
            "Epoch 257/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6649 - acc: 0.6508 - val_loss: 0.7067 - val_acc: 0.4706\n",
            "Epoch 258/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6649 - acc: 0.6508 - val_loss: 0.7068 - val_acc: 0.4706\n",
            "Epoch 259/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6648 - acc: 0.6508 - val_loss: 0.7068 - val_acc: 0.4706\n",
            "Epoch 260/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6647 - acc: 0.6508 - val_loss: 0.7069 - val_acc: 0.4706\n",
            "Epoch 261/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6646 - acc: 0.6508 - val_loss: 0.7070 - val_acc: 0.4706\n",
            "Epoch 262/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6646 - acc: 0.6508 - val_loss: 0.7070 - val_acc: 0.4706\n",
            "Epoch 263/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6645 - acc: 0.6508 - val_loss: 0.7071 - val_acc: 0.4706\n",
            "Epoch 264/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6644 - acc: 0.6508 - val_loss: 0.7072 - val_acc: 0.4706\n",
            "Epoch 265/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6644 - acc: 0.6508 - val_loss: 0.7073 - val_acc: 0.4706\n",
            "Epoch 266/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6643 - acc: 0.6508 - val_loss: 0.7073 - val_acc: 0.4706\n",
            "Epoch 267/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6642 - acc: 0.6508 - val_loss: 0.7074 - val_acc: 0.4706\n",
            "Epoch 268/500\n",
            "315/315 [==============================] - 0s 32us/step - loss: 0.6641 - acc: 0.6508 - val_loss: 0.7075 - val_acc: 0.4706\n",
            "Epoch 269/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6641 - acc: 0.6508 - val_loss: 0.7075 - val_acc: 0.4706\n",
            "Epoch 270/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6640 - acc: 0.6508 - val_loss: 0.7076 - val_acc: 0.4706\n",
            "Epoch 271/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6639 - acc: 0.6508 - val_loss: 0.7077 - val_acc: 0.4706\n",
            "Epoch 272/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6639 - acc: 0.6508 - val_loss: 0.7077 - val_acc: 0.4706\n",
            "Epoch 273/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6638 - acc: 0.6508 - val_loss: 0.7078 - val_acc: 0.4706\n",
            "Epoch 274/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6637 - acc: 0.6508 - val_loss: 0.7079 - val_acc: 0.4706\n",
            "Epoch 275/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6636 - acc: 0.6508 - val_loss: 0.7079 - val_acc: 0.4706\n",
            "Epoch 276/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6636 - acc: 0.6508 - val_loss: 0.7080 - val_acc: 0.4706\n",
            "Epoch 277/500\n",
            "315/315 [==============================] - 0s 31us/step - loss: 0.6635 - acc: 0.6508 - val_loss: 0.7081 - val_acc: 0.4706\n",
            "Epoch 278/500\n",
            "315/315 [==============================] - 0s 31us/step - loss: 0.6634 - acc: 0.6508 - val_loss: 0.7082 - val_acc: 0.4706\n",
            "Epoch 279/500\n",
            "315/315 [==============================] - 0s 31us/step - loss: 0.6634 - acc: 0.6508 - val_loss: 0.7082 - val_acc: 0.4706\n",
            "Epoch 280/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6633 - acc: 0.6508 - val_loss: 0.7083 - val_acc: 0.4706\n",
            "Epoch 281/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6632 - acc: 0.6508 - val_loss: 0.7084 - val_acc: 0.4706\n",
            "Epoch 282/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6632 - acc: 0.6508 - val_loss: 0.7084 - val_acc: 0.4706\n",
            "Epoch 283/500\n",
            "315/315 [==============================] - 0s 31us/step - loss: 0.6631 - acc: 0.6508 - val_loss: 0.7085 - val_acc: 0.4706\n",
            "Epoch 284/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6630 - acc: 0.6508 - val_loss: 0.7086 - val_acc: 0.4706\n",
            "Epoch 285/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6630 - acc: 0.6508 - val_loss: 0.7086 - val_acc: 0.4706\n",
            "Epoch 286/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6629 - acc: 0.6508 - val_loss: 0.7087 - val_acc: 0.4706\n",
            "Epoch 287/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6628 - acc: 0.6508 - val_loss: 0.7088 - val_acc: 0.4706\n",
            "Epoch 288/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6628 - acc: 0.6508 - val_loss: 0.7089 - val_acc: 0.4706\n",
            "Epoch 289/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6627 - acc: 0.6508 - val_loss: 0.7089 - val_acc: 0.4706\n",
            "Epoch 290/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6626 - acc: 0.6508 - val_loss: 0.7090 - val_acc: 0.4706\n",
            "Epoch 291/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6626 - acc: 0.6508 - val_loss: 0.7091 - val_acc: 0.4706\n",
            "Epoch 292/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6625 - acc: 0.6508 - val_loss: 0.7091 - val_acc: 0.4706\n",
            "Epoch 293/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6624 - acc: 0.6508 - val_loss: 0.7092 - val_acc: 0.4706\n",
            "Epoch 294/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6624 - acc: 0.6508 - val_loss: 0.7093 - val_acc: 0.4706\n",
            "Epoch 295/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6623 - acc: 0.6508 - val_loss: 0.7094 - val_acc: 0.4706\n",
            "Epoch 296/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6622 - acc: 0.6508 - val_loss: 0.7094 - val_acc: 0.4706\n",
            "Epoch 297/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6622 - acc: 0.6508 - val_loss: 0.7095 - val_acc: 0.4706\n",
            "Epoch 298/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6621 - acc: 0.6508 - val_loss: 0.7096 - val_acc: 0.4706\n",
            "Epoch 299/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6621 - acc: 0.6508 - val_loss: 0.7096 - val_acc: 0.4706\n",
            "Epoch 300/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6620 - acc: 0.6508 - val_loss: 0.7097 - val_acc: 0.4706\n",
            "Epoch 301/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6619 - acc: 0.6508 - val_loss: 0.7098 - val_acc: 0.4706\n",
            "Epoch 302/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6619 - acc: 0.6508 - val_loss: 0.7099 - val_acc: 0.4706\n",
            "Epoch 303/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6618 - acc: 0.6508 - val_loss: 0.7099 - val_acc: 0.4706\n",
            "Epoch 304/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6617 - acc: 0.6508 - val_loss: 0.7100 - val_acc: 0.4706\n",
            "Epoch 305/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6617 - acc: 0.6508 - val_loss: 0.7101 - val_acc: 0.4706\n",
            "Epoch 306/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6616 - acc: 0.6508 - val_loss: 0.7101 - val_acc: 0.4706\n",
            "Epoch 307/500\n",
            "315/315 [==============================] - 0s 16us/step - loss: 0.6615 - acc: 0.6508 - val_loss: 0.7102 - val_acc: 0.4706\n",
            "Epoch 308/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6615 - acc: 0.6508 - val_loss: 0.7103 - val_acc: 0.4706\n",
            "Epoch 309/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6614 - acc: 0.6508 - val_loss: 0.7104 - val_acc: 0.4706\n",
            "Epoch 310/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6614 - acc: 0.6508 - val_loss: 0.7104 - val_acc: 0.4706\n",
            "Epoch 311/500\n",
            "315/315 [==============================] - 0s 18us/step - loss: 0.6613 - acc: 0.6508 - val_loss: 0.7105 - val_acc: 0.4706\n",
            "Epoch 312/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6612 - acc: 0.6508 - val_loss: 0.7106 - val_acc: 0.4706\n",
            "Epoch 313/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6612 - acc: 0.6508 - val_loss: 0.7106 - val_acc: 0.4706\n",
            "Epoch 314/500\n",
            "315/315 [==============================] - 0s 18us/step - loss: 0.6611 - acc: 0.6508 - val_loss: 0.7107 - val_acc: 0.4706\n",
            "Epoch 315/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6611 - acc: 0.6508 - val_loss: 0.7108 - val_acc: 0.4706\n",
            "Epoch 316/500\n",
            "315/315 [==============================] - 0s 36us/step - loss: 0.6610 - acc: 0.6508 - val_loss: 0.7109 - val_acc: 0.4706\n",
            "Epoch 317/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6609 - acc: 0.6508 - val_loss: 0.7109 - val_acc: 0.4706\n",
            "Epoch 318/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6609 - acc: 0.6508 - val_loss: 0.7110 - val_acc: 0.4706\n",
            "Epoch 319/500\n",
            "315/315 [==============================] - 0s 16us/step - loss: 0.6608 - acc: 0.6508 - val_loss: 0.7111 - val_acc: 0.4706\n",
            "Epoch 320/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6608 - acc: 0.6508 - val_loss: 0.7111 - val_acc: 0.4706\n",
            "Epoch 321/500\n",
            "315/315 [==============================] - 0s 31us/step - loss: 0.6607 - acc: 0.6508 - val_loss: 0.7112 - val_acc: 0.4706\n",
            "Epoch 322/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6606 - acc: 0.6508 - val_loss: 0.7113 - val_acc: 0.4706\n",
            "Epoch 323/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6606 - acc: 0.6508 - val_loss: 0.7114 - val_acc: 0.4706\n",
            "Epoch 324/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6605 - acc: 0.6508 - val_loss: 0.7114 - val_acc: 0.4706\n",
            "Epoch 325/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6605 - acc: 0.6508 - val_loss: 0.7115 - val_acc: 0.4706\n",
            "Epoch 326/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6604 - acc: 0.6508 - val_loss: 0.7116 - val_acc: 0.4706\n",
            "Epoch 327/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6603 - acc: 0.6508 - val_loss: 0.7116 - val_acc: 0.4706\n",
            "Epoch 328/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6603 - acc: 0.6508 - val_loss: 0.7117 - val_acc: 0.4706\n",
            "Epoch 329/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6602 - acc: 0.6508 - val_loss: 0.7118 - val_acc: 0.4706\n",
            "Epoch 330/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6602 - acc: 0.6508 - val_loss: 0.7119 - val_acc: 0.4706\n",
            "Epoch 331/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6601 - acc: 0.6508 - val_loss: 0.7119 - val_acc: 0.4706\n",
            "Epoch 332/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6601 - acc: 0.6508 - val_loss: 0.7120 - val_acc: 0.4706\n",
            "Epoch 333/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6600 - acc: 0.6508 - val_loss: 0.7121 - val_acc: 0.4706\n",
            "Epoch 334/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6599 - acc: 0.6508 - val_loss: 0.7122 - val_acc: 0.4706\n",
            "Epoch 335/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6599 - acc: 0.6508 - val_loss: 0.7122 - val_acc: 0.4706\n",
            "Epoch 336/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6598 - acc: 0.6508 - val_loss: 0.7123 - val_acc: 0.4706\n",
            "Epoch 337/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6598 - acc: 0.6508 - val_loss: 0.7124 - val_acc: 0.4706\n",
            "Epoch 338/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6597 - acc: 0.6508 - val_loss: 0.7124 - val_acc: 0.4706\n",
            "Epoch 339/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6597 - acc: 0.6508 - val_loss: 0.7125 - val_acc: 0.4706\n",
            "Epoch 340/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6596 - acc: 0.6508 - val_loss: 0.7126 - val_acc: 0.4706\n",
            "Epoch 341/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6596 - acc: 0.6508 - val_loss: 0.7127 - val_acc: 0.4706\n",
            "Epoch 342/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6595 - acc: 0.6508 - val_loss: 0.7127 - val_acc: 0.4706\n",
            "Epoch 343/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6594 - acc: 0.6508 - val_loss: 0.7128 - val_acc: 0.4706\n",
            "Epoch 344/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6594 - acc: 0.6508 - val_loss: 0.7129 - val_acc: 0.4706\n",
            "Epoch 345/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6593 - acc: 0.6508 - val_loss: 0.7130 - val_acc: 0.4706\n",
            "Epoch 346/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6593 - acc: 0.6508 - val_loss: 0.7130 - val_acc: 0.4706\n",
            "Epoch 347/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6592 - acc: 0.6508 - val_loss: 0.7131 - val_acc: 0.4706\n",
            "Epoch 348/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6592 - acc: 0.6508 - val_loss: 0.7132 - val_acc: 0.4706\n",
            "Epoch 349/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6591 - acc: 0.6508 - val_loss: 0.7132 - val_acc: 0.4706\n",
            "Epoch 350/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6591 - acc: 0.6508 - val_loss: 0.7133 - val_acc: 0.4706\n",
            "Epoch 351/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6590 - acc: 0.6508 - val_loss: 0.7134 - val_acc: 0.4706\n",
            "Epoch 352/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6590 - acc: 0.6508 - val_loss: 0.7135 - val_acc: 0.4706\n",
            "Epoch 353/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6589 - acc: 0.6508 - val_loss: 0.7135 - val_acc: 0.4706\n",
            "Epoch 354/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6589 - acc: 0.6508 - val_loss: 0.7136 - val_acc: 0.4706\n",
            "Epoch 355/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6588 - acc: 0.6508 - val_loss: 0.7137 - val_acc: 0.4706\n",
            "Epoch 356/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6587 - acc: 0.6508 - val_loss: 0.7138 - val_acc: 0.4706\n",
            "Epoch 357/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6587 - acc: 0.6508 - val_loss: 0.7138 - val_acc: 0.4706\n",
            "Epoch 358/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6586 - acc: 0.6508 - val_loss: 0.7139 - val_acc: 0.4706\n",
            "Epoch 359/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6586 - acc: 0.6508 - val_loss: 0.7140 - val_acc: 0.4706\n",
            "Epoch 360/500\n",
            "315/315 [==============================] - 0s 44us/step - loss: 0.6585 - acc: 0.6508 - val_loss: 0.7141 - val_acc: 0.4706\n",
            "Epoch 361/500\n",
            "315/315 [==============================] - 0s 47us/step - loss: 0.6585 - acc: 0.6508 - val_loss: 0.7141 - val_acc: 0.4706\n",
            "Epoch 362/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6584 - acc: 0.6508 - val_loss: 0.7142 - val_acc: 0.4706\n",
            "Epoch 363/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6584 - acc: 0.6508 - val_loss: 0.7143 - val_acc: 0.4706\n",
            "Epoch 364/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6583 - acc: 0.6508 - val_loss: 0.7143 - val_acc: 0.4706\n",
            "Epoch 365/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6583 - acc: 0.6508 - val_loss: 0.7144 - val_acc: 0.4706\n",
            "Epoch 366/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6582 - acc: 0.6508 - val_loss: 0.7145 - val_acc: 0.4706\n",
            "Epoch 367/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6582 - acc: 0.6508 - val_loss: 0.7146 - val_acc: 0.4706\n",
            "Epoch 368/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6581 - acc: 0.6508 - val_loss: 0.7146 - val_acc: 0.4706\n",
            "Epoch 369/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6581 - acc: 0.6508 - val_loss: 0.7147 - val_acc: 0.4706\n",
            "Epoch 370/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6580 - acc: 0.6508 - val_loss: 0.7148 - val_acc: 0.4706\n",
            "Epoch 371/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6580 - acc: 0.6508 - val_loss: 0.7149 - val_acc: 0.4706\n",
            "Epoch 372/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6579 - acc: 0.6508 - val_loss: 0.7149 - val_acc: 0.4706\n",
            "Epoch 373/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6579 - acc: 0.6508 - val_loss: 0.7150 - val_acc: 0.4706\n",
            "Epoch 374/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6578 - acc: 0.6508 - val_loss: 0.7151 - val_acc: 0.4706\n",
            "Epoch 375/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6578 - acc: 0.6508 - val_loss: 0.7152 - val_acc: 0.4706\n",
            "Epoch 376/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6577 - acc: 0.6508 - val_loss: 0.7152 - val_acc: 0.4706\n",
            "Epoch 377/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6577 - acc: 0.6508 - val_loss: 0.7153 - val_acc: 0.4706\n",
            "Epoch 378/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6576 - acc: 0.6508 - val_loss: 0.7154 - val_acc: 0.4706\n",
            "Epoch 379/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6576 - acc: 0.6508 - val_loss: 0.7154 - val_acc: 0.4706\n",
            "Epoch 380/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6575 - acc: 0.6508 - val_loss: 0.7155 - val_acc: 0.4706\n",
            "Epoch 381/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6575 - acc: 0.6508 - val_loss: 0.7156 - val_acc: 0.4706\n",
            "Epoch 382/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6575 - acc: 0.6508 - val_loss: 0.7157 - val_acc: 0.4706\n",
            "Epoch 383/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6574 - acc: 0.6508 - val_loss: 0.7157 - val_acc: 0.4706\n",
            "Epoch 384/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6574 - acc: 0.6508 - val_loss: 0.7158 - val_acc: 0.4706\n",
            "Epoch 385/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6573 - acc: 0.6508 - val_loss: 0.7159 - val_acc: 0.4706\n",
            "Epoch 386/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6573 - acc: 0.6508 - val_loss: 0.7160 - val_acc: 0.4706\n",
            "Epoch 387/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6572 - acc: 0.6508 - val_loss: 0.7160 - val_acc: 0.4706\n",
            "Epoch 388/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6572 - acc: 0.6508 - val_loss: 0.7161 - val_acc: 0.4706\n",
            "Epoch 389/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6571 - acc: 0.6508 - val_loss: 0.7162 - val_acc: 0.4706\n",
            "Epoch 390/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6571 - acc: 0.6508 - val_loss: 0.7163 - val_acc: 0.4706\n",
            "Epoch 391/500\n",
            "315/315 [==============================] - 0s 31us/step - loss: 0.6570 - acc: 0.6508 - val_loss: 0.7163 - val_acc: 0.4706\n",
            "Epoch 392/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6570 - acc: 0.6508 - val_loss: 0.7164 - val_acc: 0.4706\n",
            "Epoch 393/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6569 - acc: 0.6508 - val_loss: 0.7165 - val_acc: 0.4706\n",
            "Epoch 394/500\n",
            "315/315 [==============================] - 0s 42us/step - loss: 0.6569 - acc: 0.6508 - val_loss: 0.7165 - val_acc: 0.4706\n",
            "Epoch 395/500\n",
            "315/315 [==============================] - 0s 32us/step - loss: 0.6568 - acc: 0.6508 - val_loss: 0.7166 - val_acc: 0.4706\n",
            "Epoch 396/500\n",
            "315/315 [==============================] - 0s 32us/step - loss: 0.6568 - acc: 0.6508 - val_loss: 0.7167 - val_acc: 0.4706\n",
            "Epoch 397/500\n",
            "315/315 [==============================] - 0s 31us/step - loss: 0.6568 - acc: 0.6508 - val_loss: 0.7168 - val_acc: 0.4706\n",
            "Epoch 398/500\n",
            "315/315 [==============================] - 0s 31us/step - loss: 0.6567 - acc: 0.6508 - val_loss: 0.7168 - val_acc: 0.4706\n",
            "Epoch 399/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6567 - acc: 0.6508 - val_loss: 0.7169 - val_acc: 0.4706\n",
            "Epoch 400/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6566 - acc: 0.6508 - val_loss: 0.7170 - val_acc: 0.4706\n",
            "Epoch 401/500\n",
            "315/315 [==============================] - 0s 36us/step - loss: 0.6566 - acc: 0.6508 - val_loss: 0.7171 - val_acc: 0.4706\n",
            "Epoch 402/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6565 - acc: 0.6508 - val_loss: 0.7171 - val_acc: 0.4706\n",
            "Epoch 403/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6565 - acc: 0.6508 - val_loss: 0.7172 - val_acc: 0.4706\n",
            "Epoch 404/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6564 - acc: 0.6508 - val_loss: 0.7173 - val_acc: 0.4706\n",
            "Epoch 405/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6564 - acc: 0.6508 - val_loss: 0.7174 - val_acc: 0.4706\n",
            "Epoch 406/500\n",
            "315/315 [==============================] - 0s 17us/step - loss: 0.6564 - acc: 0.6508 - val_loss: 0.7174 - val_acc: 0.4706\n",
            "Epoch 407/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6563 - acc: 0.6508 - val_loss: 0.7175 - val_acc: 0.4706\n",
            "Epoch 408/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6563 - acc: 0.6508 - val_loss: 0.7176 - val_acc: 0.4706\n",
            "Epoch 409/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6562 - acc: 0.6508 - val_loss: 0.7176 - val_acc: 0.4706\n",
            "Epoch 410/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6562 - acc: 0.6508 - val_loss: 0.7177 - val_acc: 0.4706\n",
            "Epoch 411/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6561 - acc: 0.6508 - val_loss: 0.7178 - val_acc: 0.4706\n",
            "Epoch 412/500\n",
            "315/315 [==============================] - 0s 18us/step - loss: 0.6561 - acc: 0.6508 - val_loss: 0.7179 - val_acc: 0.4706\n",
            "Epoch 413/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6561 - acc: 0.6508 - val_loss: 0.7179 - val_acc: 0.4706\n",
            "Epoch 414/500\n",
            "315/315 [==============================] - 0s 18us/step - loss: 0.6560 - acc: 0.6508 - val_loss: 0.7180 - val_acc: 0.4706\n",
            "Epoch 415/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6560 - acc: 0.6508 - val_loss: 0.7181 - val_acc: 0.4706\n",
            "Epoch 416/500\n",
            "315/315 [==============================] - 0s 16us/step - loss: 0.6559 - acc: 0.6508 - val_loss: 0.7182 - val_acc: 0.4706\n",
            "Epoch 417/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6559 - acc: 0.6508 - val_loss: 0.7182 - val_acc: 0.4706\n",
            "Epoch 418/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6559 - acc: 0.6508 - val_loss: 0.7183 - val_acc: 0.4706\n",
            "Epoch 419/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6558 - acc: 0.6508 - val_loss: 0.7184 - val_acc: 0.4706\n",
            "Epoch 420/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6558 - acc: 0.6508 - val_loss: 0.7185 - val_acc: 0.4706\n",
            "Epoch 421/500\n",
            "315/315 [==============================] - 0s 16us/step - loss: 0.6557 - acc: 0.6508 - val_loss: 0.7185 - val_acc: 0.4706\n",
            "Epoch 422/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6557 - acc: 0.6508 - val_loss: 0.7186 - val_acc: 0.4706\n",
            "Epoch 423/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6556 - acc: 0.6508 - val_loss: 0.7187 - val_acc: 0.4706\n",
            "Epoch 424/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6556 - acc: 0.6508 - val_loss: 0.7188 - val_acc: 0.4706\n",
            "Epoch 425/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6556 - acc: 0.6508 - val_loss: 0.7188 - val_acc: 0.4706\n",
            "Epoch 426/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6555 - acc: 0.6508 - val_loss: 0.7189 - val_acc: 0.4706\n",
            "Epoch 427/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6555 - acc: 0.6508 - val_loss: 0.7190 - val_acc: 0.4706\n",
            "Epoch 428/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6554 - acc: 0.6508 - val_loss: 0.7190 - val_acc: 0.4706\n",
            "Epoch 429/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6554 - acc: 0.6508 - val_loss: 0.7191 - val_acc: 0.4706\n",
            "Epoch 430/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6554 - acc: 0.6508 - val_loss: 0.7192 - val_acc: 0.4706\n",
            "Epoch 431/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6553 - acc: 0.6508 - val_loss: 0.7193 - val_acc: 0.4706\n",
            "Epoch 432/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6553 - acc: 0.6508 - val_loss: 0.7193 - val_acc: 0.4706\n",
            "Epoch 433/500\n",
            "315/315 [==============================] - 0s 18us/step - loss: 0.6552 - acc: 0.6508 - val_loss: 0.7194 - val_acc: 0.4706\n",
            "Epoch 434/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6552 - acc: 0.6508 - val_loss: 0.7195 - val_acc: 0.4706\n",
            "Epoch 435/500\n",
            "315/315 [==============================] - 0s 18us/step - loss: 0.6552 - acc: 0.6508 - val_loss: 0.7196 - val_acc: 0.4706\n",
            "Epoch 436/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6551 - acc: 0.6508 - val_loss: 0.7196 - val_acc: 0.4706\n",
            "Epoch 437/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6551 - acc: 0.6508 - val_loss: 0.7197 - val_acc: 0.4706\n",
            "Epoch 438/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6551 - acc: 0.6508 - val_loss: 0.7198 - val_acc: 0.4706\n",
            "Epoch 439/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6550 - acc: 0.6508 - val_loss: 0.7198 - val_acc: 0.4706\n",
            "Epoch 440/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6550 - acc: 0.6508 - val_loss: 0.7199 - val_acc: 0.4706\n",
            "Epoch 441/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6549 - acc: 0.6508 - val_loss: 0.7200 - val_acc: 0.4706\n",
            "Epoch 442/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6549 - acc: 0.6508 - val_loss: 0.7201 - val_acc: 0.4706\n",
            "Epoch 443/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6549 - acc: 0.6508 - val_loss: 0.7201 - val_acc: 0.4706\n",
            "Epoch 444/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6548 - acc: 0.6508 - val_loss: 0.7202 - val_acc: 0.4706\n",
            "Epoch 445/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6548 - acc: 0.6508 - val_loss: 0.7203 - val_acc: 0.4706\n",
            "Epoch 446/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6547 - acc: 0.6508 - val_loss: 0.7204 - val_acc: 0.4706\n",
            "Epoch 447/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6547 - acc: 0.6508 - val_loss: 0.7204 - val_acc: 0.4706\n",
            "Epoch 448/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6547 - acc: 0.6508 - val_loss: 0.7205 - val_acc: 0.4706\n",
            "Epoch 449/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6546 - acc: 0.6508 - val_loss: 0.7206 - val_acc: 0.4706\n",
            "Epoch 450/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6546 - acc: 0.6508 - val_loss: 0.7207 - val_acc: 0.4706\n",
            "Epoch 451/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6546 - acc: 0.6508 - val_loss: 0.7207 - val_acc: 0.4706\n",
            "Epoch 452/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6545 - acc: 0.6508 - val_loss: 0.7208 - val_acc: 0.4706\n",
            "Epoch 453/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6545 - acc: 0.6508 - val_loss: 0.7209 - val_acc: 0.4706\n",
            "Epoch 454/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6545 - acc: 0.6508 - val_loss: 0.7209 - val_acc: 0.4706\n",
            "Epoch 455/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6544 - acc: 0.6508 - val_loss: 0.7210 - val_acc: 0.4706\n",
            "Epoch 456/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6544 - acc: 0.6508 - val_loss: 0.7211 - val_acc: 0.4706\n",
            "Epoch 457/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6543 - acc: 0.6508 - val_loss: 0.7212 - val_acc: 0.4706\n",
            "Epoch 458/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6543 - acc: 0.6508 - val_loss: 0.7212 - val_acc: 0.4706\n",
            "Epoch 459/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6543 - acc: 0.6508 - val_loss: 0.7213 - val_acc: 0.4706\n",
            "Epoch 460/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6542 - acc: 0.6508 - val_loss: 0.7214 - val_acc: 0.4706\n",
            "Epoch 461/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6542 - acc: 0.6508 - val_loss: 0.7215 - val_acc: 0.4706\n",
            "Epoch 462/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6542 - acc: 0.6508 - val_loss: 0.7215 - val_acc: 0.4706\n",
            "Epoch 463/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6541 - acc: 0.6508 - val_loss: 0.7216 - val_acc: 0.4706\n",
            "Epoch 464/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6541 - acc: 0.6508 - val_loss: 0.7217 - val_acc: 0.4706\n",
            "Epoch 465/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6541 - acc: 0.6508 - val_loss: 0.7217 - val_acc: 0.4706\n",
            "Epoch 466/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6540 - acc: 0.6508 - val_loss: 0.7218 - val_acc: 0.4706\n",
            "Epoch 467/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6540 - acc: 0.6508 - val_loss: 0.7219 - val_acc: 0.4706\n",
            "Epoch 468/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6540 - acc: 0.6508 - val_loss: 0.7220 - val_acc: 0.4706\n",
            "Epoch 469/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6539 - acc: 0.6508 - val_loss: 0.7220 - val_acc: 0.4706\n",
            "Epoch 470/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6539 - acc: 0.6508 - val_loss: 0.7221 - val_acc: 0.4706\n",
            "Epoch 471/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6539 - acc: 0.6508 - val_loss: 0.7222 - val_acc: 0.4706\n",
            "Epoch 472/500\n",
            "315/315 [==============================] - 0s 18us/step - loss: 0.6538 - acc: 0.6508 - val_loss: 0.7222 - val_acc: 0.4706\n",
            "Epoch 473/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6538 - acc: 0.6508 - val_loss: 0.7223 - val_acc: 0.4706\n",
            "Epoch 474/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6538 - acc: 0.6508 - val_loss: 0.7224 - val_acc: 0.4706\n",
            "Epoch 475/500\n",
            "315/315 [==============================] - 0s 47us/step - loss: 0.6537 - acc: 0.6508 - val_loss: 0.7225 - val_acc: 0.4706\n",
            "Epoch 476/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6537 - acc: 0.6508 - val_loss: 0.7225 - val_acc: 0.4706\n",
            "Epoch 477/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6537 - acc: 0.6508 - val_loss: 0.7226 - val_acc: 0.4706\n",
            "Epoch 478/500\n",
            "315/315 [==============================] - 0s 33us/step - loss: 0.6536 - acc: 0.6508 - val_loss: 0.7227 - val_acc: 0.4706\n",
            "Epoch 479/500\n",
            "315/315 [==============================] - 0s 30us/step - loss: 0.6536 - acc: 0.6508 - val_loss: 0.7228 - val_acc: 0.4706\n",
            "Epoch 480/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6536 - acc: 0.6508 - val_loss: 0.7228 - val_acc: 0.4706\n",
            "Epoch 481/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6535 - acc: 0.6508 - val_loss: 0.7229 - val_acc: 0.4706\n",
            "Epoch 482/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6535 - acc: 0.6508 - val_loss: 0.7230 - val_acc: 0.4706\n",
            "Epoch 483/500\n",
            "315/315 [==============================] - 0s 21us/step - loss: 0.6535 - acc: 0.6508 - val_loss: 0.7230 - val_acc: 0.4706\n",
            "Epoch 484/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6534 - acc: 0.6508 - val_loss: 0.7231 - val_acc: 0.4706\n",
            "Epoch 485/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6534 - acc: 0.6508 - val_loss: 0.7232 - val_acc: 0.4706\n",
            "Epoch 486/500\n",
            "315/315 [==============================] - 0s 25us/step - loss: 0.6534 - acc: 0.6508 - val_loss: 0.7233 - val_acc: 0.4706\n",
            "Epoch 487/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6533 - acc: 0.6508 - val_loss: 0.7233 - val_acc: 0.4706\n",
            "Epoch 488/500\n",
            "315/315 [==============================] - 0s 23us/step - loss: 0.6533 - acc: 0.6508 - val_loss: 0.7234 - val_acc: 0.4706\n",
            "Epoch 489/500\n",
            "315/315 [==============================] - 0s 19us/step - loss: 0.6533 - acc: 0.6508 - val_loss: 0.7235 - val_acc: 0.4706\n",
            "Epoch 490/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6532 - acc: 0.6508 - val_loss: 0.7235 - val_acc: 0.4706\n",
            "Epoch 491/500\n",
            "315/315 [==============================] - 0s 29us/step - loss: 0.6532 - acc: 0.6508 - val_loss: 0.7236 - val_acc: 0.4706\n",
            "Epoch 492/500\n",
            "315/315 [==============================] - 0s 31us/step - loss: 0.6532 - acc: 0.6508 - val_loss: 0.7237 - val_acc: 0.4706\n",
            "Epoch 493/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6532 - acc: 0.6508 - val_loss: 0.7238 - val_acc: 0.4706\n",
            "Epoch 494/500\n",
            "315/315 [==============================] - 0s 26us/step - loss: 0.6531 - acc: 0.6508 - val_loss: 0.7238 - val_acc: 0.4706\n",
            "Epoch 495/500\n",
            "315/315 [==============================] - 0s 22us/step - loss: 0.6531 - acc: 0.6508 - val_loss: 0.7239 - val_acc: 0.4706\n",
            "Epoch 496/500\n",
            "315/315 [==============================] - 0s 24us/step - loss: 0.6531 - acc: 0.6508 - val_loss: 0.7240 - val_acc: 0.4706\n",
            "Epoch 497/500\n",
            "315/315 [==============================] - 0s 20us/step - loss: 0.6530 - acc: 0.6508 - val_loss: 0.7240 - val_acc: 0.4706\n",
            "Epoch 498/500\n",
            "315/315 [==============================] - 0s 27us/step - loss: 0.6530 - acc: 0.6508 - val_loss: 0.7241 - val_acc: 0.4706\n",
            "Epoch 499/500\n",
            "315/315 [==============================] - 0s 34us/step - loss: 0.6530 - acc: 0.6508 - val_loss: 0.7242 - val_acc: 0.4706\n",
            "Epoch 500/500\n",
            "315/315 [==============================] - 0s 28us/step - loss: 0.6529 - acc: 0.6508 - val_loss: 0.7243 - val_acc: 0.4706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCuKSRoeUoUd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "9c4d20bb-e2eb-4c3d-e58a-e88ada3095ae"
      },
      "source": [
        "ionosphere.iloc[:, -1]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      b\n",
              "1      g\n",
              "2      b\n",
              "3      g\n",
              "4      b\n",
              "      ..\n",
              "345    g\n",
              "346    g\n",
              "347    g\n",
              "348    g\n",
              "349    g\n",
              "Name: g, Length: 350, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1dSJFXsU6yD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHgab49IXSAU",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C74M21bXSNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras.datasets import reuters\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "import numpy as np\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdlIkalQXS6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n",
        "    num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "def to_one_hot(labels, dimension=46):\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "one_hot_train_labels = to_one_hot(train_labels)\n",
        "one_hot_test_labels = to_one_hot(test_labels)\n",
        "y_test = one_hot_test_labels\n",
        "\n",
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "partial_y_train = one_hot_train_labels[1000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RETrtWsUXYFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chollet_model = models.Sequential()\n",
        "chollet_model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "chollet_model.add(layers.Dense(64, activation='relu'))\n",
        "chollet_model.add(layers.Dense(46, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WXMSq-hXhKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "ec606835-9cc9-43b7-c3e5-51b876994099"
      },
      "source": [
        "chollet_model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = chollet_model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 2.5443 - acc: 0.5405 - val_loss: 1.6874 - val_acc: 0.6520\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 1.3742 - acc: 0.7093 - val_loss: 1.2770 - val_acc: 0.7180\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0264 - acc: 0.7757 - val_loss: 1.1146 - val_acc: 0.7500\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8105 - acc: 0.8267 - val_loss: 1.0324 - val_acc: 0.7810\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.6437 - acc: 0.8613 - val_loss: 0.9676 - val_acc: 0.7920\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.5149 - acc: 0.8887 - val_loss: 0.9308 - val_acc: 0.8110\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.4186 - acc: 0.9085 - val_loss: 0.9275 - val_acc: 0.8150\n",
            "Epoch 8/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3375 - acc: 0.9268 - val_loss: 0.9122 - val_acc: 0.8200\n",
            "Epoch 9/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2855 - acc: 0.9390 - val_loss: 0.9163 - val_acc: 0.8170\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2390 - acc: 0.9449 - val_loss: 0.9369 - val_acc: 0.8100\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2056 - acc: 0.9511 - val_loss: 0.9143 - val_acc: 0.8190\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1834 - acc: 0.9518 - val_loss: 0.9661 - val_acc: 0.8080\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1619 - acc: 0.9523 - val_loss: 0.9527 - val_acc: 0.8120\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.1482 - acc: 0.9549 - val_loss: 0.9962 - val_acc: 0.8090\n",
            "Epoch 15/20\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1404 - acc: 0.9540 - val_loss: 1.0321 - val_acc: 0.7970\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 0s 58us/step - loss: 0.1303 - acc: 0.9560 - val_loss: 1.0349 - val_acc: 0.7990\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1213 - acc: 0.9580 - val_loss: 1.0461 - val_acc: 0.8020\n",
            "Epoch 18/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.1212 - acc: 0.9565 - val_loss: 1.0610 - val_acc: 0.8020\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1110 - acc: 0.9575 - val_loss: 1.0975 - val_acc: 0.8010\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.1132 - acc: 0.9568 - val_loss: 1.1298 - val_acc: 0.7960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI-FebYPXpKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "32611c4f-d58e-44e6-e4f7-7a823904b787"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "\n",
        "plt.plot(epochs, acc, label='Training acc')\n",
        "plt.plot(epochs, val_acc, label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-NkEJIxZFs2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c758f84-4c8d-4755-ccdf-21338c3af6b2"
      },
      "source": [
        "np.random.seed(6473)\n",
        "dropout_rates = np.random.uniform(low = 0.0, high = 0.5, size = (8,))\n",
        "l2_penalties_exp = np.random.uniform(low = -6, high = -1, size = (8,))\n",
        "num_units = np.random.uniform(low = 46, high = 128, size = (8,)).astype(int)\n",
        "l2_penalties = 10**l2_penalties_exp\n",
        "\n",
        "val_acc = np.zeros((8,))\n",
        "\n",
        "for (i, (dropout_rate, l2_penalty, num_unit)) in enumerate(zip(dropout_rates, l2_penalties, num_units)):\n",
        "  model_dropout = models.Sequential()\n",
        "\n",
        "  model_dropout.add(layers.Dropout(rate = dropout_rate))\n",
        "  model_dropout.add(layers.Dense(num_unit, activation='relu', kernel_regularizer=regularizers.l2(l = l2_penalty), input_shape=(10000,)))\n",
        "  model_dropout.add(layers.Dropout(rate = dropout_rate))\n",
        "  model_dropout.add(layers.Dense(num_unit, activation='relu', kernel_regularizer=regularizers.l2(l = l2_penalty)))\n",
        "  model_dropout.add(layers.Dropout(rate = dropout_rate))\n",
        "  model_dropout.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "  model_dropout.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  history = model_dropout.fit(partial_x_train,\n",
        "                      partial_y_train,\n",
        "                      epochs=100,\n",
        "                      batch_size=512,\n",
        "                      validation_data=(x_val, y_val))\n",
        "  \n",
        "  val_acc[i] = history.history['val_acc'][-1]"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 3.3158 - acc: 0.3266 - val_loss: 2.3515 - val_acc: 0.5260\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 2.1381 - acc: 0.5114 - val_loss: 1.6845 - val_acc: 0.6050\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7165 - acc: 0.5981 - val_loss: 1.4768 - val_acc: 0.6710\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.5072 - acc: 0.6596 - val_loss: 1.3303 - val_acc: 0.7000\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 1.3415 - acc: 0.6989 - val_loss: 1.2395 - val_acc: 0.7210\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.2375 - acc: 0.7214 - val_loss: 1.1771 - val_acc: 0.7560\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.1461 - acc: 0.7497 - val_loss: 1.1216 - val_acc: 0.7610\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 1.0396 - acc: 0.7715 - val_loss: 1.0744 - val_acc: 0.7760\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.9705 - acc: 0.7863 - val_loss: 1.0444 - val_acc: 0.7870\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.9011 - acc: 0.7964 - val_loss: 1.0264 - val_acc: 0.7940\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8418 - acc: 0.8133 - val_loss: 1.0066 - val_acc: 0.8060\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.7905 - acc: 0.8208 - val_loss: 0.9977 - val_acc: 0.8050\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.7468 - acc: 0.8317 - val_loss: 0.9898 - val_acc: 0.8100\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7082 - acc: 0.8381 - val_loss: 0.9815 - val_acc: 0.8160\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6697 - acc: 0.8479 - val_loss: 0.9767 - val_acc: 0.8240\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6372 - acc: 0.8566 - val_loss: 0.9850 - val_acc: 0.8140\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.6183 - acc: 0.8622 - val_loss: 0.9714 - val_acc: 0.8230\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.5943 - acc: 0.8658 - val_loss: 0.9763 - val_acc: 0.8230\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.5593 - acc: 0.8776 - val_loss: 0.9584 - val_acc: 0.8240\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.5460 - acc: 0.8750 - val_loss: 0.9654 - val_acc: 0.8210\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.5286 - acc: 0.8855 - val_loss: 0.9569 - val_acc: 0.8210\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.5013 - acc: 0.8919 - val_loss: 0.9517 - val_acc: 0.8220\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.5030 - acc: 0.8929 - val_loss: 0.9700 - val_acc: 0.8220\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.4697 - acc: 0.9019 - val_loss: 0.9638 - val_acc: 0.8250\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4743 - acc: 0.8965 - val_loss: 0.9758 - val_acc: 0.8200\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.4662 - acc: 0.9004 - val_loss: 0.9738 - val_acc: 0.8200\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.4501 - acc: 0.9055 - val_loss: 0.9682 - val_acc: 0.8230\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.4458 - acc: 0.9082 - val_loss: 0.9851 - val_acc: 0.8260\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.4259 - acc: 0.9108 - val_loss: 0.9975 - val_acc: 0.8250\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.4171 - acc: 0.9147 - val_loss: 0.9881 - val_acc: 0.8280\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.4243 - acc: 0.9109 - val_loss: 1.0141 - val_acc: 0.8170\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.4069 - acc: 0.9121 - val_loss: 1.0155 - val_acc: 0.8260\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4070 - acc: 0.9167 - val_loss: 1.0121 - val_acc: 0.8230\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3989 - acc: 0.9196 - val_loss: 1.0125 - val_acc: 0.8300\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3906 - acc: 0.9223 - val_loss: 1.0159 - val_acc: 0.8260\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3970 - acc: 0.9189 - val_loss: 1.0232 - val_acc: 0.8230\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.3848 - acc: 0.9263 - val_loss: 1.0154 - val_acc: 0.8270\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3925 - acc: 0.9222 - val_loss: 1.0118 - val_acc: 0.8240\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3937 - acc: 0.9233 - val_loss: 1.0099 - val_acc: 0.8290\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3795 - acc: 0.9266 - val_loss: 1.0125 - val_acc: 0.8260\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3839 - acc: 0.9243 - val_loss: 1.0240 - val_acc: 0.8280\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3710 - acc: 0.9286 - val_loss: 1.0458 - val_acc: 0.8290\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3778 - acc: 0.9231 - val_loss: 1.0225 - val_acc: 0.8340\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3649 - acc: 0.9291 - val_loss: 1.0368 - val_acc: 0.8280\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3623 - acc: 0.9282 - val_loss: 1.0470 - val_acc: 0.8280\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3592 - acc: 0.9302 - val_loss: 1.0517 - val_acc: 0.8250\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3519 - acc: 0.9321 - val_loss: 1.0440 - val_acc: 0.8280\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3471 - acc: 0.9337 - val_loss: 1.0370 - val_acc: 0.8280\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3541 - acc: 0.9355 - val_loss: 1.0516 - val_acc: 0.8270\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3526 - acc: 0.9306 - val_loss: 1.0496 - val_acc: 0.8320\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3588 - acc: 0.9315 - val_loss: 1.0507 - val_acc: 0.8290\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3504 - acc: 0.9346 - val_loss: 1.0524 - val_acc: 0.8280\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3565 - acc: 0.9318 - val_loss: 1.0588 - val_acc: 0.8280\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3519 - acc: 0.9328 - val_loss: 1.0687 - val_acc: 0.8300\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3532 - acc: 0.9328 - val_loss: 1.0646 - val_acc: 0.8310\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 74us/step - loss: 0.3471 - acc: 0.9337 - val_loss: 1.0554 - val_acc: 0.8320\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3410 - acc: 0.9357 - val_loss: 1.0684 - val_acc: 0.8310\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3513 - acc: 0.9342 - val_loss: 1.0528 - val_acc: 0.8250\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3338 - acc: 0.9407 - val_loss: 1.0555 - val_acc: 0.8280\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3396 - acc: 0.9354 - val_loss: 1.0744 - val_acc: 0.8290\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3453 - acc: 0.9361 - val_loss: 1.0555 - val_acc: 0.8300\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3345 - acc: 0.9389 - val_loss: 1.0715 - val_acc: 0.8280\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3313 - acc: 0.9400 - val_loss: 1.0699 - val_acc: 0.8310\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 74us/step - loss: 0.3395 - acc: 0.9359 - val_loss: 1.0649 - val_acc: 0.8300\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3319 - acc: 0.9412 - val_loss: 1.0668 - val_acc: 0.8300\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3415 - acc: 0.9401 - val_loss: 1.0699 - val_acc: 0.8220\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3297 - acc: 0.9367 - val_loss: 1.0579 - val_acc: 0.8240\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3382 - acc: 0.9370 - val_loss: 1.0621 - val_acc: 0.8260\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3262 - acc: 0.9402 - val_loss: 1.0698 - val_acc: 0.8260\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3273 - acc: 0.9379 - val_loss: 1.0846 - val_acc: 0.8290\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 74us/step - loss: 0.3381 - acc: 0.9337 - val_loss: 1.0714 - val_acc: 0.8200\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3320 - acc: 0.9392 - val_loss: 1.0885 - val_acc: 0.8280\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3291 - acc: 0.9391 - val_loss: 1.0858 - val_acc: 0.8320\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3293 - acc: 0.9404 - val_loss: 1.0981 - val_acc: 0.8310\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3265 - acc: 0.9405 - val_loss: 1.0823 - val_acc: 0.8320\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3319 - acc: 0.9376 - val_loss: 1.0903 - val_acc: 0.8270\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3317 - acc: 0.9394 - val_loss: 1.0980 - val_acc: 0.8220\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3222 - acc: 0.9399 - val_loss: 1.1131 - val_acc: 0.8270\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3243 - acc: 0.9409 - val_loss: 1.1048 - val_acc: 0.8240\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3292 - acc: 0.9362 - val_loss: 1.0929 - val_acc: 0.8220\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3331 - acc: 0.9352 - val_loss: 1.0961 - val_acc: 0.8210\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3298 - acc: 0.9405 - val_loss: 1.1136 - val_acc: 0.8150\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3313 - acc: 0.9387 - val_loss: 1.0939 - val_acc: 0.8190\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3160 - acc: 0.9422 - val_loss: 1.1141 - val_acc: 0.8210\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3307 - acc: 0.9374 - val_loss: 1.1043 - val_acc: 0.8290\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3158 - acc: 0.9406 - val_loss: 1.1161 - val_acc: 0.8220\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3298 - acc: 0.9384 - val_loss: 1.1317 - val_acc: 0.8250\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3288 - acc: 0.9384 - val_loss: 1.1143 - val_acc: 0.8210\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3151 - acc: 0.9414 - val_loss: 1.1202 - val_acc: 0.8220\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3159 - acc: 0.9442 - val_loss: 1.1360 - val_acc: 0.8250\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3062 - acc: 0.9432 - val_loss: 1.1327 - val_acc: 0.8180\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3100 - acc: 0.9455 - val_loss: 1.1333 - val_acc: 0.8190\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3190 - acc: 0.9407 - val_loss: 1.1240 - val_acc: 0.8230\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3142 - acc: 0.9460 - val_loss: 1.1303 - val_acc: 0.8170\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3149 - acc: 0.9429 - val_loss: 1.1353 - val_acc: 0.8190\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3134 - acc: 0.9399 - val_loss: 1.1371 - val_acc: 0.8190\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3204 - acc: 0.9392 - val_loss: 1.1595 - val_acc: 0.8210\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3144 - acc: 0.9432 - val_loss: 1.1314 - val_acc: 0.8260\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3093 - acc: 0.9456 - val_loss: 1.1399 - val_acc: 0.8180\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3167 - acc: 0.9424 - val_loss: 1.1380 - val_acc: 0.8190\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 3.4652 - acc: 0.3782 - val_loss: 2.6056 - val_acc: 0.5630\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 2.1512 - acc: 0.5862 - val_loss: 1.6646 - val_acc: 0.6600\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.5488 - acc: 0.6825 - val_loss: 1.3794 - val_acc: 0.7250\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 1.2780 - acc: 0.7427 - val_loss: 1.2536 - val_acc: 0.7570\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 1.1099 - acc: 0.7870 - val_loss: 1.1864 - val_acc: 0.7770\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.9829 - acc: 0.8156 - val_loss: 1.1307 - val_acc: 0.7940\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.8666 - acc: 0.8425 - val_loss: 1.0840 - val_acc: 0.8120\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.7800 - acc: 0.8690 - val_loss: 1.0686 - val_acc: 0.8070\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.7163 - acc: 0.8804 - val_loss: 1.0529 - val_acc: 0.8220\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6554 - acc: 0.8936 - val_loss: 1.0514 - val_acc: 0.8270\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.6185 - acc: 0.9049 - val_loss: 1.0437 - val_acc: 0.8320\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.5796 - acc: 0.9161 - val_loss: 1.0536 - val_acc: 0.8230\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.5550 - acc: 0.9221 - val_loss: 1.0469 - val_acc: 0.8240\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.5274 - acc: 0.9237 - val_loss: 1.0577 - val_acc: 0.8230\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.5108 - acc: 0.9315 - val_loss: 1.0595 - val_acc: 0.8280\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.5124 - acc: 0.9297 - val_loss: 1.0525 - val_acc: 0.8240\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4939 - acc: 0.9323 - val_loss: 1.0558 - val_acc: 0.8280\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.4691 - acc: 0.9399 - val_loss: 1.0600 - val_acc: 0.8210\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.4700 - acc: 0.9389 - val_loss: 1.0629 - val_acc: 0.8150\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.4583 - acc: 0.9385 - val_loss: 1.0659 - val_acc: 0.8190\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.4486 - acc: 0.9434 - val_loss: 1.0603 - val_acc: 0.8220\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4435 - acc: 0.9449 - val_loss: 1.0581 - val_acc: 0.8240\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.4304 - acc: 0.9450 - val_loss: 1.0659 - val_acc: 0.8210\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.4317 - acc: 0.9463 - val_loss: 1.0884 - val_acc: 0.8180\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.4245 - acc: 0.9459 - val_loss: 1.0723 - val_acc: 0.8300\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.4232 - acc: 0.9450 - val_loss: 1.0904 - val_acc: 0.8240\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.4177 - acc: 0.9504 - val_loss: 1.0998 - val_acc: 0.8140\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4184 - acc: 0.9460 - val_loss: 1.0900 - val_acc: 0.8250\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4111 - acc: 0.9480 - val_loss: 1.0788 - val_acc: 0.8190\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.4070 - acc: 0.9523 - val_loss: 1.0763 - val_acc: 0.8220\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.4080 - acc: 0.9475 - val_loss: 1.0735 - val_acc: 0.8170\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.4002 - acc: 0.9494 - val_loss: 1.0756 - val_acc: 0.8220\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.3968 - acc: 0.9505 - val_loss: 1.0978 - val_acc: 0.8270\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3906 - acc: 0.9505 - val_loss: 1.0922 - val_acc: 0.8140\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3938 - acc: 0.9505 - val_loss: 1.0830 - val_acc: 0.8190\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.3915 - acc: 0.9515 - val_loss: 1.0833 - val_acc: 0.8170\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3790 - acc: 0.9519 - val_loss: 1.1042 - val_acc: 0.8210\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3844 - acc: 0.9501 - val_loss: 1.1077 - val_acc: 0.8140\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3810 - acc: 0.9518 - val_loss: 1.0709 - val_acc: 0.8240\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3746 - acc: 0.9520 - val_loss: 1.0982 - val_acc: 0.8190\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3723 - acc: 0.9538 - val_loss: 1.1156 - val_acc: 0.8170\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3715 - acc: 0.9520 - val_loss: 1.1090 - val_acc: 0.8200\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3708 - acc: 0.9531 - val_loss: 1.1133 - val_acc: 0.8190\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3719 - acc: 0.9511 - val_loss: 1.0883 - val_acc: 0.8190\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3702 - acc: 0.9509 - val_loss: 1.0951 - val_acc: 0.8160\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 75us/step - loss: 0.3621 - acc: 0.9536 - val_loss: 1.0998 - val_acc: 0.8100\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 75us/step - loss: 0.3663 - acc: 0.9498 - val_loss: 1.0908 - val_acc: 0.8230\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3653 - acc: 0.9524 - val_loss: 1.0992 - val_acc: 0.8210\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3519 - acc: 0.9553 - val_loss: 1.1182 - val_acc: 0.8160\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3579 - acc: 0.9551 - val_loss: 1.1073 - val_acc: 0.8170\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3511 - acc: 0.9550 - val_loss: 1.1144 - val_acc: 0.8190\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3556 - acc: 0.9544 - val_loss: 1.1154 - val_acc: 0.8160\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3572 - acc: 0.9536 - val_loss: 1.1007 - val_acc: 0.8170\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3488 - acc: 0.9540 - val_loss: 1.1054 - val_acc: 0.8160\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 74us/step - loss: 0.3519 - acc: 0.9548 - val_loss: 1.1145 - val_acc: 0.8120\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3448 - acc: 0.9538 - val_loss: 1.0945 - val_acc: 0.8190\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3488 - acc: 0.9520 - val_loss: 1.0876 - val_acc: 0.8130\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3488 - acc: 0.9524 - val_loss: 1.1065 - val_acc: 0.8150\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3440 - acc: 0.9551 - val_loss: 1.1026 - val_acc: 0.8140\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3426 - acc: 0.9536 - val_loss: 1.1129 - val_acc: 0.8160\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3339 - acc: 0.9545 - val_loss: 1.0731 - val_acc: 0.8230\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 74us/step - loss: 0.3404 - acc: 0.9553 - val_loss: 1.0836 - val_acc: 0.8160\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3385 - acc: 0.9577 - val_loss: 1.0923 - val_acc: 0.8110\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3315 - acc: 0.9551 - val_loss: 1.1257 - val_acc: 0.8140\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3343 - acc: 0.9562 - val_loss: 1.1166 - val_acc: 0.8190\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3360 - acc: 0.9541 - val_loss: 1.1147 - val_acc: 0.8150\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 75us/step - loss: 0.3360 - acc: 0.9548 - val_loss: 1.1314 - val_acc: 0.8170\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3300 - acc: 0.9550 - val_loss: 1.1270 - val_acc: 0.8210\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3323 - acc: 0.9546 - val_loss: 1.1241 - val_acc: 0.8170\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3376 - acc: 0.9539 - val_loss: 1.1063 - val_acc: 0.8170\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3294 - acc: 0.9564 - val_loss: 1.1181 - val_acc: 0.8120\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3273 - acc: 0.9565 - val_loss: 1.1161 - val_acc: 0.8130\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3235 - acc: 0.9568 - val_loss: 1.1011 - val_acc: 0.8210\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3239 - acc: 0.9580 - val_loss: 1.0981 - val_acc: 0.8120\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3243 - acc: 0.9557 - val_loss: 1.0958 - val_acc: 0.8160\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3251 - acc: 0.9551 - val_loss: 1.1084 - val_acc: 0.8180\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3166 - acc: 0.9577 - val_loss: 1.0888 - val_acc: 0.8190\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 74us/step - loss: 0.3176 - acc: 0.9563 - val_loss: 1.1314 - val_acc: 0.8140\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3209 - acc: 0.9582 - val_loss: 1.1151 - val_acc: 0.8180\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3184 - acc: 0.9580 - val_loss: 1.1087 - val_acc: 0.8210\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3245 - acc: 0.9543 - val_loss: 1.1084 - val_acc: 0.8180\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3169 - acc: 0.9568 - val_loss: 1.1022 - val_acc: 0.8200\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3154 - acc: 0.9579 - val_loss: 1.1186 - val_acc: 0.8180\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3101 - acc: 0.9575 - val_loss: 1.1357 - val_acc: 0.8200\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3162 - acc: 0.9553 - val_loss: 1.1189 - val_acc: 0.8160\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3165 - acc: 0.9562 - val_loss: 1.1128 - val_acc: 0.8200\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3099 - acc: 0.9565 - val_loss: 1.1155 - val_acc: 0.8160\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3106 - acc: 0.9545 - val_loss: 1.1066 - val_acc: 0.8240\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3114 - acc: 0.9546 - val_loss: 1.1183 - val_acc: 0.8190\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3115 - acc: 0.9543 - val_loss: 1.0899 - val_acc: 0.8180\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3082 - acc: 0.9582 - val_loss: 1.1111 - val_acc: 0.8170\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3133 - acc: 0.9563 - val_loss: 1.0861 - val_acc: 0.8220\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3118 - acc: 0.9564 - val_loss: 1.0908 - val_acc: 0.8200\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3060 - acc: 0.9570 - val_loss: 1.0990 - val_acc: 0.8170\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3058 - acc: 0.9562 - val_loss: 1.1008 - val_acc: 0.8210\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3048 - acc: 0.9590 - val_loss: 1.1083 - val_acc: 0.8140\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3058 - acc: 0.9575 - val_loss: 1.0977 - val_acc: 0.8170\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3013 - acc: 0.9603 - val_loss: 1.1209 - val_acc: 0.8110\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.3003 - acc: 0.9583 - val_loss: 1.0930 - val_acc: 0.8120\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.3073 - acc: 0.9548 - val_loss: 1.1073 - val_acc: 0.8170\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 4.3689 - acc: 0.4330 - val_loss: 3.1182 - val_acc: 0.5440\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.5939 - acc: 0.5956 - val_loss: 2.1976 - val_acc: 0.6420\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.0487 - acc: 0.6891 - val_loss: 1.9188 - val_acc: 0.6980\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8027 - acc: 0.7326 - val_loss: 1.7851 - val_acc: 0.7200\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6428 - acc: 0.7578 - val_loss: 1.6854 - val_acc: 0.7460\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.5359 - acc: 0.7759 - val_loss: 1.6139 - val_acc: 0.7610\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.4514 - acc: 0.7930 - val_loss: 1.5690 - val_acc: 0.7660\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.3880 - acc: 0.8056 - val_loss: 1.5405 - val_acc: 0.7640\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.3355 - acc: 0.8170 - val_loss: 1.4993 - val_acc: 0.7740\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.2988 - acc: 0.8197 - val_loss: 1.4810 - val_acc: 0.7800\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.2562 - acc: 0.8304 - val_loss: 1.4572 - val_acc: 0.7820\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.2316 - acc: 0.8299 - val_loss: 1.4365 - val_acc: 0.7830\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.1978 - acc: 0.8394 - val_loss: 1.4356 - val_acc: 0.7830\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.1802 - acc: 0.8386 - val_loss: 1.4238 - val_acc: 0.7810\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.1537 - acc: 0.8460 - val_loss: 1.3944 - val_acc: 0.7900\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.1350 - acc: 0.8517 - val_loss: 1.3939 - val_acc: 0.7900\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.1179 - acc: 0.8510 - val_loss: 1.3792 - val_acc: 0.7890\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.1012 - acc: 0.8545 - val_loss: 1.3612 - val_acc: 0.7960\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.0796 - acc: 0.8614 - val_loss: 1.3683 - val_acc: 0.7930\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.0624 - acc: 0.8617 - val_loss: 1.3513 - val_acc: 0.7950\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.0566 - acc: 0.8647 - val_loss: 1.3379 - val_acc: 0.8060\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.0432 - acc: 0.8685 - val_loss: 1.3318 - val_acc: 0.8000\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.0308 - acc: 0.8682 - val_loss: 1.3419 - val_acc: 0.7940\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.0233 - acc: 0.8686 - val_loss: 1.3358 - val_acc: 0.8030\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.0090 - acc: 0.8727 - val_loss: 1.3182 - val_acc: 0.8000\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.9975 - acc: 0.8732 - val_loss: 1.3307 - val_acc: 0.7970\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.9813 - acc: 0.8761 - val_loss: 1.3078 - val_acc: 0.7960\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.9706 - acc: 0.8792 - val_loss: 1.3058 - val_acc: 0.8020\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.9670 - acc: 0.8810 - val_loss: 1.2970 - val_acc: 0.8040\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.9566 - acc: 0.8812 - val_loss: 1.3065 - val_acc: 0.8000\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.9439 - acc: 0.8814 - val_loss: 1.2990 - val_acc: 0.8020\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.9436 - acc: 0.8847 - val_loss: 1.2925 - val_acc: 0.8100\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.9321 - acc: 0.8846 - val_loss: 1.2828 - val_acc: 0.8090\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.9292 - acc: 0.8859 - val_loss: 1.2813 - val_acc: 0.8130\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.9148 - acc: 0.8881 - val_loss: 1.2695 - val_acc: 0.8000\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.9070 - acc: 0.8910 - val_loss: 1.2765 - val_acc: 0.8100\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.9064 - acc: 0.8880 - val_loss: 1.2764 - val_acc: 0.8140\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.8946 - acc: 0.8954 - val_loss: 1.2668 - val_acc: 0.8040\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.8921 - acc: 0.8920 - val_loss: 1.2681 - val_acc: 0.8190\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.8937 - acc: 0.8915 - val_loss: 1.2683 - val_acc: 0.8030\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8761 - acc: 0.8996 - val_loss: 1.2598 - val_acc: 0.8130\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.8729 - acc: 0.8954 - val_loss: 1.2608 - val_acc: 0.8090\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8682 - acc: 0.8978 - val_loss: 1.2560 - val_acc: 0.8060\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.8625 - acc: 0.8994 - val_loss: 1.2472 - val_acc: 0.8080\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8587 - acc: 0.8964 - val_loss: 1.2557 - val_acc: 0.8100\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.8493 - acc: 0.9008 - val_loss: 1.2484 - val_acc: 0.8080\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.8410 - acc: 0.9043 - val_loss: 1.2569 - val_acc: 0.8090\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.8390 - acc: 0.9035 - val_loss: 1.2433 - val_acc: 0.8110\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.8348 - acc: 0.9052 - val_loss: 1.2312 - val_acc: 0.8130\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8202 - acc: 0.9068 - val_loss: 1.2424 - val_acc: 0.8100\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.8190 - acc: 0.9084 - val_loss: 1.2505 - val_acc: 0.8090\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.8193 - acc: 0.9052 - val_loss: 1.2392 - val_acc: 0.8080\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.8159 - acc: 0.9072 - val_loss: 1.2390 - val_acc: 0.8130\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.8145 - acc: 0.9080 - val_loss: 1.2256 - val_acc: 0.8110\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.8099 - acc: 0.9077 - val_loss: 1.2295 - val_acc: 0.8160\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.8115 - acc: 0.9116 - val_loss: 1.2293 - val_acc: 0.8170\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.8019 - acc: 0.9099 - val_loss: 1.2319 - val_acc: 0.8140\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.7990 - acc: 0.9104 - val_loss: 1.2334 - val_acc: 0.8030\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.7878 - acc: 0.9123 - val_loss: 1.2395 - val_acc: 0.8090\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 0.7914 - acc: 0.9113 - val_loss: 1.2266 - val_acc: 0.8050\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.7711 - acc: 0.9173 - val_loss: 1.2145 - val_acc: 0.8130\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.7846 - acc: 0.9122 - val_loss: 1.2350 - val_acc: 0.8130\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7812 - acc: 0.9121 - val_loss: 1.2326 - val_acc: 0.8070\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7690 - acc: 0.9171 - val_loss: 1.2282 - val_acc: 0.8050\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7821 - acc: 0.9113 - val_loss: 1.2250 - val_acc: 0.8130\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.7723 - acc: 0.9129 - val_loss: 1.2406 - val_acc: 0.8150\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7628 - acc: 0.9149 - val_loss: 1.2274 - val_acc: 0.8080\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.7573 - acc: 0.9191 - val_loss: 1.2173 - val_acc: 0.8150\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7537 - acc: 0.9171 - val_loss: 1.2076 - val_acc: 0.8080\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7479 - acc: 0.9183 - val_loss: 1.2223 - val_acc: 0.8200\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.7465 - acc: 0.9198 - val_loss: 1.2103 - val_acc: 0.8180\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7406 - acc: 0.9207 - val_loss: 1.2175 - val_acc: 0.8120\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7453 - acc: 0.9191 - val_loss: 1.2161 - val_acc: 0.8110\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7401 - acc: 0.9183 - val_loss: 1.2158 - val_acc: 0.8070\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7438 - acc: 0.9166 - val_loss: 1.2020 - val_acc: 0.8150\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7397 - acc: 0.9171 - val_loss: 1.2153 - val_acc: 0.8140\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7238 - acc: 0.9240 - val_loss: 1.1985 - val_acc: 0.8140\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7266 - acc: 0.9212 - val_loss: 1.2062 - val_acc: 0.8150\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7229 - acc: 0.9221 - val_loss: 1.2023 - val_acc: 0.8230\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7129 - acc: 0.9232 - val_loss: 1.2036 - val_acc: 0.8160\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7134 - acc: 0.9217 - val_loss: 1.2049 - val_acc: 0.8100\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7079 - acc: 0.9240 - val_loss: 1.1890 - val_acc: 0.8210\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7028 - acc: 0.9272 - val_loss: 1.1905 - val_acc: 0.8150\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.7125 - acc: 0.9227 - val_loss: 1.2124 - val_acc: 0.8120\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7140 - acc: 0.9226 - val_loss: 1.1924 - val_acc: 0.8100\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.7019 - acc: 0.9231 - val_loss: 1.1928 - val_acc: 0.8190\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.7019 - acc: 0.9267 - val_loss: 1.1929 - val_acc: 0.8090\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.6930 - acc: 0.9285 - val_loss: 1.1929 - val_acc: 0.8190\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6943 - acc: 0.9290 - val_loss: 1.1916 - val_acc: 0.8220\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6958 - acc: 0.9243 - val_loss: 1.1855 - val_acc: 0.8240\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.6879 - acc: 0.9273 - val_loss: 1.1987 - val_acc: 0.8190\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.6863 - acc: 0.9283 - val_loss: 1.1858 - val_acc: 0.8160\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6849 - acc: 0.9277 - val_loss: 1.1937 - val_acc: 0.8090\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6800 - acc: 0.9263 - val_loss: 1.1842 - val_acc: 0.8180\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.6817 - acc: 0.9280 - val_loss: 1.1992 - val_acc: 0.8110\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.6776 - acc: 0.9297 - val_loss: 1.1871 - val_acc: 0.8190\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.6824 - acc: 0.9280 - val_loss: 1.1903 - val_acc: 0.8240\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.6743 - acc: 0.9287 - val_loss: 1.1973 - val_acc: 0.8060\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.6720 - acc: 0.9287 - val_loss: 1.1922 - val_acc: 0.8230\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.6651 - acc: 0.9291 - val_loss: 1.1730 - val_acc: 0.8220\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 3.4459 - acc: 0.2865 - val_loss: 2.7227 - val_acc: 0.4220\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 2.3262 - acc: 0.4990 - val_loss: 1.7502 - val_acc: 0.6100\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7671 - acc: 0.5906 - val_loss: 1.4764 - val_acc: 0.6670\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.5078 - acc: 0.6527 - val_loss: 1.3226 - val_acc: 0.6970\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.3526 - acc: 0.6907 - val_loss: 1.2190 - val_acc: 0.7350\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.2262 - acc: 0.7204 - val_loss: 1.1421 - val_acc: 0.7480\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.1211 - acc: 0.7438 - val_loss: 1.0941 - val_acc: 0.7610\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.0386 - acc: 0.7642 - val_loss: 1.0450 - val_acc: 0.7770\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.9573 - acc: 0.7795 - val_loss: 1.0072 - val_acc: 0.7830\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8878 - acc: 0.7944 - val_loss: 0.9745 - val_acc: 0.7880\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.8214 - acc: 0.8068 - val_loss: 0.9563 - val_acc: 0.7940\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.7636 - acc: 0.8170 - val_loss: 0.9379 - val_acc: 0.8000\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.7217 - acc: 0.8244 - val_loss: 0.9238 - val_acc: 0.8050\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.6830 - acc: 0.8336 - val_loss: 0.9109 - val_acc: 0.8150\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.6501 - acc: 0.8430 - val_loss: 0.9096 - val_acc: 0.8110\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.5938 - acc: 0.8576 - val_loss: 0.9000 - val_acc: 0.8140\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.5680 - acc: 0.8616 - val_loss: 0.8951 - val_acc: 0.8180\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.5399 - acc: 0.8651 - val_loss: 0.8925 - val_acc: 0.8130\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.5141 - acc: 0.8741 - val_loss: 0.8824 - val_acc: 0.8180\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4863 - acc: 0.8766 - val_loss: 0.8909 - val_acc: 0.8190\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.4525 - acc: 0.8856 - val_loss: 0.8914 - val_acc: 0.8140\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.4503 - acc: 0.8875 - val_loss: 0.8900 - val_acc: 0.8180\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.4301 - acc: 0.8920 - val_loss: 0.8877 - val_acc: 0.8230\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.4185 - acc: 0.8963 - val_loss: 0.8941 - val_acc: 0.8230\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.4036 - acc: 0.8975 - val_loss: 0.9042 - val_acc: 0.8170\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3927 - acc: 0.8980 - val_loss: 0.9047 - val_acc: 0.8240\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3857 - acc: 0.9005 - val_loss: 0.9144 - val_acc: 0.8260\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.3560 - acc: 0.9093 - val_loss: 0.9197 - val_acc: 0.8240\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.3458 - acc: 0.9098 - val_loss: 0.9238 - val_acc: 0.8290\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.3398 - acc: 0.9132 - val_loss: 0.9366 - val_acc: 0.8240\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3374 - acc: 0.9107 - val_loss: 0.9246 - val_acc: 0.8300\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 0.3307 - acc: 0.9152 - val_loss: 0.9247 - val_acc: 0.8300\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3121 - acc: 0.9187 - val_loss: 0.9294 - val_acc: 0.8270\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.3161 - acc: 0.9171 - val_loss: 0.9241 - val_acc: 0.8280\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.3057 - acc: 0.9204 - val_loss: 0.9278 - val_acc: 0.8320\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2935 - acc: 0.9232 - val_loss: 0.9434 - val_acc: 0.8310\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.2942 - acc: 0.9247 - val_loss: 0.9346 - val_acc: 0.8300\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2814 - acc: 0.9252 - val_loss: 0.9522 - val_acc: 0.8280\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.2792 - acc: 0.9255 - val_loss: 0.9583 - val_acc: 0.8280\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2742 - acc: 0.9260 - val_loss: 0.9708 - val_acc: 0.8290\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.2708 - acc: 0.9272 - val_loss: 0.9557 - val_acc: 0.8290\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.2631 - acc: 0.9258 - val_loss: 0.9746 - val_acc: 0.8290\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.2646 - acc: 0.9313 - val_loss: 0.9629 - val_acc: 0.8260\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2514 - acc: 0.9344 - val_loss: 0.9902 - val_acc: 0.8240\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.2572 - acc: 0.9320 - val_loss: 0.9956 - val_acc: 0.8270\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2487 - acc: 0.9331 - val_loss: 1.0099 - val_acc: 0.8240\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.2389 - acc: 0.9336 - val_loss: 1.0174 - val_acc: 0.8220\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.2411 - acc: 0.9356 - val_loss: 1.0100 - val_acc: 0.8220\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2448 - acc: 0.9327 - val_loss: 1.0130 - val_acc: 0.8210\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2411 - acc: 0.9300 - val_loss: 1.0117 - val_acc: 0.8210\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2304 - acc: 0.9376 - val_loss: 1.0173 - val_acc: 0.8180\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2260 - acc: 0.9389 - val_loss: 1.0337 - val_acc: 0.8180\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.2292 - acc: 0.9386 - val_loss: 1.0163 - val_acc: 0.8220\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.2233 - acc: 0.9400 - val_loss: 1.0246 - val_acc: 0.8190\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2223 - acc: 0.9410 - val_loss: 1.0266 - val_acc: 0.8190\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2251 - acc: 0.9371 - val_loss: 1.0395 - val_acc: 0.8210\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.2131 - acc: 0.9407 - val_loss: 1.0486 - val_acc: 0.8190\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2215 - acc: 0.9409 - val_loss: 1.0405 - val_acc: 0.8200\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2167 - acc: 0.9381 - val_loss: 1.0451 - val_acc: 0.8180\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.2135 - acc: 0.9387 - val_loss: 1.0587 - val_acc: 0.8190\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2105 - acc: 0.9405 - val_loss: 1.0771 - val_acc: 0.8180\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2161 - acc: 0.9402 - val_loss: 1.0728 - val_acc: 0.8170\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.2019 - acc: 0.9449 - val_loss: 1.0802 - val_acc: 0.8160\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.2115 - acc: 0.9420 - val_loss: 1.0653 - val_acc: 0.8190\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.2123 - acc: 0.9390 - val_loss: 1.0725 - val_acc: 0.8210\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.2049 - acc: 0.9439 - val_loss: 1.0691 - val_acc: 0.8170\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 76us/step - loss: 0.2005 - acc: 0.9440 - val_loss: 1.0872 - val_acc: 0.8160\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.2056 - acc: 0.9410 - val_loss: 1.0868 - val_acc: 0.8150\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.2052 - acc: 0.9424 - val_loss: 1.0951 - val_acc: 0.8130\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1996 - acc: 0.9427 - val_loss: 1.1003 - val_acc: 0.8150\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.2073 - acc: 0.9420 - val_loss: 1.0994 - val_acc: 0.8160\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.2035 - acc: 0.9431 - val_loss: 1.1003 - val_acc: 0.8180\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.1927 - acc: 0.9434 - val_loss: 1.1161 - val_acc: 0.8210\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.1987 - acc: 0.9432 - val_loss: 1.1035 - val_acc: 0.8200\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.2017 - acc: 0.9426 - val_loss: 1.1126 - val_acc: 0.8210\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1942 - acc: 0.9446 - val_loss: 1.1123 - val_acc: 0.8170\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.1853 - acc: 0.9455 - val_loss: 1.1093 - val_acc: 0.8220\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1928 - acc: 0.9449 - val_loss: 1.1267 - val_acc: 0.8200\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.1884 - acc: 0.9459 - val_loss: 1.1304 - val_acc: 0.8180\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1883 - acc: 0.9481 - val_loss: 1.1461 - val_acc: 0.8190\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1903 - acc: 0.9461 - val_loss: 1.1297 - val_acc: 0.8200\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.1845 - acc: 0.9491 - val_loss: 1.1211 - val_acc: 0.8200\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1810 - acc: 0.9473 - val_loss: 1.1444 - val_acc: 0.8230\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1841 - acc: 0.9463 - val_loss: 1.1392 - val_acc: 0.8210\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1805 - acc: 0.9480 - val_loss: 1.1532 - val_acc: 0.8190\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1791 - acc: 0.9501 - val_loss: 1.1514 - val_acc: 0.8210\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1803 - acc: 0.9455 - val_loss: 1.1444 - val_acc: 0.8210\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.1812 - acc: 0.9476 - val_loss: 1.1382 - val_acc: 0.8250\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1879 - acc: 0.9456 - val_loss: 1.1496 - val_acc: 0.8250\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.1762 - acc: 0.9473 - val_loss: 1.1559 - val_acc: 0.8230\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.1866 - acc: 0.9459 - val_loss: 1.1390 - val_acc: 0.8190\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 0.1887 - acc: 0.9464 - val_loss: 1.1434 - val_acc: 0.8160\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 0.1734 - acc: 0.9494 - val_loss: 1.1501 - val_acc: 0.8190\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.1729 - acc: 0.9494 - val_loss: 1.1522 - val_acc: 0.8250\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.1760 - acc: 0.9479 - val_loss: 1.1689 - val_acc: 0.8160\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.1738 - acc: 0.9469 - val_loss: 1.1758 - val_acc: 0.8190\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.1737 - acc: 0.9486 - val_loss: 1.1699 - val_acc: 0.8230\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1754 - acc: 0.9488 - val_loss: 1.1685 - val_acc: 0.8250\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1747 - acc: 0.9486 - val_loss: 1.1795 - val_acc: 0.8190\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1781 - acc: 0.9469 - val_loss: 1.1631 - val_acc: 0.8190\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 8.4009 - acc: 0.2881 - val_loss: 5.5186 - val_acc: 0.3540\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 4.7809 - acc: 0.4342 - val_loss: 3.9953 - val_acc: 0.5430\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 3.8542 - acc: 0.5284 - val_loss: 3.3607 - val_acc: 0.5950\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 3.3051 - acc: 0.5739 - val_loss: 2.9527 - val_acc: 0.6270\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 2.9681 - acc: 0.5928 - val_loss: 2.6683 - val_acc: 0.6360\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 2.7072 - acc: 0.6054 - val_loss: 2.4550 - val_acc: 0.6590\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 2.5143 - acc: 0.6210 - val_loss: 2.3003 - val_acc: 0.6640\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.3530 - acc: 0.6252 - val_loss: 2.1677 - val_acc: 0.6640\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.2334 - acc: 0.6309 - val_loss: 2.0697 - val_acc: 0.6710\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.1424 - acc: 0.6367 - val_loss: 1.9928 - val_acc: 0.6680\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.0714 - acc: 0.6463 - val_loss: 1.9294 - val_acc: 0.6740\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 2.0144 - acc: 0.6480 - val_loss: 1.8885 - val_acc: 0.6760\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.9620 - acc: 0.6528 - val_loss: 1.8474 - val_acc: 0.6740\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.9291 - acc: 0.6527 - val_loss: 1.8153 - val_acc: 0.6790\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.9086 - acc: 0.6548 - val_loss: 1.7886 - val_acc: 0.6840\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.8747 - acc: 0.6664 - val_loss: 1.7696 - val_acc: 0.6850\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8548 - acc: 0.6590 - val_loss: 1.7519 - val_acc: 0.6890\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.8384 - acc: 0.6622 - val_loss: 1.7307 - val_acc: 0.6860\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.8264 - acc: 0.6710 - val_loss: 1.7253 - val_acc: 0.6860\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.8070 - acc: 0.6700 - val_loss: 1.7111 - val_acc: 0.6920\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.8066 - acc: 0.6670 - val_loss: 1.7017 - val_acc: 0.6890\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.7859 - acc: 0.6718 - val_loss: 1.6999 - val_acc: 0.6870\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.7855 - acc: 0.6708 - val_loss: 1.6894 - val_acc: 0.6860\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7846 - acc: 0.6706 - val_loss: 1.6881 - val_acc: 0.6930\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7747 - acc: 0.6739 - val_loss: 1.6719 - val_acc: 0.6860\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7567 - acc: 0.6709 - val_loss: 1.6690 - val_acc: 0.6870\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 1.7533 - acc: 0.6731 - val_loss: 1.6561 - val_acc: 0.6980\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7379 - acc: 0.6775 - val_loss: 1.6506 - val_acc: 0.6890\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7404 - acc: 0.6775 - val_loss: 1.6515 - val_acc: 0.6900\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7405 - acc: 0.6736 - val_loss: 1.6498 - val_acc: 0.6870\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7268 - acc: 0.6771 - val_loss: 1.6387 - val_acc: 0.6940\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7274 - acc: 0.6763 - val_loss: 1.6341 - val_acc: 0.6980\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7129 - acc: 0.6789 - val_loss: 1.6338 - val_acc: 0.6910\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.7194 - acc: 0.6788 - val_loss: 1.6247 - val_acc: 0.6970\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7012 - acc: 0.6813 - val_loss: 1.6258 - val_acc: 0.6880\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6999 - acc: 0.6794 - val_loss: 1.6218 - val_acc: 0.6950\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7000 - acc: 0.6818 - val_loss: 1.6085 - val_acc: 0.6990\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7016 - acc: 0.6820 - val_loss: 1.6123 - val_acc: 0.6960\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6977 - acc: 0.6805 - val_loss: 1.6071 - val_acc: 0.7000\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6852 - acc: 0.6815 - val_loss: 1.6010 - val_acc: 0.6940\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.6709 - acc: 0.6855 - val_loss: 1.5950 - val_acc: 0.7000\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 1.6774 - acc: 0.6827 - val_loss: 1.5925 - val_acc: 0.7000\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6853 - acc: 0.6809 - val_loss: 1.6023 - val_acc: 0.6930\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6778 - acc: 0.6850 - val_loss: 1.5989 - val_acc: 0.6960\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6730 - acc: 0.6824 - val_loss: 1.5899 - val_acc: 0.6960\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6610 - acc: 0.6891 - val_loss: 1.5905 - val_acc: 0.6980\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6670 - acc: 0.6844 - val_loss: 1.5774 - val_acc: 0.6990\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 1.6575 - acc: 0.6888 - val_loss: 1.5826 - val_acc: 0.7010\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6481 - acc: 0.6868 - val_loss: 1.5782 - val_acc: 0.7070\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.6606 - acc: 0.6883 - val_loss: 1.5691 - val_acc: 0.7030\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.6625 - acc: 0.6884 - val_loss: 1.5784 - val_acc: 0.6990\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6516 - acc: 0.6882 - val_loss: 1.5802 - val_acc: 0.7020\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6588 - acc: 0.6883 - val_loss: 1.5720 - val_acc: 0.7000\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6453 - acc: 0.6880 - val_loss: 1.5679 - val_acc: 0.7010\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6419 - acc: 0.6908 - val_loss: 1.5765 - val_acc: 0.6970\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6448 - acc: 0.6860 - val_loss: 1.5788 - val_acc: 0.6920\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6339 - acc: 0.6903 - val_loss: 1.5685 - val_acc: 0.7000\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6310 - acc: 0.6909 - val_loss: 1.5619 - val_acc: 0.7070\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6404 - acc: 0.6873 - val_loss: 1.5626 - val_acc: 0.7010\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6328 - acc: 0.6929 - val_loss: 1.5631 - val_acc: 0.7010\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6324 - acc: 0.6904 - val_loss: 1.5547 - val_acc: 0.7090\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.6246 - acc: 0.6956 - val_loss: 1.5549 - val_acc: 0.7020\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6282 - acc: 0.6944 - val_loss: 1.5472 - val_acc: 0.7040\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6324 - acc: 0.6924 - val_loss: 1.5538 - val_acc: 0.7130\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6182 - acc: 0.6976 - val_loss: 1.5540 - val_acc: 0.7060\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6155 - acc: 0.6953 - val_loss: 1.5439 - val_acc: 0.7070\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6079 - acc: 0.6951 - val_loss: 1.5500 - val_acc: 0.7020\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.6102 - acc: 0.7010 - val_loss: 1.5403 - val_acc: 0.7080\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6166 - acc: 0.6958 - val_loss: 1.5396 - val_acc: 0.7060\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.6079 - acc: 0.6938 - val_loss: 1.5420 - val_acc: 0.7070\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.6159 - acc: 0.6967 - val_loss: 1.5418 - val_acc: 0.7000\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.6070 - acc: 0.6964 - val_loss: 1.5329 - val_acc: 0.7050\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.6052 - acc: 0.6993 - val_loss: 1.5222 - val_acc: 0.7150\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.6088 - acc: 0.6957 - val_loss: 1.5388 - val_acc: 0.7030\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6059 - acc: 0.6976 - val_loss: 1.5337 - val_acc: 0.7080\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.5983 - acc: 0.6989 - val_loss: 1.5316 - val_acc: 0.7100\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6108 - acc: 0.6964 - val_loss: 1.5321 - val_acc: 0.7130\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.5799 - acc: 0.7026 - val_loss: 1.5223 - val_acc: 0.7160\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.6051 - acc: 0.6964 - val_loss: 1.5354 - val_acc: 0.7070\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5888 - acc: 0.7037 - val_loss: 1.5246 - val_acc: 0.7140\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.5860 - acc: 0.7027 - val_loss: 1.5323 - val_acc: 0.7040\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5964 - acc: 0.6964 - val_loss: 1.5220 - val_acc: 0.7170\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5858 - acc: 0.7018 - val_loss: 1.5141 - val_acc: 0.7150\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.5859 - acc: 0.7007 - val_loss: 1.5170 - val_acc: 0.7170\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.5852 - acc: 0.7015 - val_loss: 1.5188 - val_acc: 0.7220\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.5760 - acc: 0.7023 - val_loss: 1.5210 - val_acc: 0.7070\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.5734 - acc: 0.7060 - val_loss: 1.5192 - val_acc: 0.7130\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5753 - acc: 0.7087 - val_loss: 1.5162 - val_acc: 0.7160\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5780 - acc: 0.7013 - val_loss: 1.5177 - val_acc: 0.7180\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5703 - acc: 0.7065 - val_loss: 1.5146 - val_acc: 0.7170\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.5723 - acc: 0.7022 - val_loss: 1.5194 - val_acc: 0.7230\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.5734 - acc: 0.7106 - val_loss: 1.5185 - val_acc: 0.7180\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.5654 - acc: 0.7058 - val_loss: 1.5244 - val_acc: 0.7160\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5716 - acc: 0.7037 - val_loss: 1.5124 - val_acc: 0.7230\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5684 - acc: 0.7058 - val_loss: 1.5112 - val_acc: 0.7190\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5691 - acc: 0.7041 - val_loss: 1.5122 - val_acc: 0.7190\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5598 - acc: 0.7050 - val_loss: 1.5183 - val_acc: 0.7090\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.5558 - acc: 0.7046 - val_loss: 1.5099 - val_acc: 0.7260\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5728 - acc: 0.7052 - val_loss: 1.5118 - val_acc: 0.7170\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.5651 - acc: 0.7042 - val_loss: 1.4954 - val_acc: 0.7230\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 14.3295 - acc: 0.3046 - val_loss: 8.6030 - val_acc: 0.5190\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 7.2637 - acc: 0.4758 - val_loss: 6.0485 - val_acc: 0.5340\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 5.6194 - acc: 0.5104 - val_loss: 4.8297 - val_acc: 0.5500\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 4.5694 - acc: 0.5277 - val_loss: 4.0070 - val_acc: 0.5610\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 3.8655 - acc: 0.5405 - val_loss: 3.4334 - val_acc: 0.5700\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 3.3656 - acc: 0.5509 - val_loss: 3.0145 - val_acc: 0.5690\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.9879 - acc: 0.5585 - val_loss: 2.7111 - val_acc: 0.5780\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 2.7163 - acc: 0.5596 - val_loss: 2.4846 - val_acc: 0.5740\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.5074 - acc: 0.5700 - val_loss: 2.3223 - val_acc: 0.5910\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 2.3731 - acc: 0.5689 - val_loss: 2.2052 - val_acc: 0.5790\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 2.2546 - acc: 0.5729 - val_loss: 2.1170 - val_acc: 0.5910\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 2.1788 - acc: 0.5713 - val_loss: 2.0534 - val_acc: 0.5870\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.1207 - acc: 0.5803 - val_loss: 2.0093 - val_acc: 0.5940\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.0786 - acc: 0.5787 - val_loss: 1.9765 - val_acc: 0.6030\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 2.0447 - acc: 0.5881 - val_loss: 1.9555 - val_acc: 0.5890\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.0325 - acc: 0.5857 - val_loss: 1.9315 - val_acc: 0.6090\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 2.0047 - acc: 0.5960 - val_loss: 1.9119 - val_acc: 0.6090\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.9917 - acc: 0.5972 - val_loss: 1.9033 - val_acc: 0.6220\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9824 - acc: 0.5946 - val_loss: 1.8894 - val_acc: 0.6240\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.9729 - acc: 0.6012 - val_loss: 1.8821 - val_acc: 0.6100\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 73us/step - loss: 1.9576 - acc: 0.6031 - val_loss: 1.8701 - val_acc: 0.6250\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.9586 - acc: 0.6016 - val_loss: 1.8626 - val_acc: 0.6390\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9453 - acc: 0.6107 - val_loss: 1.8568 - val_acc: 0.6260\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.9393 - acc: 0.6154 - val_loss: 1.8499 - val_acc: 0.6210\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.9336 - acc: 0.6100 - val_loss: 1.8413 - val_acc: 0.6420\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9272 - acc: 0.6171 - val_loss: 1.8364 - val_acc: 0.6440\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9199 - acc: 0.6198 - val_loss: 1.8317 - val_acc: 0.6460\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.9138 - acc: 0.6209 - val_loss: 1.8226 - val_acc: 0.6440\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9035 - acc: 0.6252 - val_loss: 1.8139 - val_acc: 0.6560\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9086 - acc: 0.6223 - val_loss: 1.8100 - val_acc: 0.6560\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9046 - acc: 0.6279 - val_loss: 1.8207 - val_acc: 0.6470\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.9057 - acc: 0.6235 - val_loss: 1.8049 - val_acc: 0.6600\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8933 - acc: 0.6263 - val_loss: 1.7982 - val_acc: 0.6600\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8918 - acc: 0.6250 - val_loss: 1.8008 - val_acc: 0.6600\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.8868 - acc: 0.6272 - val_loss: 1.7969 - val_acc: 0.6510\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 1.8793 - acc: 0.6294 - val_loss: 1.7910 - val_acc: 0.6710\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.8775 - acc: 0.6315 - val_loss: 1.7973 - val_acc: 0.6530\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.8814 - acc: 0.6324 - val_loss: 1.7824 - val_acc: 0.6650\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8673 - acc: 0.6409 - val_loss: 1.7855 - val_acc: 0.6490\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8633 - acc: 0.6399 - val_loss: 1.7792 - val_acc: 0.6710\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8610 - acc: 0.6394 - val_loss: 1.7700 - val_acc: 0.6590\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 72us/step - loss: 1.8671 - acc: 0.6349 - val_loss: 1.7700 - val_acc: 0.6760\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.8607 - acc: 0.6423 - val_loss: 1.7602 - val_acc: 0.6700\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.8585 - acc: 0.6437 - val_loss: 1.7625 - val_acc: 0.6650\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.8598 - acc: 0.6447 - val_loss: 1.7741 - val_acc: 0.6570\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.8610 - acc: 0.6403 - val_loss: 1.7575 - val_acc: 0.6750\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8465 - acc: 0.6452 - val_loss: 1.7494 - val_acc: 0.6800\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8519 - acc: 0.6409 - val_loss: 1.7538 - val_acc: 0.6720\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8364 - acc: 0.6447 - val_loss: 1.7501 - val_acc: 0.6730\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8403 - acc: 0.6477 - val_loss: 1.7416 - val_acc: 0.6770\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8440 - acc: 0.6468 - val_loss: 1.7515 - val_acc: 0.6740\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8424 - acc: 0.6427 - val_loss: 1.7388 - val_acc: 0.6770\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.8314 - acc: 0.6471 - val_loss: 1.7372 - val_acc: 0.6720\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8324 - acc: 0.6455 - val_loss: 1.7343 - val_acc: 0.6810\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8305 - acc: 0.6482 - val_loss: 1.7321 - val_acc: 0.6780\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8225 - acc: 0.6525 - val_loss: 1.7370 - val_acc: 0.6690\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.8299 - acc: 0.6482 - val_loss: 1.7289 - val_acc: 0.6790\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8243 - acc: 0.6542 - val_loss: 1.7281 - val_acc: 0.6770\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.8191 - acc: 0.6505 - val_loss: 1.7265 - val_acc: 0.6850\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.8173 - acc: 0.6521 - val_loss: 1.7257 - val_acc: 0.6790\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8073 - acc: 0.6506 - val_loss: 1.7129 - val_acc: 0.6850\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.8216 - acc: 0.6527 - val_loss: 1.7263 - val_acc: 0.6820\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8126 - acc: 0.6590 - val_loss: 1.7196 - val_acc: 0.6770\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8069 - acc: 0.6565 - val_loss: 1.7171 - val_acc: 0.6760\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.8108 - acc: 0.6538 - val_loss: 1.7152 - val_acc: 0.6840\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7926 - acc: 0.6574 - val_loss: 1.7042 - val_acc: 0.6830\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.8014 - acc: 0.6596 - val_loss: 1.7119 - val_acc: 0.6820\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.8036 - acc: 0.6580 - val_loss: 1.7226 - val_acc: 0.6850\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.8001 - acc: 0.6626 - val_loss: 1.7097 - val_acc: 0.6820\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7903 - acc: 0.6606 - val_loss: 1.7090 - val_acc: 0.6820\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7928 - acc: 0.6587 - val_loss: 1.7060 - val_acc: 0.6870\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7922 - acc: 0.6590 - val_loss: 1.6970 - val_acc: 0.6780\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.8006 - acc: 0.6601 - val_loss: 1.7101 - val_acc: 0.6820\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7955 - acc: 0.6636 - val_loss: 1.7064 - val_acc: 0.6780\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7886 - acc: 0.6592 - val_loss: 1.6972 - val_acc: 0.6910\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7959 - acc: 0.6622 - val_loss: 1.6990 - val_acc: 0.6820\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.7889 - acc: 0.6645 - val_loss: 1.6940 - val_acc: 0.6840\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7905 - acc: 0.6602 - val_loss: 1.7046 - val_acc: 0.6770\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.7933 - acc: 0.6625 - val_loss: 1.7016 - val_acc: 0.6800\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7773 - acc: 0.6660 - val_loss: 1.6981 - val_acc: 0.6850\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.7786 - acc: 0.6605 - val_loss: 1.6949 - val_acc: 0.6870\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.7807 - acc: 0.6637 - val_loss: 1.6826 - val_acc: 0.6880\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7758 - acc: 0.6636 - val_loss: 1.7047 - val_acc: 0.6750\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7839 - acc: 0.6639 - val_loss: 1.6932 - val_acc: 0.6890\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7924 - acc: 0.6626 - val_loss: 1.6833 - val_acc: 0.6870\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7787 - acc: 0.6644 - val_loss: 1.6865 - val_acc: 0.6870\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7783 - acc: 0.6642 - val_loss: 1.6859 - val_acc: 0.6850\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7753 - acc: 0.6607 - val_loss: 1.6770 - val_acc: 0.6840\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7720 - acc: 0.6666 - val_loss: 1.6856 - val_acc: 0.6920\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7739 - acc: 0.6660 - val_loss: 1.6912 - val_acc: 0.6910\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7760 - acc: 0.6685 - val_loss: 1.6776 - val_acc: 0.6860\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7589 - acc: 0.6671 - val_loss: 1.6824 - val_acc: 0.6830\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7623 - acc: 0.6668 - val_loss: 1.6817 - val_acc: 0.6870\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.7635 - acc: 0.6695 - val_loss: 1.6645 - val_acc: 0.6920\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.7747 - acc: 0.6651 - val_loss: 1.6731 - val_acc: 0.6890\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 1.7695 - acc: 0.6678 - val_loss: 1.6776 - val_acc: 0.6850\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7572 - acc: 0.6678 - val_loss: 1.6759 - val_acc: 0.6900\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.7610 - acc: 0.6671 - val_loss: 1.6770 - val_acc: 0.6840\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.7624 - acc: 0.6681 - val_loss: 1.6745 - val_acc: 0.6850\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.7535 - acc: 0.6642 - val_loss: 1.6672 - val_acc: 0.6890\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 3.4785 - acc: 0.2854 - val_loss: 2.6588 - val_acc: 0.4570\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 2.4818 - acc: 0.4545 - val_loss: 1.8563 - val_acc: 0.5660\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.9788 - acc: 0.5462 - val_loss: 1.6112 - val_acc: 0.6440\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.7318 - acc: 0.6039 - val_loss: 1.4750 - val_acc: 0.6880\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.5846 - acc: 0.6503 - val_loss: 1.3790 - val_acc: 0.7030\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4803 - acc: 0.6764 - val_loss: 1.3191 - val_acc: 0.7190\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4050 - acc: 0.6911 - val_loss: 1.2744 - val_acc: 0.7330\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3425 - acc: 0.7093 - val_loss: 1.2326 - val_acc: 0.7440\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2818 - acc: 0.7234 - val_loss: 1.2109 - val_acc: 0.7500\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.2482 - acc: 0.7325 - val_loss: 1.1799 - val_acc: 0.7690\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2122 - acc: 0.7410 - val_loss: 1.1607 - val_acc: 0.7730\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1560 - acc: 0.7577 - val_loss: 1.1445 - val_acc: 0.7770\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1210 - acc: 0.7640 - val_loss: 1.1340 - val_acc: 0.7800\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.0885 - acc: 0.7735 - val_loss: 1.1206 - val_acc: 0.7920\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.0727 - acc: 0.7706 - val_loss: 1.1134 - val_acc: 0.7930\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.0347 - acc: 0.7781 - val_loss: 1.1059 - val_acc: 0.7990\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.0208 - acc: 0.7858 - val_loss: 1.0955 - val_acc: 0.7970\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.9812 - acc: 0.7908 - val_loss: 1.0898 - val_acc: 0.8020\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9685 - acc: 0.7993 - val_loss: 1.0948 - val_acc: 0.8080\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9659 - acc: 0.7987 - val_loss: 1.0863 - val_acc: 0.8040\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9374 - acc: 0.8092 - val_loss: 1.0850 - val_acc: 0.8100\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9303 - acc: 0.8087 - val_loss: 1.0750 - val_acc: 0.8130\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9125 - acc: 0.8126 - val_loss: 1.0841 - val_acc: 0.8070\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.8938 - acc: 0.8162 - val_loss: 1.0708 - val_acc: 0.8180\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8748 - acc: 0.8220 - val_loss: 1.0806 - val_acc: 0.8090\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.8833 - acc: 0.8197 - val_loss: 1.0795 - val_acc: 0.8180\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8627 - acc: 0.8225 - val_loss: 1.0887 - val_acc: 0.8110\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.8757 - acc: 0.8217 - val_loss: 1.0698 - val_acc: 0.8160\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.8425 - acc: 0.8305 - val_loss: 1.0599 - val_acc: 0.8140\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8331 - acc: 0.8316 - val_loss: 1.0739 - val_acc: 0.8140\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.8338 - acc: 0.8349 - val_loss: 1.0706 - val_acc: 0.8190\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.8203 - acc: 0.8344 - val_loss: 1.0660 - val_acc: 0.8220\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.8262 - acc: 0.8374 - val_loss: 1.0671 - val_acc: 0.8210\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.8083 - acc: 0.8409 - val_loss: 1.0707 - val_acc: 0.8200\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8149 - acc: 0.8371 - val_loss: 1.0679 - val_acc: 0.8240\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.8137 - acc: 0.8404 - val_loss: 1.0606 - val_acc: 0.8220\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.7959 - acc: 0.8434 - val_loss: 1.0588 - val_acc: 0.8150\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8027 - acc: 0.8413 - val_loss: 1.0599 - val_acc: 0.8190\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7946 - acc: 0.8490 - val_loss: 1.0567 - val_acc: 0.8170\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7693 - acc: 0.8504 - val_loss: 1.0634 - val_acc: 0.8200\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7942 - acc: 0.8410 - val_loss: 1.0695 - val_acc: 0.8180\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7736 - acc: 0.8500 - val_loss: 1.0579 - val_acc: 0.8240\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7628 - acc: 0.8525 - val_loss: 1.0678 - val_acc: 0.8190\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7601 - acc: 0.8555 - val_loss: 1.0710 - val_acc: 0.8230\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7688 - acc: 0.8539 - val_loss: 1.0645 - val_acc: 0.8230\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7531 - acc: 0.8533 - val_loss: 1.0716 - val_acc: 0.8210\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7448 - acc: 0.8557 - val_loss: 1.0656 - val_acc: 0.8230\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7375 - acc: 0.8587 - val_loss: 1.0743 - val_acc: 0.8230\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7279 - acc: 0.8622 - val_loss: 1.0754 - val_acc: 0.8190\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7416 - acc: 0.8617 - val_loss: 1.0642 - val_acc: 0.8210\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.7489 - acc: 0.8549 - val_loss: 1.0835 - val_acc: 0.8180\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7443 - acc: 0.8591 - val_loss: 1.0733 - val_acc: 0.8160\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7152 - acc: 0.8638 - val_loss: 1.0801 - val_acc: 0.8230\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7194 - acc: 0.8657 - val_loss: 1.0813 - val_acc: 0.8190\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7127 - acc: 0.8626 - val_loss: 1.0819 - val_acc: 0.8190\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7169 - acc: 0.8647 - val_loss: 1.0805 - val_acc: 0.8140\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7241 - acc: 0.8643 - val_loss: 1.0895 - val_acc: 0.8180\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7191 - acc: 0.8673 - val_loss: 1.0817 - val_acc: 0.8220\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7159 - acc: 0.8647 - val_loss: 1.0774 - val_acc: 0.8250\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7164 - acc: 0.8678 - val_loss: 1.0812 - val_acc: 0.8220\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7277 - acc: 0.8629 - val_loss: 1.0823 - val_acc: 0.8280\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6974 - acc: 0.8720 - val_loss: 1.0748 - val_acc: 0.8230\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7047 - acc: 0.8705 - val_loss: 1.0784 - val_acc: 0.8270\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6978 - acc: 0.8730 - val_loss: 1.0973 - val_acc: 0.8200\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7091 - acc: 0.8667 - val_loss: 1.0843 - val_acc: 0.8220\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6998 - acc: 0.8696 - val_loss: 1.0818 - val_acc: 0.8240\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6984 - acc: 0.8703 - val_loss: 1.0802 - val_acc: 0.8270\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7031 - acc: 0.8750 - val_loss: 1.0824 - val_acc: 0.8210\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6811 - acc: 0.8748 - val_loss: 1.0922 - val_acc: 0.8270\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6863 - acc: 0.8780 - val_loss: 1.0801 - val_acc: 0.8260\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6900 - acc: 0.8741 - val_loss: 1.0818 - val_acc: 0.8290\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6889 - acc: 0.8767 - val_loss: 1.0802 - val_acc: 0.8240\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6951 - acc: 0.8726 - val_loss: 1.0793 - val_acc: 0.8250\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6762 - acc: 0.8799 - val_loss: 1.0798 - val_acc: 0.8260\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6910 - acc: 0.8775 - val_loss: 1.0917 - val_acc: 0.8240\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6909 - acc: 0.8747 - val_loss: 1.0846 - val_acc: 0.8280\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6764 - acc: 0.8771 - val_loss: 1.0887 - val_acc: 0.8270\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6812 - acc: 0.8768 - val_loss: 1.0972 - val_acc: 0.8250\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6682 - acc: 0.8797 - val_loss: 1.0851 - val_acc: 0.8280\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6842 - acc: 0.8799 - val_loss: 1.0764 - val_acc: 0.8340\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6739 - acc: 0.8805 - val_loss: 1.0920 - val_acc: 0.8310\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6662 - acc: 0.8820 - val_loss: 1.0933 - val_acc: 0.8250\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.6678 - acc: 0.8830 - val_loss: 1.0872 - val_acc: 0.8330\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6796 - acc: 0.8767 - val_loss: 1.0908 - val_acc: 0.8270\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6695 - acc: 0.8801 - val_loss: 1.0769 - val_acc: 0.8310\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6573 - acc: 0.8825 - val_loss: 1.0845 - val_acc: 0.8390\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6766 - acc: 0.8751 - val_loss: 1.0753 - val_acc: 0.8340\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6589 - acc: 0.8795 - val_loss: 1.0782 - val_acc: 0.8310\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.6673 - acc: 0.8841 - val_loss: 1.0879 - val_acc: 0.8320\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.6734 - acc: 0.8789 - val_loss: 1.0666 - val_acc: 0.8350\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6570 - acc: 0.8865 - val_loss: 1.0773 - val_acc: 0.8360\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6648 - acc: 0.8814 - val_loss: 1.0804 - val_acc: 0.8320\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6581 - acc: 0.8819 - val_loss: 1.0829 - val_acc: 0.8260\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6642 - acc: 0.8804 - val_loss: 1.0893 - val_acc: 0.8260\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6586 - acc: 0.8847 - val_loss: 1.1025 - val_acc: 0.8250\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6497 - acc: 0.8816 - val_loss: 1.1014 - val_acc: 0.8250\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6557 - acc: 0.8846 - val_loss: 1.0908 - val_acc: 0.8230\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6462 - acc: 0.8825 - val_loss: 1.0898 - val_acc: 0.8270\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.6594 - acc: 0.8844 - val_loss: 1.0720 - val_acc: 0.8260\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6417 - acc: 0.8885 - val_loss: 1.0968 - val_acc: 0.8280\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 3.5306 - acc: 0.2632 - val_loss: 2.9105 - val_acc: 0.4710\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 2.5025 - acc: 0.4897 - val_loss: 1.8527 - val_acc: 0.5700\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.8360 - acc: 0.5827 - val_loss: 1.5124 - val_acc: 0.6560\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.5537 - acc: 0.6451 - val_loss: 1.3473 - val_acc: 0.6940\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.4000 - acc: 0.6800 - val_loss: 1.2651 - val_acc: 0.7070\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.2813 - acc: 0.7051 - val_loss: 1.2007 - val_acc: 0.7160\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1936 - acc: 0.7190 - val_loss: 1.1471 - val_acc: 0.7330\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1085 - acc: 0.7325 - val_loss: 1.1088 - val_acc: 0.7470\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0443 - acc: 0.7444 - val_loss: 1.0751 - val_acc: 0.7670\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9789 - acc: 0.7679 - val_loss: 1.0481 - val_acc: 0.7730\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9065 - acc: 0.7784 - val_loss: 1.0221 - val_acc: 0.7820\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8602 - acc: 0.7939 - val_loss: 0.9933 - val_acc: 0.8020\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.8102 - acc: 0.8076 - val_loss: 0.9850 - val_acc: 0.8040\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7646 - acc: 0.8172 - val_loss: 0.9659 - val_acc: 0.8020\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7342 - acc: 0.8196 - val_loss: 0.9468 - val_acc: 0.8110\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7072 - acc: 0.8264 - val_loss: 0.9412 - val_acc: 0.8140\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6598 - acc: 0.8381 - val_loss: 0.9365 - val_acc: 0.8130\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6386 - acc: 0.8419 - val_loss: 0.9338 - val_acc: 0.8190\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.6031 - acc: 0.8507 - val_loss: 0.9290 - val_acc: 0.8250\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.5914 - acc: 0.8495 - val_loss: 0.9226 - val_acc: 0.8240\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.5723 - acc: 0.8584 - val_loss: 0.9244 - val_acc: 0.8250\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.5491 - acc: 0.8633 - val_loss: 0.9263 - val_acc: 0.8200\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.5217 - acc: 0.8661 - val_loss: 0.9283 - val_acc: 0.8190\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4976 - acc: 0.8737 - val_loss: 0.9411 - val_acc: 0.8210\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.5008 - acc: 0.8682 - val_loss: 0.9275 - val_acc: 0.8220\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4801 - acc: 0.8763 - val_loss: 0.9457 - val_acc: 0.8240\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4648 - acc: 0.8786 - val_loss: 0.9542 - val_acc: 0.8240\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4433 - acc: 0.8908 - val_loss: 0.9601 - val_acc: 0.8220\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4471 - acc: 0.8859 - val_loss: 0.9443 - val_acc: 0.8240\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4311 - acc: 0.8871 - val_loss: 0.9592 - val_acc: 0.8240\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4190 - acc: 0.8911 - val_loss: 0.9587 - val_acc: 0.8280\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.4172 - acc: 0.8926 - val_loss: 0.9540 - val_acc: 0.8250\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3939 - acc: 0.9004 - val_loss: 0.9666 - val_acc: 0.8270\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.3831 - acc: 0.9020 - val_loss: 0.9825 - val_acc: 0.8260\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.3878 - acc: 0.9008 - val_loss: 0.9820 - val_acc: 0.8260\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.3672 - acc: 0.9044 - val_loss: 0.9841 - val_acc: 0.8260\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.3624 - acc: 0.9065 - val_loss: 0.9928 - val_acc: 0.8240\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.3644 - acc: 0.9060 - val_loss: 1.0018 - val_acc: 0.8260\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3617 - acc: 0.9064 - val_loss: 0.9916 - val_acc: 0.8240\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.3542 - acc: 0.9090 - val_loss: 1.0010 - val_acc: 0.8280\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.3347 - acc: 0.9164 - val_loss: 1.0151 - val_acc: 0.8220\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3441 - acc: 0.9129 - val_loss: 1.0222 - val_acc: 0.8250\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.3277 - acc: 0.9116 - val_loss: 1.0388 - val_acc: 0.8260\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.3299 - acc: 0.9100 - val_loss: 1.0310 - val_acc: 0.8290\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3258 - acc: 0.9167 - val_loss: 1.0294 - val_acc: 0.8280\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.3099 - acc: 0.9162 - val_loss: 1.0456 - val_acc: 0.8240\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3120 - acc: 0.9182 - val_loss: 1.0486 - val_acc: 0.8260\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.3128 - acc: 0.9159 - val_loss: 1.0515 - val_acc: 0.8270\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3059 - acc: 0.9171 - val_loss: 1.0600 - val_acc: 0.8250\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2959 - acc: 0.9226 - val_loss: 1.0515 - val_acc: 0.8320\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2931 - acc: 0.9218 - val_loss: 1.0695 - val_acc: 0.8320\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2998 - acc: 0.9217 - val_loss: 1.0720 - val_acc: 0.8280\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2905 - acc: 0.9241 - val_loss: 1.0702 - val_acc: 0.8270\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2980 - acc: 0.9208 - val_loss: 1.0592 - val_acc: 0.8280\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2824 - acc: 0.9267 - val_loss: 1.0807 - val_acc: 0.8240\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2848 - acc: 0.9241 - val_loss: 1.0911 - val_acc: 0.8240\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2791 - acc: 0.9257 - val_loss: 1.0855 - val_acc: 0.8230\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2763 - acc: 0.9262 - val_loss: 1.0979 - val_acc: 0.8230\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.2771 - acc: 0.9268 - val_loss: 1.0996 - val_acc: 0.8200\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2687 - acc: 0.9313 - val_loss: 1.0901 - val_acc: 0.8210\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2678 - acc: 0.9306 - val_loss: 1.0934 - val_acc: 0.8250\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.2727 - acc: 0.9271 - val_loss: 1.0860 - val_acc: 0.8280\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2651 - acc: 0.9273 - val_loss: 1.0925 - val_acc: 0.8240\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2594 - acc: 0.9313 - val_loss: 1.0994 - val_acc: 0.8280\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2724 - acc: 0.9305 - val_loss: 1.0928 - val_acc: 0.8280\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2504 - acc: 0.9377 - val_loss: 1.0989 - val_acc: 0.8280\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2621 - acc: 0.9301 - val_loss: 1.1219 - val_acc: 0.8260\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2505 - acc: 0.9334 - val_loss: 1.1271 - val_acc: 0.8220\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2538 - acc: 0.9313 - val_loss: 1.1256 - val_acc: 0.8210\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2492 - acc: 0.9351 - val_loss: 1.1203 - val_acc: 0.8270\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2510 - acc: 0.9330 - val_loss: 1.1293 - val_acc: 0.8260\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2546 - acc: 0.9317 - val_loss: 1.1322 - val_acc: 0.8260\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2584 - acc: 0.9281 - val_loss: 1.1400 - val_acc: 0.8200\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.2494 - acc: 0.9336 - val_loss: 1.1385 - val_acc: 0.8230\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2472 - acc: 0.9340 - val_loss: 1.1542 - val_acc: 0.8250\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2494 - acc: 0.9326 - val_loss: 1.1493 - val_acc: 0.8210\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2328 - acc: 0.9394 - val_loss: 1.1673 - val_acc: 0.8220\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2489 - acc: 0.9339 - val_loss: 1.1705 - val_acc: 0.8210\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2410 - acc: 0.9344 - val_loss: 1.1560 - val_acc: 0.8190\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2409 - acc: 0.9366 - val_loss: 1.1558 - val_acc: 0.8220\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2436 - acc: 0.9351 - val_loss: 1.1889 - val_acc: 0.8160\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2492 - acc: 0.9303 - val_loss: 1.1713 - val_acc: 0.8170\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2344 - acc: 0.9382 - val_loss: 1.1753 - val_acc: 0.8200\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2395 - acc: 0.9359 - val_loss: 1.1676 - val_acc: 0.8190\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2259 - acc: 0.9399 - val_loss: 1.1819 - val_acc: 0.8200\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2416 - acc: 0.9365 - val_loss: 1.1763 - val_acc: 0.8230\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2351 - acc: 0.9362 - val_loss: 1.1678 - val_acc: 0.8160\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.2359 - acc: 0.9395 - val_loss: 1.1822 - val_acc: 0.8180\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.2319 - acc: 0.9386 - val_loss: 1.1686 - val_acc: 0.8140\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2393 - acc: 0.9344 - val_loss: 1.1613 - val_acc: 0.8190\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2377 - acc: 0.9357 - val_loss: 1.1849 - val_acc: 0.8180\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2302 - acc: 0.9414 - val_loss: 1.1862 - val_acc: 0.8180\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2197 - acc: 0.9397 - val_loss: 1.1997 - val_acc: 0.8170\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2326 - acc: 0.9354 - val_loss: 1.1910 - val_acc: 0.8190\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2283 - acc: 0.9395 - val_loss: 1.1846 - val_acc: 0.8230\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2239 - acc: 0.9397 - val_loss: 1.2018 - val_acc: 0.8250\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2173 - acc: 0.9422 - val_loss: 1.2283 - val_acc: 0.8170\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2371 - acc: 0.9342 - val_loss: 1.2099 - val_acc: 0.8130\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2186 - acc: 0.9397 - val_loss: 1.1990 - val_acc: 0.8110\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2288 - acc: 0.9386 - val_loss: 1.2041 - val_acc: 0.8190\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTV0LaGxjyvN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "3b0b8530-a2ea-4a21-de48-6cfa68e37eec"
      },
      "source": [
        "plt.scatter(dropout_rates[0:8], val_acc[0:8])\n",
        "plt.show()\n",
        "plt.scatter(l2_penalties[0:8], val_acc[0:8])\n",
        "plt.show()\n",
        "plt.scatter(num_units[0:8], val_acc[0:8])\n",
        "plt.show()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAXiklEQVR4nO3df4xdZZ3H8ffHgZZZI7S1YwL9QUss\nhYIbulyrLll3FaGVrLSJxJ2uLuASG1fBBE1DG4maiolu45J1UzUlCxWiVLYLdbLCjriAu2tAe2sr\npTVTh6IyU7KO4MiujtCW7/5xz8jp7Z3eczv3zp2Z5/NKbnruc57z3O857ZxPz487RxGBmZml5zXt\nLsDMzNrDAWBmligHgJlZohwAZmaJcgCYmSXKAWBmlqhCASBplaQ+Sf2SNtSYv1DSo5L2SHpS0lVZ\n+yJJI5L2Zq+v5pa5VNK+bMwvSVLzVsvMzOpRve8BSOoADgJXAAPALmBtRBzI9dkK7ImIr0haBjwY\nEYskLQL+LSIurjHuD4GPAT8AHgS+FBEPNWWtzMysriJHACuA/og4FBEvA9uB1VV9Ajgzmz4LOHyy\nASWdDZwZEU9EJYHuBtY0VLmZmY3LaQX6zAOezb0fAN5S1eczwHck3QS8FnhXbt5iSXuAF4FbI+K/\nsjEHqsacV6+QuXPnxqJFiwqUbGZmo3bv3v2riOiqbi8SAEWsBbZFxBclvQ24R9LFwHPAwoh4XtKl\nwE5JFzUysKR1wDqAhQsXUi6Xm1SymVkaJP28VnuRU0CDwILc+/lZW94NwH0AEfE4cAYwNyJeiojn\ns/bdwNPA+dny8+uMSbbc1ogoRUSpq+uEADMzs1NUJAB2AUskLZY0A+gGeqr6/AK4HEDShVQCYEhS\nV3YRGUnnAUuAQxHxHPCipLdmd/9cC3yrKWtkZmaF1D0FFBFHJd0I9AIdwJ0RsV/SJqAcET3AJ4A7\nJN1M5YLw9RERkt4ObJJ0BHgF+HBEvJAN/RFgG9AJPJS9zMxsgtS9DXQyKZVK4WsAZmaNkbQ7IkrV\n7f4msJlZohwAZmaJcgCYmSXKAWBmligHgJlZohwAZmaJcgCYmSXKAWBmligHgJlZohwAZmaJcgCY\nmSXKAWBmligHgJlZohwAZmaJcgCYmSXKAWBmligHgJlZohwAZmaJqvtMYDMza62dewbZ3NvH4eER\nzpnVyfqVS1mzfF7LP9cBYGbWRjv3DLLx/n2MHDkGwODwCBvv3wfQ8hDwKSAzszba3Nv3h53/qJEj\nx9jc29fyzy4UAJJWSeqT1C9pQ435CyU9KmmPpCclXZW1XyFpt6R92Z/vzC3zWDbm3uz1huatlpnZ\n1HB4eKSh9maqewpIUgewBbgCGAB2SeqJiAO5brcC90XEVyQtAx4EFgG/At4TEYclXQz0AvljmvdH\nRLk5q2JmNvWcM6uTwRo7+3Nmdbb8s4scAawA+iPiUES8DGwHVlf1CeDMbPos4DBAROyJiMNZ+36g\nU9LM8ZdtZjY9rF+5lM7TO45r6zy9g/Url7b8s4tcBJ4HPJt7PwC8parPZ4DvSLoJeC3wrhrjvBf4\nUUS8lGu7S9Ix4F+B2yIiqheStA5YB7Bw4cIC5ZqZTR2jF3qn8l1Aa4FtEfFFSW8D7pF0cUS8AiDp\nIuALwJW5Zd4fEYOSXkclAP4GuLt64IjYCmwFKJVKJwTEeLTr1iszs7w1y+e1Zd9T5BTQILAg935+\n1pZ3A3AfQEQ8DpwBzAWQNB94ALg2Ip4eXSAiBrM//xf4BpVTTRNm9NarweERgldvvdq5p3rVzMym\npyIBsAtYImmxpBlAN9BT1ecXwOUAki6kEgBDkmYB3wY2RMT3RztLOk3SaECcDvwl8NR4V6YR7bz1\nysxsMqh7Cigijkq6kcodPB3AnRGxX9ImoBwRPcAngDsk3UzlgvD1ERHZcm8EPiXpU9mQVwK/BXqz\nnX8H8F3gjmav3Mm089Yrs8mo0VOi0/kU6nRet7xC1wAi4kEqt3bm2z6Vmz4AXFZjuduA28YY9tLi\nZTZfO2+9MptsGv02aju/vdpq03ndqiX7TeB23nplNtk0ekp0Op9Cnc7rVi3Z3wXUzluvppJUDoVT\n1+gp0el8CnU6r1u1ZAMA2nfr1VSR0qFw6ho9JTqdT6FO53WrluwpIKsvpUPh1DV6SnQ6n0KdzutW\nLekjADu5lA6FU9foKdHpfAp1Oq9bNdX47QuTVqlUinLZvztuolz2+UdqHgrPm9XJ9ze8s8YSZjYZ\nSdodEaXqdp8CsjGldChsliKfArIxpXQobJYiB4CdlO+UMpu+fArIzCxRDgAzs0Q5AMzMEuUAMDNL\nlAPAzCxRDgAzs0Q5AMzMEuUAMDNLlAPAzCxRDgAzs0Q5AMzMEuUAMDNLVKEAkLRKUp+kfkkbasxf\nKOlRSXskPSnpqty8jdlyfZJWFh3TzMxaq24ASOoAtgDvBpYBayUtq+p2K3BfRCwHuoEvZ8suy95f\nBKwCviypo+CYZmbWQkWOAFYA/RFxKCJeBrYDq6v6BHBmNn0WcDibXg1sj4iXIuIZoD8br8iYZmbW\nQkUCYB7wbO79QNaW9xngA5IGgAeBm+osW2RMACStk1SWVB4aGipQrpmZFdGsi8BrgW0RMR+4CrhH\nUlPGjoitEVGKiFJXV1czhjQzM4o9EWwQWJB7Pz9ry7uByjl+IuJxSWcAc+ssW29MMzNroSL/S98F\nLJG0WNIMKhd1e6r6/AK4HEDShcAZwFDWr1vSTEmLgSXADwuOaWZmLVT3CCAijkq6EegFOoA7I2K/\npE1AOSJ6gE8Ad0i6mcoF4esjIoD9ku4DDgBHgY9GxDGAWmO2YP3MzGwMquynp4ZSqRTlcrndZZiZ\nTSmSdkdEqbrd3wQ2M0uUA8DMLFEOADOzRDkAzMwS5QAwM0uUA8DMLFEOADOzRDkAzMwS5QAwM0uU\nA8DMLFEOADOzRDkAzMwS5QAwM0uUA8DMLFEOADOzRDkAzMwS5QAwM0uUA8DMLFEOADOzRDkAzMwS\nVSgAJK2S1CepX9KGGvNvl7Q3ex2UNJy1vyPXvlfS7yWtyeZtk/RMbt4lzV01MzM7mdPqdZDUAWwB\nrgAGgF2SeiLiwGifiLg51/8mYHnW/ihwSdY+B+gHvpMbfn1E7GjCepiZWYOKHAGsAPoj4lBEvAxs\nB1afpP9a4N4a7dcAD0XE7xov08zMmq1IAMwDns29H8jaTiDpXGAx8EiN2d2cGAyfk/RkdgppZoFa\nzMysSZp9Ebgb2BERx/KNks4G3gT05po3AhcAbwbmALfUGlDSOkllSeWhoaEml2tmlq4iATAILMi9\nn5+11VLrf/kA7wMeiIgjow0R8VxUvATcReVU0wkiYmtElCKi1NXVVaBcMzMrokgA7AKWSFosaQaV\nnXxPdSdJFwCzgcdrjHHCdYHsqABJAtYATzVWupmZjUfdu4Ai4qikG6mcvukA7oyI/ZI2AeWIGA2D\nbmB7RER+eUmLqBxBfK9q6K9L6gIE7AU+PJ4VMTOzxqhqfz2plUqlKJfL7S7DzGxKkbQ7IkrV7f4m\nsJlZohwAZmaJcgCYmSXKAWBmligHgJlZohwAZmaJcgCYmSXKAWBmligHgJlZohwAZmaJcgCYmSXK\nAWBmligHgJlZohwAZmaJcgCYmSXKAWBmligHgJlZohwAZmaJcgCYmSXKAWBmligHgJlZogoFgKRV\nkvok9UvaUGP+7ZL2Zq+DkoZz847l5vXk2hdL+kE25jclzWjOKpmZWRF1A0BSB7AFeDewDFgraVm+\nT0TcHBGXRMQlwD8B9+dmj4zOi4irc+1fAG6PiDcCvwZuGOe6mJlZA4ocAawA+iPiUES8DGwHVp+k\n/1rg3pMNKEnAO4EdWdPXgDUFajEzsyYpEgDzgGdz7weythNIOhdYDDySaz5DUlnSE5JGd/KvB4Yj\n4miBMddly5eHhoYKlGtmZkWc1uTxuoEdEXEs13ZuRAxKOg94RNI+4DdFB4yIrcBWgFKpFE2t1sws\nYUWOAAaBBbn387O2WrqpOv0TEYPZn4eAx4DlwPPALEmjAXSyMc3MrAWKBMAuYEl2184MKjv5nupO\nki4AZgOP59pmS5qZTc8FLgMOREQAjwLXZF2vA741nhUxM7PG1A2A7Dz9jUAv8BPgvojYL2mTpPxd\nPd3A9mznPupCoCzpx1R2+J+PiAPZvFuAj0vqp3JN4J/HvzpmZlaUjt9fT26lUinK5XK7yzAzm1Ik\n7Y6IUnW7vwlsZpYoB4CZWaIcAGZmiXIAmJklygFgZpYoB4CZWaIcAGZmiXIAmJklygFgZpYoB4CZ\nWaIcAGZmiXIAmJklygFgZpYoB4CZWaIcAGZmiXIAmJklygFgZpYoB4CZWaIcAGZmiXIAmJklygFg\nZpaoQgEgaZWkPkn9kjbUmH+7pL3Z66Ck4az9EkmPS9ov6UlJf5VbZpukZ3LLXdK81TIzs3pOq9dB\nUgewBbgCGAB2SeqJiAOjfSLi5lz/m4Dl2dvfAddGxE8lnQPsltQbEcPZ/PURsaNJ62JmZg2oGwDA\nCqA/Ig4BSNoOrAYOjNF/LfBpgIg4ONoYEYcl/RLoAobHWNbMprCdewbZ3NvH4eERzpnVyfqVS1mz\nfF67y7IxFDkFNA94Nvd+IGs7gaRzgcXAIzXmrQBmAE/nmj+XnRq6XdLMMcZcJ6ksqTw0NFSgXDNr\nh517Btl4/z4Gh0cIYHB4hI3372PnnsF2l2ZjaPZF4G5gR0QcyzdKOhu4B/hgRLySNW8ELgDeDMwB\nbqk1YERsjYhSRJS6urqaXK6ZNcvm3j5Gjhz3o8/IkWNs7u1rU0VWT5EAGAQW5N7Pz9pq6QbuzTdI\nOhP4NvDJiHhitD0inouKl4C7qJxqMrMp6vDwSEPt1n5FAmAXsETSYkkzqOzke6o7SboAmA08nmub\nATwA3F19sTc7KkCSgDXAU6e6EmbWfufM6myo3dqvbgBExFHgRqAX+AlwX0Tsl7RJ0tW5rt3A9oiI\nXNv7gLcD19e43fPrkvYB+4C5wG1NWB8za5P1K5fSeXrHcW2dp3ewfuXSNlVk9ej4/fXkViqVolwu\nt7sMMxuD7wKanCTtjohSdXuR20DNzApZs3yed/hTiH8VhJlZohwAZmaJcgCYmSXKAWBmligHgJlZ\nohwAZmaJcgCYmSXKAWBmligHgJlZohwAZmaJcgCYmSXKAWBmligHgJlZohwAZmaJcgCYmSXKAWBm\nligHgJlZohwAZmaJcgCYmSWqUABIWiWpT1K/pA015t8uaW/2OihpODfvOkk/zV7X5dovlbQvG/NL\nktScVTIzsyLqPhReUgewBbgCGAB2SeqJiAOjfSLi5lz/m4Dl2fQc4NNACQhgd7bsr4GvAB8CfgA8\nCKwCHmrSepmZWR1FjgBWAP0RcSgiXga2A6tP0n8tcG82vRJ4OCJeyHb6DwOrJJ0NnBkRT0REAHcD\na055LczMrGFFAmAe8Gzu/UDWdgJJ5wKLgUfqLDsvmy4y5jpJZUnloaGhAuWamVkRzb4I3A3siIhj\nzRowIrZGRCkiSl1dXc0a1swseUUCYBBYkHs/P2urpZtXT/+cbNnBbLrImGZm1gJFAmAXsETSYkkz\nqOzke6o7SboAmA08nmvuBa6UNFvSbOBKoDcingNelPTW7O6fa4FvjXNdzMysAXXvAoqIo5JupLIz\n7wDujIj9kjYB5YgYDYNuYHt2UXd02RckfZZKiABsiogXsumPANuATip3//gOIDOzCaTc/nrSK5VK\nUS6X212GmdmUIml3RJSq2/1NYDOzRDkAzMwS5QAwM0uUA8DMLFEOADOzRNW9DdTM7GR27hlkc28f\nh4dHOGdWJ+tXLmXN8pq/2cUmGQeAmZ2ynXsG2Xj/PkaOVH77y+DwCBvv3wfgEJgCfArIzE7Z5t6+\nP+z8R40cOcbm3r42VWSNcACY2Sk7PDzSULtNLg4AMztl58zqbKjdJhcHgJmdsvUrl9J5esdxbZ2n\nd7B+5dI2VWSN8EVgMztloxd6fRfQ1OQAMLNxWbN8nnf4U5RPAZmZJcoBYGaWKAeAmVmiHABmZoly\nAJiZJcoBYGaWKAeAmVmiHABmZokqFACSVknqk9QvacMYfd4n6YCk/ZK+kbW9Q9Le3Ov3ktZk87ZJ\neiY375LmrZaZmdVT95vAkjqALcAVwACwS1JPRBzI9VkCbAQui4hfS3oDQEQ8ClyS9ZkD9APfyQ2/\nPiJ2NGtlzMysuCJHACuA/og4FBEvA9uB1VV9PgRsiYhfA0TEL2uMcw3wUET8bjwFm5lZcxQJgHnA\ns7n3A1lb3vnA+ZK+L+kJSatqjNMN3FvV9jlJT0q6XdLMWh8uaZ2ksqTy0NBQgXLNzKyIZl0EPg1Y\nAvwFsBa4Q9Ks0ZmSzgbeBPTmltkIXAC8GZgD3FJr4IjYGhGliCh1dXU1qVwzMysSAIPAgtz7+Vlb\n3gDQExFHIuIZ4CCVQBj1PuCBiDgy2hARz0XFS8BdVE41mZnZBCkSALuAJZIWS5pB5VROT1WfnVT+\n94+kuVROCR3KzV9L1emf7KgASQLWAE+dQv1mZnaK6t4FFBFHJd1I5fRNB3BnROyXtAkoR0RPNu9K\nSQeAY1Tu7nkeQNIiKkcQ36sa+uuSugABe4EPN2eVzMysCEVEu2sorFQqRblcbncZZmZTiqTdEVGq\nbvc3gc3MEuUAMDNLlAPAzCxRDgAzs0RNqYvAkoaAn9eYNRf41QSXU49rKm4y1uWaipuMdbmm450b\nESd8k3ZKBcBYJJVrXeFuJ9dU3GSsyzUVNxnrck3F+BSQmVmiHABmZomaLgGwtd0F1OCaipuMdbmm\n4iZjXa6pgGlxDcDMzBo3XY4AzMysQZM6AOo9i1jSTEnfzOb/IPvFc0haJGkk97zhr05gTW+X9CNJ\nRyVdUzXvOkk/zV7XNaumJtR1LLetqn/Taytr+nj2HOknJf2HpHNz81qyrcZZU0u2U8G6PixpX/bZ\n/y1pWW7exmy5Pkkr211TK3/+itSV6/deSSGplGtry7Yaq6ZWb6u6ImJSvqj85tGngfOAGcCPgWVV\nfT4CfDWb7ga+mU0vAp5qU02LgD8G7gauybXPofIrsucAs7Pp2e2uK5v3f23aVu8A/iib/rvc319L\nttV4amrVdmqgrjNz01cD/55NL8v6zwQWZ+N0tLmmlvz8Fa0r6/c64D+BJ4BSu7fVSWpq2bYq8prM\nRwBFnkW8GvhaNr0DuFyS2llTRPwsIp4EXqladiXwcES8EJVnJz8M1Hp05kTX1SpFano0Xn1G9BNU\nHjYErdtW46mplYrU9WLu7WuB0Yt3q4HtEfFSVB7G1E9zHq40nppaqch+AeCzwBeA3+fa2ratTlJT\nW03mACjyLOI/9ImIo8BvgNdn8xZL2iPpe5L+bAJrasWyrR77DFWeu/yEpDVtqukG4KFTXHYiaoLW\nbKfCdUn6qKSngb8HPtbIshNcE7Tm569QXZL+BFgQEd9udNk21ASt21Z11X0gzBT1HLAwIp6XdCmw\nU9JFVf9jsVedGxGDks4DHpG0LyKenqgPl/QBoAT8+UR9Zj1j1NTW7RQRW4Atkv4auBVo6nWkUzFG\nTW37+ZP0GuAfgOtb/VlF1amprfuqyXwEUORZxH/oI+k04Czg+ewQ73mAiNhN5fzc+RNUUyuWbenY\nETGY/XkIeAxYPlE1SXoX8Eng6qg8H7rwshNcU6u2U+G6crZTeYzqqSzb8ppa+PNXpK7XARcDj0n6\nGfBWoCe76NqubTVmTS3eVvW16+JDvReVo5NDVC7WjF5Yuaiqz0c5/iLwfdl0F9nFHSoXZgaBORNR\nU67vNk68CPwMlYuas7PpcdfUhLpmAzOz6bnAT6lxAatFf3/LqfyDX1LV3pJtNc6aWrKdGqhrSW76\nPVQexwpwEcdf2DxEcy5sjqemlvz8NfpvPev/GK9ecG3btjpJTS3bVoVqn6gPOsUNexVwMPuB/GTW\ntonK/8wAzgD+hcrFnB8C52Xt7wX2U3nW8I+A90xgTW+mcg7wt8DzwP7csn+b1doPfHCCt1XNuoA/\nBfZl/2j3ATdMYE3fBf4n+3vaC/S0eludak2t3E4F6/rH3L/pR/M7GCpHK08DfcC7211TK3/+itRV\n1fcxsp1tO7fVWDW1elvVe/mbwGZmiZrM1wDMzKyFHABmZolyAJiZJcoBYGaWKAeAmVmiHABmZoly\nAJiZJcoBYGaWqP8Hbv1G+35yCAwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngJlZov4JpgSPjNOFQRIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD8CAYAAACYebj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pTWOR6114og",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f7a7c6ce-5ee9-44c1-c016-62c2b03f4aff"
      },
      "source": [
        "dropout_rates[np.argmax(val_acc)]\n",
        "dropout_rates[np.argmax(val_acc)]\n",
        "l2_penalties[np.argmax(val_acc)]\n",
        "l2_penalties"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.07921565e-04, 5.93294760e-04, 6.13314921e-03, 7.05996491e-06,\n",
              "       3.29495360e-02, 6.27575153e-02, 4.50840982e-04, 1.81720181e-05])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsbUROQ02N5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c109faa7-2482-4510-b8ec-4b09642c6f39"
      },
      "source": [
        "final_model = models.Sequential()\n",
        "\n",
        "final_model.add(layers.Dropout(rate = 0.35))\n",
        "final_model.add(layers.Dense(80, activation='relu', input_shape=(10000,)))\n",
        "final_model.add(layers.Dropout(rate = 0.35))\n",
        "final_model.add(layers.Dense(80, activation='relu'))\n",
        "final_model.add(layers.Dropout(rate = 0.35))\n",
        "final_model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "final_model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = final_model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=100,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 9s 1ms/step - loss: 3.2783 - acc: 0.3582 - val_loss: 2.4726 - val_acc: 0.5140\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 2.1864 - acc: 0.4977 - val_loss: 1.7071 - val_acc: 0.5990\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.7090 - acc: 0.6050 - val_loss: 1.4593 - val_acc: 0.6710\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4711 - acc: 0.6646 - val_loss: 1.3192 - val_acc: 0.7050\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.3203 - acc: 0.6917 - val_loss: 1.2166 - val_acc: 0.7220\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1977 - acc: 0.7172 - val_loss: 1.1405 - val_acc: 0.7400\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.0911 - acc: 0.7392 - val_loss: 1.0815 - val_acc: 0.7610\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.0093 - acc: 0.7611 - val_loss: 1.0309 - val_acc: 0.7690\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9261 - acc: 0.7747 - val_loss: 0.9897 - val_acc: 0.7750\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.8562 - acc: 0.7899 - val_loss: 0.9616 - val_acc: 0.7910\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7852 - acc: 0.8086 - val_loss: 0.9415 - val_acc: 0.7950\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7424 - acc: 0.8178 - val_loss: 0.9222 - val_acc: 0.8010\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6845 - acc: 0.8300 - val_loss: 0.9071 - val_acc: 0.8100\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6355 - acc: 0.8416 - val_loss: 0.8935 - val_acc: 0.8200\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.5902 - acc: 0.8522 - val_loss: 0.8957 - val_acc: 0.8110\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.5653 - acc: 0.8607 - val_loss: 0.8929 - val_acc: 0.8200\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.5188 - acc: 0.8730 - val_loss: 0.8824 - val_acc: 0.8240\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.5036 - acc: 0.8728 - val_loss: 0.8905 - val_acc: 0.8210\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.4745 - acc: 0.8810 - val_loss: 0.8785 - val_acc: 0.8240\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4448 - acc: 0.8859 - val_loss: 0.8881 - val_acc: 0.8210\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4283 - acc: 0.8872 - val_loss: 0.8820 - val_acc: 0.8230\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.4064 - acc: 0.8926 - val_loss: 0.8800 - val_acc: 0.8310\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3872 - acc: 0.8995 - val_loss: 0.8838 - val_acc: 0.8310\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.3700 - acc: 0.9032 - val_loss: 0.9092 - val_acc: 0.8250\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3510 - acc: 0.9074 - val_loss: 0.8898 - val_acc: 0.8330\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.3523 - acc: 0.9069 - val_loss: 0.9134 - val_acc: 0.8270\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.3516 - acc: 0.9105 - val_loss: 0.9121 - val_acc: 0.8320\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 0.3306 - acc: 0.9103 - val_loss: 0.9239 - val_acc: 0.8290\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.3123 - acc: 0.9194 - val_loss: 0.9244 - val_acc: 0.8310\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.3159 - acc: 0.9162 - val_loss: 0.9234 - val_acc: 0.8320\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.3090 - acc: 0.9163 - val_loss: 0.9136 - val_acc: 0.8310\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2878 - acc: 0.9230 - val_loss: 0.9226 - val_acc: 0.8320\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2865 - acc: 0.9238 - val_loss: 0.9379 - val_acc: 0.8280\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2748 - acc: 0.9245 - val_loss: 0.9558 - val_acc: 0.8210\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2758 - acc: 0.9252 - val_loss: 0.9470 - val_acc: 0.8270\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2482 - acc: 0.9323 - val_loss: 0.9720 - val_acc: 0.8270\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2512 - acc: 0.9321 - val_loss: 0.9790 - val_acc: 0.8240\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2597 - acc: 0.9296 - val_loss: 0.9670 - val_acc: 0.8250\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2407 - acc: 0.9336 - val_loss: 0.9886 - val_acc: 0.8220\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2337 - acc: 0.9345 - val_loss: 0.9997 - val_acc: 0.8270\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2468 - acc: 0.9320 - val_loss: 0.9947 - val_acc: 0.8170\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2332 - acc: 0.9332 - val_loss: 1.0112 - val_acc: 0.8190\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2255 - acc: 0.9366 - val_loss: 1.0132 - val_acc: 0.8180\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2216 - acc: 0.9362 - val_loss: 1.0126 - val_acc: 0.8170\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.2178 - acc: 0.9359 - val_loss: 1.0126 - val_acc: 0.8230\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2197 - acc: 0.9386 - val_loss: 1.0123 - val_acc: 0.8210\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2157 - acc: 0.9410 - val_loss: 1.0202 - val_acc: 0.8230\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.2102 - acc: 0.9392 - val_loss: 1.0442 - val_acc: 0.8220\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2079 - acc: 0.9375 - val_loss: 1.0283 - val_acc: 0.8150\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2094 - acc: 0.9380 - val_loss: 1.0339 - val_acc: 0.8220\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.2063 - acc: 0.9410 - val_loss: 1.0472 - val_acc: 0.8220\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2017 - acc: 0.9432 - val_loss: 1.0394 - val_acc: 0.8170\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2051 - acc: 0.9405 - val_loss: 1.0404 - val_acc: 0.8180\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2006 - acc: 0.9406 - val_loss: 1.0523 - val_acc: 0.8190\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1892 - acc: 0.9449 - val_loss: 1.0726 - val_acc: 0.8250\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1956 - acc: 0.9420 - val_loss: 1.0660 - val_acc: 0.8230\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1873 - acc: 0.9437 - val_loss: 1.0644 - val_acc: 0.8190\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1897 - acc: 0.9442 - val_loss: 1.0781 - val_acc: 0.8200\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1811 - acc: 0.9473 - val_loss: 1.0795 - val_acc: 0.8220\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1880 - acc: 0.9430 - val_loss: 1.0913 - val_acc: 0.8190\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1972 - acc: 0.9412 - val_loss: 1.0766 - val_acc: 0.8220\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1712 - acc: 0.9464 - val_loss: 1.0781 - val_acc: 0.8190\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1806 - acc: 0.9474 - val_loss: 1.0809 - val_acc: 0.8270\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1779 - acc: 0.9425 - val_loss: 1.0870 - val_acc: 0.8210\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1672 - acc: 0.9493 - val_loss: 1.1184 - val_acc: 0.8250\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1707 - acc: 0.9461 - val_loss: 1.1220 - val_acc: 0.8170\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.1728 - acc: 0.9454 - val_loss: 1.0954 - val_acc: 0.8270\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1740 - acc: 0.9480 - val_loss: 1.1071 - val_acc: 0.8260\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1703 - acc: 0.9465 - val_loss: 1.1190 - val_acc: 0.8200\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1645 - acc: 0.9461 - val_loss: 1.1215 - val_acc: 0.8190\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1711 - acc: 0.9480 - val_loss: 1.1233 - val_acc: 0.8230\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1652 - acc: 0.9460 - val_loss: 1.1288 - val_acc: 0.8270\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1689 - acc: 0.9483 - val_loss: 1.1329 - val_acc: 0.8180\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1648 - acc: 0.9498 - val_loss: 1.1416 - val_acc: 0.8200\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1571 - acc: 0.9500 - val_loss: 1.1515 - val_acc: 0.8160\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1562 - acc: 0.9511 - val_loss: 1.1642 - val_acc: 0.8170\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1547 - acc: 0.9531 - val_loss: 1.1597 - val_acc: 0.8190\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1576 - acc: 0.9514 - val_loss: 1.1639 - val_acc: 0.8210\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1613 - acc: 0.9500 - val_loss: 1.1593 - val_acc: 0.8210\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1642 - acc: 0.9486 - val_loss: 1.1566 - val_acc: 0.8220\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1589 - acc: 0.9505 - val_loss: 1.1555 - val_acc: 0.8180\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1569 - acc: 0.9514 - val_loss: 1.1533 - val_acc: 0.8220\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1631 - acc: 0.9490 - val_loss: 1.1443 - val_acc: 0.8200\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1579 - acc: 0.9513 - val_loss: 1.1587 - val_acc: 0.8160\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1531 - acc: 0.9510 - val_loss: 1.1534 - val_acc: 0.8210\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1469 - acc: 0.9535 - val_loss: 1.1740 - val_acc: 0.8260\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1508 - acc: 0.9493 - val_loss: 1.1743 - val_acc: 0.8190\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.1539 - acc: 0.9530 - val_loss: 1.1790 - val_acc: 0.8170\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1456 - acc: 0.9518 - val_loss: 1.1714 - val_acc: 0.8190\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1487 - acc: 0.9534 - val_loss: 1.1691 - val_acc: 0.8220\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1497 - acc: 0.9496 - val_loss: 1.1689 - val_acc: 0.8220\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1486 - acc: 0.9529 - val_loss: 1.1725 - val_acc: 0.8230\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1397 - acc: 0.9541 - val_loss: 1.1790 - val_acc: 0.8180\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1397 - acc: 0.9533 - val_loss: 1.1754 - val_acc: 0.8200\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1399 - acc: 0.9541 - val_loss: 1.1899 - val_acc: 0.8160\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1461 - acc: 0.9518 - val_loss: 1.2006 - val_acc: 0.8170\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1425 - acc: 0.9538 - val_loss: 1.1966 - val_acc: 0.8180\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1424 - acc: 0.9523 - val_loss: 1.1926 - val_acc: 0.8180\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1485 - acc: 0.9514 - val_loss: 1.2038 - val_acc: 0.8180\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1433 - acc: 0.9533 - val_loss: 1.1957 - val_acc: 0.8160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WucnaK6g2pAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "d848432a-1dcf-41d0-8e68-abdb0e7e6fb1"
      },
      "source": [
        "print(chollet_model.evaluate(x_test, y_test))\n",
        "print(final_model.evaluate(x_test, y_test))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2246/2246 [==============================] - 0s 113us/step\n",
            "[1.2420880563112635, 0.7760463045944832]\n",
            "2246/2246 [==============================] - 0s 107us/step\n",
            "[1.4232519968003963, 0.8023152271234235]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l963rFnQqnvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}