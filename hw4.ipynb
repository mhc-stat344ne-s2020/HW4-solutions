{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSYV3U4JZdzt",
        "colab_type": "text"
      },
      "source": [
        "## Goals\n",
        "\n",
        "The goal of the coding part of this homework assignment is to practice the process of tuning hyperparameters to find a good neural network model.\n",
        "\n",
        "We will work with the same Reuters newswires data set from the last homework.  In that assignment, we accepted the model that Chollet used in the book.  Here we will see if we can improve that model at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO0Kphe9bPJk",
        "colab_type": "text"
      },
      "source": [
        "## Module Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C74M21bXSNw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "02a55b4c-db0c-48e2-8b24-dbc6fe5e4468"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras.datasets import reuters\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "import numpy as np\n",
        "import copy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzsvi3njSYZO",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdlIkalQXS6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_and_val_data, train_and_val_labels), (test_data, test_labels) = reuters.load_data(\n",
        "    num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "x_train_and_val = vectorize_sequences(train_and_val_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "def to_one_hot(labels, dimension=46):\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "one_hot_train_and_val_labels = to_one_hot(train_and_val_labels)\n",
        "y_train_and_val = one_hot_train_and_val_labels\n",
        "\n",
        "one_hot_test_labels = to_one_hot(test_labels)\n",
        "y_test = one_hot_test_labels\n",
        "\n",
        "x_val = x_train_and_val[:1000]\n",
        "x_train = x_train_and_val[1000:]\n",
        "\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "y_train = one_hot_train_labels[1000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhrYx59ySdc_",
        "colab_type": "text"
      },
      "source": [
        "## Chollet's Model\n",
        "You don't have to do anything here other than run the code below.  This is our baseline model, from the book."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RETrtWsUXYFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chollet_model = models.Sequential()\n",
        "chollet_model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "chollet_model.add(layers.Dense(64, activation='relu'))\n",
        "chollet_model.add(layers.Dense(46, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WXMSq-hXhKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "68691421-ee43-4688-c93a-b970ffa71c6c"
      },
      "source": [
        "chollet_model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = chollet_model.fit(x_train,\n",
        "                    y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "7982/7982 [==============================] - 1s 131us/step - loss: 2.5443 - acc: 0.5405 - val_loss: 1.6874 - val_acc: 0.6520\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.3742 - acc: 0.7093 - val_loss: 1.2770 - val_acc: 0.7180\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.0264 - acc: 0.7757 - val_loss: 1.1146 - val_acc: 0.7500\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.8105 - acc: 0.8267 - val_loss: 1.0324 - val_acc: 0.7810\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.6437 - acc: 0.8613 - val_loss: 0.9676 - val_acc: 0.7920\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.5149 - acc: 0.8887 - val_loss: 0.9308 - val_acc: 0.8110\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.4186 - acc: 0.9085 - val_loss: 0.9275 - val_acc: 0.8150\n",
            "Epoch 8/20\n",
            "7982/7982 [==============================] - 0s 53us/step - loss: 0.3375 - acc: 0.9268 - val_loss: 0.9122 - val_acc: 0.8200\n",
            "Epoch 9/20\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.2855 - acc: 0.9390 - val_loss: 0.9163 - val_acc: 0.8170\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 0s 53us/step - loss: 0.2390 - acc: 0.9449 - val_loss: 0.9369 - val_acc: 0.8100\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.2056 - acc: 0.9511 - val_loss: 0.9143 - val_acc: 0.8190\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.1834 - acc: 0.9518 - val_loss: 0.9661 - val_acc: 0.8080\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 0s 52us/step - loss: 0.1619 - acc: 0.9523 - val_loss: 0.9527 - val_acc: 0.8120\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1482 - acc: 0.9549 - val_loss: 0.9962 - val_acc: 0.8090\n",
            "Epoch 15/20\n",
            "7982/7982 [==============================] - 0s 53us/step - loss: 0.1404 - acc: 0.9540 - val_loss: 1.0321 - val_acc: 0.7970\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.1303 - acc: 0.9560 - val_loss: 1.0349 - val_acc: 0.7990\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.1213 - acc: 0.9580 - val_loss: 1.0461 - val_acc: 0.8020\n",
            "Epoch 18/20\n",
            "7982/7982 [==============================] - 0s 52us/step - loss: 0.1212 - acc: 0.9565 - val_loss: 1.0610 - val_acc: 0.8020\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.1110 - acc: 0.9575 - val_loss: 1.0975 - val_acc: 0.8010\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 0s 51us/step - loss: 0.1132 - acc: 0.9568 - val_loss: 1.1298 - val_acc: 0.7960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI-FebYPXpKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "5e586721-8831-4b3c-b8f3-0499a9011151"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "\n",
        "plt.plot(epochs, acc, label='Training acc')\n",
        "plt.plot(epochs, val_acc, label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j_fcs-bSqFi",
        "colab_type": "text"
      },
      "source": [
        "The validation set accuracy of this model ended up at about 0.8.  We'll try to find a combination of hyperparameters with better validation set accuracy, and then refit both our final model and Chollet's model to the full combined training and validation set data and evaluate their performance on the test set.\n",
        "\n",
        "## Hyperparameter tuning, adding regularization\n",
        "\n",
        "We have a pretty good model with some evidence that it is overfitting the training data.  Let's see if we can tune some of the hyperparameters of the model to achieve better validation set accuracy.  We will consider adding regularization using an $L_2$ penalty and/or dropout.  In this case, the size of our input layer (10000 features) is much larger than the size of our hidden layers, so it seems possible that we'd want to use dropout on the input layer, potentially with a different (larger?) dropout rate than the dropout rate for the hidden layers.  This means we will have three hyperparameters to pick: (1) dropout rate for the input layer; (2) dropout rate for the hidden layers; and (3) the penalty for $L_2$ regularization, applied to both hidden layers.  Let's use random search to explore performance of models with different values of these three hyperparameters.\n",
        "\n",
        "To do that, we will need to:\n",
        "\n",
        " * randomly generate sets of values for each of these hyperparameters.  Let's generate 8 sets of values; this should be enough to give a sense of which hyperparameters have the biggest impact on performance of this model without taking forever to fit (it will take long enough with just 8).\n",
        " * Specify and fit a model with each combination of hyperparameter settings, and save the validation set accuracy.\n",
        "\n",
        "#### 1. Random generation of hyperparameter values.\n",
        "\n",
        " * Common values for the dropout rate are between 0.0 (no dropout) and 0.5.  However, the size of our input features is so large that it seems possible we would want an even larger dropout rate on the input layer; let's consider values between 0.0 and 0.9 for the input layer (but note that dropout rates this large are unusual).  You can generate these uniformly at random in these intervals using the np.random.uniform function.  You want 8 of them, so set your size appropriately.\n",
        " * Common values for the penalty parameter $\\lambda$ are from about 0 (no $L_2$ penalty) to 0.1.  However, the effect of the penalty does not scale linearly: the effect on model performance of going from a penalty of $\\lambda = 0.0001$ to $\\lambda = 0.0002$ is larger than the effect of going from $\\lambda = 0.01$ to $\\lambda = 0.0101$ (though to be honest the relationship looked pretty close to linear in my results).  This means we would like to explore more values of $\\lambda$ near 0 than near 0.1.  We can do this by working on a logarithmic scale.  We will:\n",
        "    * First, generate exponents $u$ uniformly distributed between -6 and -1.  You want 8 of these.\n",
        "    * Second, set the values of $\\lambda$ to try to $10^{u}$.  In this way, our values of lambda will be between $10^{-6}$ and $10^{-1}$, with more values closer to $10^{-6}$.\n",
        "\n",
        "We'll assume that Chollet did a pretty thorough exporation of the number of layers and number of units per layer, and keep those hyperparameters fixed.  We'll also stick with the rmsprop optimizer with batch size of 512 he used.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwG6y4ajnDH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a9b53b60-3faf-463b-f09c-12ec03a88b8d"
      },
      "source": [
        "# Setting a seed so we all get comparable results\n",
        "np.random.seed(64730)\n",
        "\n",
        "# Generate dropout rates for the input layer:\n",
        "# 8 numbers uniformly distributed from 0 to 0.9\n",
        "input_dropout_rates = np.random.uniform(low = 0.0, high = 0.9, size = (8,))\n",
        "print(\"input dropout rates = \" + str(input_dropout_rates))\n",
        "\n",
        "# Generate dropout rates for the hidden layers:\n",
        "# 8 numbers uniformly distributed from 0 to 0.5\n",
        "hidden_dropout_rates = np.random.uniform(low = 0.0, high = 0.5, size = (8,))\n",
        "print(\"hidden dropout rates = \" + str(hidden_dropout_rates))\n",
        "\n",
        "# Generate L_2 penalty parameters, in two stages:\n",
        "# u should be uniformly distributed from -6 to -1\n",
        "# then l2_penalties is 10 to the power of u\n",
        "# Note that in python, lambda is a reserved keyword so it's best to use a\n",
        "# different name, which is why I've gone with l2_penalties here\n",
        "u = np.random.uniform(low = -6, high = -1, size = (8,))\n",
        "l2_penalties = 10**u\n",
        "print(\"l2_penalties = \" + str(l2_penalties))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dropout rates = [0.72807915 0.78121579 0.33759414 0.60574263 0.43113313 0.17192187\n",
            " 0.41814111 0.78693711]\n",
            "dropout rates = [0.30954935 0.09599261 0.06038929 0.03211626 0.43386078 0.01608936\n",
            " 0.48785323 0.40673594]\n",
            "l2_penalties = [1.17703389e-06 8.94505148e-05 1.94648204e-05 4.16051385e-03\n",
            " 2.23930277e-05 1.49160276e-06 7.48310905e-03 2.21459085e-04]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY4Oxi38jZTC",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Fit models and save validation set performance for each\n",
        "\n",
        "Write and code below to define and fit all 8 of your models with different values for the dropout rates and $L_2$ penalty parameter lambda.\n",
        "\n",
        " * Use dropout on the input layer as well as each hidden layer\n",
        " * Use 64 units and a relu activation for all hidden layers\n",
        " * Because regularization can slow down getting a good fit to the training data, let's use 100 epochs instead of 20.\n",
        "\n",
        "This will take several minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kvnOkc3mbyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce4c7ee4-4bc7-422e-c478-7a2669ca10ef"
      },
      "source": [
        "val_acc = np.zeros((8,))\n",
        "\n",
        "for (i, input_dropout_rate, hidden_dropout_rate, l2_penalty) in \\\n",
        "  zip(range(8), input_dropout_rates, hidden_dropout_rates, l2_penalties):\n",
        "  # define model\n",
        "  model = models.Sequential()\n",
        "\n",
        "  # add dropout layer for inputs, with its input_dropout_rate\n",
        "  model.add(layers.Dropout(rate = input_dropout_rate))\n",
        "\n",
        "  # add first hidden layer with L2 regularization and another dropout layer\n",
        "  model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l = l2_penalty), input_shape=(10000,)))\n",
        "  model.add(layers.Dropout(rate = hidden_dropout_rate))\n",
        "\n",
        "  # add second hidden layer with L2 regularization and another dropout layer\n",
        "  model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l = l2_penalty)))\n",
        "  model.add(layers.Dropout(rate = hidden_dropout_rate))\n",
        "\n",
        "  # add output layer\n",
        "  model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "  # compile model\n",
        "  model.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  # fit model using x_train and y_train, 100 epochs, a batch_size of 512, and\n",
        "  # validation_data = (x_val, y_val)\n",
        "  history = model.fit(x_train,\n",
        "                      y_train,\n",
        "                      epochs=100,\n",
        "                      batch_size=512,\n",
        "                      validation_data=(x_val, y_val))\n",
        "  \n",
        "  # save the validation set classification accuracy after 100 epochs\n",
        "  val_acc[i] = history.history['val_acc'][-1]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.728079 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 3s 404us/step - loss: 3.0306 - acc: 0.3681 - val_loss: 2.1040 - val_acc: 0.5430\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 2.1170 - acc: 0.5266 - val_loss: 1.6441 - val_acc: 0.6090\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.8481 - acc: 0.5779 - val_loss: 1.4803 - val_acc: 0.6690\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.7128 - acc: 0.6066 - val_loss: 1.3837 - val_acc: 0.6920\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.6050 - acc: 0.6239 - val_loss: 1.3182 - val_acc: 0.7030\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.5175 - acc: 0.6471 - val_loss: 1.2601 - val_acc: 0.7120\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4652 - acc: 0.6559 - val_loss: 1.2177 - val_acc: 0.7130\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3838 - acc: 0.6686 - val_loss: 1.1800 - val_acc: 0.7200\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3469 - acc: 0.6817 - val_loss: 1.1440 - val_acc: 0.7360\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2912 - acc: 0.6957 - val_loss: 1.1098 - val_acc: 0.7490\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2837 - acc: 0.6969 - val_loss: 1.0813 - val_acc: 0.7620\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.2233 - acc: 0.7115 - val_loss: 1.0488 - val_acc: 0.7720\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2119 - acc: 0.7095 - val_loss: 1.0290 - val_acc: 0.7740\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1764 - acc: 0.7191 - val_loss: 1.0136 - val_acc: 0.7800\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1437 - acc: 0.7269 - val_loss: 0.9910 - val_acc: 0.7820\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.1085 - acc: 0.7360 - val_loss: 0.9781 - val_acc: 0.7870\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.0933 - acc: 0.7394 - val_loss: 0.9564 - val_acc: 0.7870\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.0693 - acc: 0.7393 - val_loss: 0.9409 - val_acc: 0.7930\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0658 - acc: 0.7434 - val_loss: 0.9315 - val_acc: 0.7970\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0229 - acc: 0.7504 - val_loss: 0.9173 - val_acc: 0.8040\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0097 - acc: 0.7543 - val_loss: 0.9146 - val_acc: 0.8050\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.9909 - acc: 0.7556 - val_loss: 0.9029 - val_acc: 0.8090\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9652 - acc: 0.7595 - val_loss: 0.8979 - val_acc: 0.8090\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.9625 - acc: 0.7625 - val_loss: 0.8821 - val_acc: 0.8150\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.9439 - acc: 0.7671 - val_loss: 0.8666 - val_acc: 0.8190\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.9330 - acc: 0.7669 - val_loss: 0.8618 - val_acc: 0.8240\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.9074 - acc: 0.7735 - val_loss: 0.8605 - val_acc: 0.8200\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.9084 - acc: 0.7701 - val_loss: 0.8585 - val_acc: 0.8250\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9071 - acc: 0.7711 - val_loss: 0.8526 - val_acc: 0.8230\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.8888 - acc: 0.7774 - val_loss: 0.8461 - val_acc: 0.8270\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.8644 - acc: 0.7808 - val_loss: 0.8424 - val_acc: 0.8300\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.8608 - acc: 0.7859 - val_loss: 0.8456 - val_acc: 0.8270\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.8764 - acc: 0.7794 - val_loss: 0.8352 - val_acc: 0.8340\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.8424 - acc: 0.7873 - val_loss: 0.8375 - val_acc: 0.8300\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.8292 - acc: 0.7871 - val_loss: 0.8260 - val_acc: 0.8300\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.8142 - acc: 0.7902 - val_loss: 0.8371 - val_acc: 0.8270\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.8105 - acc: 0.7923 - val_loss: 0.8201 - val_acc: 0.8320\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7909 - acc: 0.7952 - val_loss: 0.8123 - val_acc: 0.8350\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.8158 - acc: 0.7939 - val_loss: 0.8087 - val_acc: 0.8380\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7836 - acc: 0.7992 - val_loss: 0.8090 - val_acc: 0.8370\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7627 - acc: 0.8101 - val_loss: 0.8111 - val_acc: 0.8310\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7707 - acc: 0.7972 - val_loss: 0.8073 - val_acc: 0.8330\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7580 - acc: 0.8069 - val_loss: 0.8063 - val_acc: 0.8310\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7690 - acc: 0.8061 - val_loss: 0.8048 - val_acc: 0.8340\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7668 - acc: 0.7999 - val_loss: 0.8134 - val_acc: 0.8310\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.7409 - acc: 0.8073 - val_loss: 0.8034 - val_acc: 0.8300\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.7394 - acc: 0.8057 - val_loss: 0.8063 - val_acc: 0.8290\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7211 - acc: 0.8097 - val_loss: 0.8110 - val_acc: 0.8280\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7387 - acc: 0.8083 - val_loss: 0.8162 - val_acc: 0.8300\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7411 - acc: 0.8094 - val_loss: 0.8202 - val_acc: 0.8270\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7389 - acc: 0.8053 - val_loss: 0.8121 - val_acc: 0.8310\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7069 - acc: 0.8161 - val_loss: 0.8079 - val_acc: 0.8330\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7150 - acc: 0.8106 - val_loss: 0.8047 - val_acc: 0.8350\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7124 - acc: 0.8172 - val_loss: 0.8081 - val_acc: 0.8350\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7062 - acc: 0.8172 - val_loss: 0.8140 - val_acc: 0.8340\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6933 - acc: 0.8197 - val_loss: 0.8133 - val_acc: 0.8320\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.6863 - acc: 0.8198 - val_loss: 0.8109 - val_acc: 0.8330\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.6908 - acc: 0.8171 - val_loss: 0.8114 - val_acc: 0.8310\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6874 - acc: 0.8230 - val_loss: 0.8027 - val_acc: 0.8340\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6857 - acc: 0.8198 - val_loss: 0.8005 - val_acc: 0.8330\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6682 - acc: 0.8275 - val_loss: 0.8140 - val_acc: 0.8340\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6901 - acc: 0.8217 - val_loss: 0.8038 - val_acc: 0.8370\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6869 - acc: 0.8205 - val_loss: 0.7996 - val_acc: 0.8370\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6675 - acc: 0.8195 - val_loss: 0.8032 - val_acc: 0.8360\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6786 - acc: 0.8218 - val_loss: 0.8022 - val_acc: 0.8310\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6746 - acc: 0.8213 - val_loss: 0.8043 - val_acc: 0.8310\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6612 - acc: 0.8280 - val_loss: 0.7946 - val_acc: 0.8380\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6565 - acc: 0.8301 - val_loss: 0.8018 - val_acc: 0.8330\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.6501 - acc: 0.8296 - val_loss: 0.8143 - val_acc: 0.8280\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6422 - acc: 0.8279 - val_loss: 0.8090 - val_acc: 0.8330\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6517 - acc: 0.8300 - val_loss: 0.8043 - val_acc: 0.8370\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6635 - acc: 0.8256 - val_loss: 0.8082 - val_acc: 0.8310\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6363 - acc: 0.8299 - val_loss: 0.8057 - val_acc: 0.8340\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6463 - acc: 0.8297 - val_loss: 0.7975 - val_acc: 0.8300\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6379 - acc: 0.8355 - val_loss: 0.8045 - val_acc: 0.8360\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.6388 - acc: 0.8350 - val_loss: 0.8089 - val_acc: 0.8310\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.6283 - acc: 0.8375 - val_loss: 0.8078 - val_acc: 0.8300\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6299 - acc: 0.8329 - val_loss: 0.8105 - val_acc: 0.8360\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6303 - acc: 0.8286 - val_loss: 0.8107 - val_acc: 0.8300\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6232 - acc: 0.8341 - val_loss: 0.8063 - val_acc: 0.8330\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6248 - acc: 0.8373 - val_loss: 0.8037 - val_acc: 0.8360\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6191 - acc: 0.8325 - val_loss: 0.7993 - val_acc: 0.8320\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6234 - acc: 0.8374 - val_loss: 0.8036 - val_acc: 0.8330\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6293 - acc: 0.8332 - val_loss: 0.8067 - val_acc: 0.8330\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.6084 - acc: 0.8371 - val_loss: 0.8076 - val_acc: 0.8310\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.6148 - acc: 0.8332 - val_loss: 0.8107 - val_acc: 0.8340\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6174 - acc: 0.8369 - val_loss: 0.8001 - val_acc: 0.8360\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.6235 - acc: 0.8324 - val_loss: 0.8084 - val_acc: 0.8350\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.5989 - acc: 0.8405 - val_loss: 0.8154 - val_acc: 0.8300\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.5950 - acc: 0.8420 - val_loss: 0.8243 - val_acc: 0.8310\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6126 - acc: 0.8406 - val_loss: 0.8192 - val_acc: 0.8320\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.5966 - acc: 0.8419 - val_loss: 0.8297 - val_acc: 0.8330\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6039 - acc: 0.8400 - val_loss: 0.8119 - val_acc: 0.8340\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6047 - acc: 0.8371 - val_loss: 0.8124 - val_acc: 0.8290\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.5813 - acc: 0.8406 - val_loss: 0.8244 - val_acc: 0.8280\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6055 - acc: 0.8394 - val_loss: 0.8245 - val_acc: 0.8280\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.5866 - acc: 0.8439 - val_loss: 0.8213 - val_acc: 0.8320\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6073 - acc: 0.8381 - val_loss: 0.8165 - val_acc: 0.8320\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.5863 - acc: 0.8436 - val_loss: 0.8175 - val_acc: 0.8280\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.5852 - acc: 0.8439 - val_loss: 0.8242 - val_acc: 0.8300\n",
            "WARNING:tensorflow:Large dropout rate: 0.781216 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 3s 405us/step - loss: 2.9372 - acc: 0.4277 - val_loss: 2.0815 - val_acc: 0.5380\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 2.0562 - acc: 0.5396 - val_loss: 1.6343 - val_acc: 0.6330\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.7679 - acc: 0.6089 - val_loss: 1.4263 - val_acc: 0.6590\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.6095 - acc: 0.6361 - val_loss: 1.3149 - val_acc: 0.7030\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.5062 - acc: 0.6535 - val_loss: 1.2284 - val_acc: 0.7180\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.4259 - acc: 0.6763 - val_loss: 1.1666 - val_acc: 0.7350\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3419 - acc: 0.6928 - val_loss: 1.1177 - val_acc: 0.7460\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3058 - acc: 0.6969 - val_loss: 1.0839 - val_acc: 0.7610\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2508 - acc: 0.7101 - val_loss: 1.0480 - val_acc: 0.7760\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1862 - acc: 0.7239 - val_loss: 1.0197 - val_acc: 0.7850\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1550 - acc: 0.7241 - val_loss: 0.9945 - val_acc: 0.7890\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1270 - acc: 0.7389 - val_loss: 0.9730 - val_acc: 0.8060\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.0986 - acc: 0.7377 - val_loss: 0.9517 - val_acc: 0.8060\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0482 - acc: 0.7513 - val_loss: 0.9367 - val_acc: 0.8060\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0267 - acc: 0.7519 - val_loss: 0.9289 - val_acc: 0.8010\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.0029 - acc: 0.7592 - val_loss: 0.9164 - val_acc: 0.8120\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9892 - acc: 0.7596 - val_loss: 0.9081 - val_acc: 0.8120\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9452 - acc: 0.7712 - val_loss: 0.8918 - val_acc: 0.8180\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9392 - acc: 0.7694 - val_loss: 0.8914 - val_acc: 0.8200\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9238 - acc: 0.7747 - val_loss: 0.8741 - val_acc: 0.8150\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.9131 - acc: 0.7745 - val_loss: 0.8676 - val_acc: 0.8200\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8859 - acc: 0.7799 - val_loss: 0.8639 - val_acc: 0.8230\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.8804 - acc: 0.7824 - val_loss: 0.8467 - val_acc: 0.8260\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.8655 - acc: 0.7869 - val_loss: 0.8426 - val_acc: 0.8320\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8580 - acc: 0.7861 - val_loss: 0.8443 - val_acc: 0.8270\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8487 - acc: 0.7918 - val_loss: 0.8345 - val_acc: 0.8340\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.8328 - acc: 0.7889 - val_loss: 0.8297 - val_acc: 0.8270\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.8257 - acc: 0.7954 - val_loss: 0.8326 - val_acc: 0.8330\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.8025 - acc: 0.7999 - val_loss: 0.8327 - val_acc: 0.8320\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.8037 - acc: 0.7957 - val_loss: 0.8297 - val_acc: 0.8300\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7851 - acc: 0.8001 - val_loss: 0.8265 - val_acc: 0.8310\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7850 - acc: 0.8016 - val_loss: 0.8241 - val_acc: 0.8300\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7753 - acc: 0.8072 - val_loss: 0.8268 - val_acc: 0.8300\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7764 - acc: 0.8032 - val_loss: 0.8238 - val_acc: 0.8320\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7733 - acc: 0.8043 - val_loss: 0.8221 - val_acc: 0.8360\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.7543 - acc: 0.8073 - val_loss: 0.8303 - val_acc: 0.8320\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7374 - acc: 0.8106 - val_loss: 0.8297 - val_acc: 0.8310\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7321 - acc: 0.8074 - val_loss: 0.8336 - val_acc: 0.8310\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.7374 - acc: 0.8066 - val_loss: 0.8289 - val_acc: 0.8280\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7508 - acc: 0.8082 - val_loss: 0.8243 - val_acc: 0.8320\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.7397 - acc: 0.8136 - val_loss: 0.8214 - val_acc: 0.8380\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7301 - acc: 0.8126 - val_loss: 0.8171 - val_acc: 0.8340\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.7418 - acc: 0.8120 - val_loss: 0.8122 - val_acc: 0.8360\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7268 - acc: 0.8150 - val_loss: 0.8096 - val_acc: 0.8330\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7154 - acc: 0.8176 - val_loss: 0.8314 - val_acc: 0.8290\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.7041 - acc: 0.8163 - val_loss: 0.8336 - val_acc: 0.8300\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6888 - acc: 0.8232 - val_loss: 0.8253 - val_acc: 0.8280\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6990 - acc: 0.8217 - val_loss: 0.8361 - val_acc: 0.8270\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6932 - acc: 0.8252 - val_loss: 0.8320 - val_acc: 0.8320\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6974 - acc: 0.8201 - val_loss: 0.8294 - val_acc: 0.8300\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.7023 - acc: 0.8213 - val_loss: 0.8200 - val_acc: 0.8310\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.6956 - acc: 0.8222 - val_loss: 0.8260 - val_acc: 0.8320\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6872 - acc: 0.8218 - val_loss: 0.8184 - val_acc: 0.8350\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.6722 - acc: 0.8230 - val_loss: 0.8233 - val_acc: 0.8360\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6930 - acc: 0.8190 - val_loss: 0.8222 - val_acc: 0.8390\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6691 - acc: 0.8237 - val_loss: 0.8132 - val_acc: 0.8380\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6826 - acc: 0.8276 - val_loss: 0.8134 - val_acc: 0.8350\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.6689 - acc: 0.8265 - val_loss: 0.8262 - val_acc: 0.8340\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6567 - acc: 0.8307 - val_loss: 0.8364 - val_acc: 0.8350\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6658 - acc: 0.8276 - val_loss: 0.8294 - val_acc: 0.8380\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6480 - acc: 0.8264 - val_loss: 0.8425 - val_acc: 0.8340\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6667 - acc: 0.8250 - val_loss: 0.8337 - val_acc: 0.8360\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.6818 - acc: 0.8235 - val_loss: 0.8243 - val_acc: 0.8420\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6809 - acc: 0.8188 - val_loss: 0.8282 - val_acc: 0.8360\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6491 - acc: 0.8289 - val_loss: 0.8343 - val_acc: 0.8330\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6633 - acc: 0.8282 - val_loss: 0.8365 - val_acc: 0.8420\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.6568 - acc: 0.8280 - val_loss: 0.8457 - val_acc: 0.8320\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6319 - acc: 0.8369 - val_loss: 0.8493 - val_acc: 0.8330\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6452 - acc: 0.8312 - val_loss: 0.8431 - val_acc: 0.8310\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6506 - acc: 0.8321 - val_loss: 0.8349 - val_acc: 0.8460\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6374 - acc: 0.8329 - val_loss: 0.8478 - val_acc: 0.8340\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6575 - acc: 0.8297 - val_loss: 0.8544 - val_acc: 0.8280\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6246 - acc: 0.8348 - val_loss: 0.8510 - val_acc: 0.8320\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6399 - acc: 0.8336 - val_loss: 0.8504 - val_acc: 0.8320\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6423 - acc: 0.8349 - val_loss: 0.8520 - val_acc: 0.8340\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6493 - acc: 0.8314 - val_loss: 0.8417 - val_acc: 0.8340\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6291 - acc: 0.8340 - val_loss: 0.8507 - val_acc: 0.8300\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.6342 - acc: 0.8343 - val_loss: 0.8545 - val_acc: 0.8320\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6450 - acc: 0.8296 - val_loss: 0.8367 - val_acc: 0.8360\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6410 - acc: 0.8325 - val_loss: 0.8479 - val_acc: 0.8330\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6567 - acc: 0.8332 - val_loss: 0.8478 - val_acc: 0.8290\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6226 - acc: 0.8355 - val_loss: 0.8583 - val_acc: 0.8340\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6346 - acc: 0.8336 - val_loss: 0.8491 - val_acc: 0.8340\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6295 - acc: 0.8349 - val_loss: 0.8465 - val_acc: 0.8370\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6315 - acc: 0.8361 - val_loss: 0.8510 - val_acc: 0.8350\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6375 - acc: 0.8317 - val_loss: 0.8453 - val_acc: 0.8370\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6283 - acc: 0.8350 - val_loss: 0.8536 - val_acc: 0.8290\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6305 - acc: 0.8365 - val_loss: 0.8405 - val_acc: 0.8400\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6246 - acc: 0.8393 - val_loss: 0.8396 - val_acc: 0.8370\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.6195 - acc: 0.8371 - val_loss: 0.8371 - val_acc: 0.8340\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.6131 - acc: 0.8380 - val_loss: 0.8457 - val_acc: 0.8330\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6065 - acc: 0.8395 - val_loss: 0.8520 - val_acc: 0.8430\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6356 - acc: 0.8356 - val_loss: 0.8659 - val_acc: 0.8360\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.6245 - acc: 0.8376 - val_loss: 0.8507 - val_acc: 0.8400\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6132 - acc: 0.8400 - val_loss: 0.8580 - val_acc: 0.8370\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.6204 - acc: 0.8390 - val_loss: 0.8568 - val_acc: 0.8350\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6078 - acc: 0.8421 - val_loss: 0.8551 - val_acc: 0.8380\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.6255 - acc: 0.8312 - val_loss: 0.8533 - val_acc: 0.8320\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6159 - acc: 0.8349 - val_loss: 0.8527 - val_acc: 0.8320\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.5960 - acc: 0.8438 - val_loss: 0.8558 - val_acc: 0.8340\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 3s 411us/step - loss: 2.8453 - acc: 0.4728 - val_loss: 1.8614 - val_acc: 0.6130\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.6236 - acc: 0.6555 - val_loss: 1.3809 - val_acc: 0.6990\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.2726 - acc: 0.7202 - val_loss: 1.1895 - val_acc: 0.7430\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.0735 - acc: 0.7672 - val_loss: 1.0961 - val_acc: 0.7520\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9211 - acc: 0.7937 - val_loss: 1.0508 - val_acc: 0.7570\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.7980 - acc: 0.8190 - val_loss: 0.9565 - val_acc: 0.7930\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7063 - acc: 0.8463 - val_loss: 0.9243 - val_acc: 0.8090\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.6215 - acc: 0.8614 - val_loss: 0.9134 - val_acc: 0.8010\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.5484 - acc: 0.8805 - val_loss: 0.8868 - val_acc: 0.8170\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4992 - acc: 0.8900 - val_loss: 0.9075 - val_acc: 0.8090\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.4501 - acc: 0.9004 - val_loss: 0.8623 - val_acc: 0.8220\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.3963 - acc: 0.9126 - val_loss: 0.8778 - val_acc: 0.8200\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.3667 - acc: 0.9218 - val_loss: 0.9061 - val_acc: 0.8090\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.3453 - acc: 0.9218 - val_loss: 0.8722 - val_acc: 0.8300\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.3227 - acc: 0.9251 - val_loss: 0.8923 - val_acc: 0.8150\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2919 - acc: 0.9320 - val_loss: 0.8922 - val_acc: 0.8240\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2765 - acc: 0.9345 - val_loss: 0.9010 - val_acc: 0.8190\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.2585 - acc: 0.9374 - val_loss: 0.9158 - val_acc: 0.8260\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2539 - acc: 0.9379 - val_loss: 0.9161 - val_acc: 0.8260\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.2540 - acc: 0.9400 - val_loss: 0.9137 - val_acc: 0.8180\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.2292 - acc: 0.9421 - val_loss: 0.9562 - val_acc: 0.8120\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.2283 - acc: 0.9441 - val_loss: 0.9576 - val_acc: 0.8100\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2122 - acc: 0.9427 - val_loss: 0.9914 - val_acc: 0.8130\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2227 - acc: 0.9426 - val_loss: 0.9584 - val_acc: 0.8180\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2088 - acc: 0.9464 - val_loss: 0.9578 - val_acc: 0.8160\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1985 - acc: 0.9486 - val_loss: 0.9569 - val_acc: 0.8210\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1888 - acc: 0.9515 - val_loss: 0.9747 - val_acc: 0.8110\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1859 - acc: 0.9518 - val_loss: 0.9863 - val_acc: 0.8090\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1842 - acc: 0.9535 - val_loss: 0.9889 - val_acc: 0.8140\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1863 - acc: 0.9513 - val_loss: 1.0001 - val_acc: 0.8110\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1810 - acc: 0.9525 - val_loss: 1.0017 - val_acc: 0.8190\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1763 - acc: 0.9503 - val_loss: 1.0069 - val_acc: 0.8210\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1718 - acc: 0.9534 - val_loss: 1.0022 - val_acc: 0.8270\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.1667 - acc: 0.9539 - val_loss: 1.0767 - val_acc: 0.8040\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1700 - acc: 0.9518 - val_loss: 1.0298 - val_acc: 0.8100\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.1648 - acc: 0.9555 - val_loss: 1.0551 - val_acc: 0.8050\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1640 - acc: 0.9514 - val_loss: 1.0050 - val_acc: 0.8230\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1646 - acc: 0.9535 - val_loss: 1.0354 - val_acc: 0.8170\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1653 - acc: 0.9516 - val_loss: 1.0191 - val_acc: 0.8160\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.1605 - acc: 0.9548 - val_loss: 1.0146 - val_acc: 0.8130\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1594 - acc: 0.9538 - val_loss: 1.0366 - val_acc: 0.8140\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1582 - acc: 0.9540 - val_loss: 1.0105 - val_acc: 0.8150\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1615 - acc: 0.9530 - val_loss: 1.0408 - val_acc: 0.8130\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1521 - acc: 0.9574 - val_loss: 1.0653 - val_acc: 0.8100\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1537 - acc: 0.9567 - val_loss: 1.0545 - val_acc: 0.8090\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1501 - acc: 0.9541 - val_loss: 1.0783 - val_acc: 0.8060\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1510 - acc: 0.9559 - val_loss: 1.0579 - val_acc: 0.8100\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1494 - acc: 0.9554 - val_loss: 1.0325 - val_acc: 0.8190\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1523 - acc: 0.9538 - val_loss: 1.0861 - val_acc: 0.8090\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1475 - acc: 0.9568 - val_loss: 1.0451 - val_acc: 0.8140\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1470 - acc: 0.9562 - val_loss: 1.0648 - val_acc: 0.8090\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1492 - acc: 0.9564 - val_loss: 1.0847 - val_acc: 0.8120\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1491 - acc: 0.9546 - val_loss: 1.0648 - val_acc: 0.8110\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1474 - acc: 0.9558 - val_loss: 1.0708 - val_acc: 0.8100\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1446 - acc: 0.9577 - val_loss: 1.0639 - val_acc: 0.8110\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1402 - acc: 0.9562 - val_loss: 1.0536 - val_acc: 0.8170\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1444 - acc: 0.9558 - val_loss: 1.0804 - val_acc: 0.8080\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1442 - acc: 0.9570 - val_loss: 1.0892 - val_acc: 0.8120\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1433 - acc: 0.9551 - val_loss: 1.0615 - val_acc: 0.8110\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1448 - acc: 0.9546 - val_loss: 1.0560 - val_acc: 0.8100\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1346 - acc: 0.9598 - val_loss: 1.1137 - val_acc: 0.8170\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1393 - acc: 0.9588 - val_loss: 1.1125 - val_acc: 0.8150\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1384 - acc: 0.9592 - val_loss: 1.1174 - val_acc: 0.8140\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1342 - acc: 0.9575 - val_loss: 1.0902 - val_acc: 0.8090\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1288 - acc: 0.9593 - val_loss: 1.1048 - val_acc: 0.8060\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1467 - acc: 0.9550 - val_loss: 1.1145 - val_acc: 0.8040\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1359 - acc: 0.9582 - val_loss: 1.0973 - val_acc: 0.8090\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1325 - acc: 0.9569 - val_loss: 1.1084 - val_acc: 0.8050\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1274 - acc: 0.9588 - val_loss: 1.0919 - val_acc: 0.8100\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1302 - acc: 0.9587 - val_loss: 1.0984 - val_acc: 0.8130\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1351 - acc: 0.9589 - val_loss: 1.1058 - val_acc: 0.8070\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1294 - acc: 0.9569 - val_loss: 1.1142 - val_acc: 0.8070\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1316 - acc: 0.9582 - val_loss: 1.0958 - val_acc: 0.8070\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1333 - acc: 0.9562 - val_loss: 1.0874 - val_acc: 0.8150\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1358 - acc: 0.9563 - val_loss: 1.1023 - val_acc: 0.8110\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1268 - acc: 0.9589 - val_loss: 1.1155 - val_acc: 0.8000\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1347 - acc: 0.9577 - val_loss: 1.1116 - val_acc: 0.8030\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1312 - acc: 0.9585 - val_loss: 1.1173 - val_acc: 0.8120\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1333 - acc: 0.9577 - val_loss: 1.1121 - val_acc: 0.8140\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1285 - acc: 0.9563 - val_loss: 1.1480 - val_acc: 0.8030\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1263 - acc: 0.9598 - val_loss: 1.1405 - val_acc: 0.8040\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1294 - acc: 0.9598 - val_loss: 1.1456 - val_acc: 0.8050\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1295 - acc: 0.9585 - val_loss: 1.1524 - val_acc: 0.8020\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1289 - acc: 0.9588 - val_loss: 1.1266 - val_acc: 0.8110\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1272 - acc: 0.9570 - val_loss: 1.1286 - val_acc: 0.8130\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1252 - acc: 0.9597 - val_loss: 1.1586 - val_acc: 0.8090\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1243 - acc: 0.9578 - val_loss: 1.1745 - val_acc: 0.8090\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1298 - acc: 0.9554 - val_loss: 1.1706 - val_acc: 0.8060\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.1270 - acc: 0.9593 - val_loss: 1.1664 - val_acc: 0.8090\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1267 - acc: 0.9588 - val_loss: 1.1661 - val_acc: 0.8090\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1222 - acc: 0.9628 - val_loss: 1.1723 - val_acc: 0.8070\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1252 - acc: 0.9582 - val_loss: 1.1745 - val_acc: 0.8120\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1263 - acc: 0.9582 - val_loss: 1.1320 - val_acc: 0.8080\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1291 - acc: 0.9568 - val_loss: 1.1484 - val_acc: 0.8090\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.1268 - acc: 0.9582 - val_loss: 1.1549 - val_acc: 0.8090\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1225 - acc: 0.9574 - val_loss: 1.1927 - val_acc: 0.8060\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1220 - acc: 0.9585 - val_loss: 1.1971 - val_acc: 0.8100\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1231 - acc: 0.9617 - val_loss: 1.1927 - val_acc: 0.8050\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1213 - acc: 0.9597 - val_loss: 1.1737 - val_acc: 0.8090\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1226 - acc: 0.9582 - val_loss: 1.1840 - val_acc: 0.8110\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 3s 430us/step - loss: 3.2629 - acc: 0.4820 - val_loss: 2.3044 - val_acc: 0.6080\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 2.1450 - acc: 0.6282 - val_loss: 1.8687 - val_acc: 0.6760\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.8641 - acc: 0.6736 - val_loss: 1.7022 - val_acc: 0.6980\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.7160 - acc: 0.6966 - val_loss: 1.6055 - val_acc: 0.7200\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.6409 - acc: 0.7140 - val_loss: 1.5464 - val_acc: 0.7360\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.5656 - acc: 0.7276 - val_loss: 1.5040 - val_acc: 0.7390\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.5158 - acc: 0.7439 - val_loss: 1.4813 - val_acc: 0.7460\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4622 - acc: 0.7488 - val_loss: 1.4384 - val_acc: 0.7610\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.4287 - acc: 0.7570 - val_loss: 1.4198 - val_acc: 0.7710\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.4089 - acc: 0.7601 - val_loss: 1.3944 - val_acc: 0.7570\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.3806 - acc: 0.7622 - val_loss: 1.3818 - val_acc: 0.7770\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.3609 - acc: 0.7682 - val_loss: 1.3651 - val_acc: 0.7800\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.3380 - acc: 0.7756 - val_loss: 1.3614 - val_acc: 0.7870\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3227 - acc: 0.7721 - val_loss: 1.3691 - val_acc: 0.7630\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.3018 - acc: 0.7766 - val_loss: 1.3468 - val_acc: 0.7720\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2789 - acc: 0.7756 - val_loss: 1.3362 - val_acc: 0.7830\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2716 - acc: 0.7830 - val_loss: 1.3200 - val_acc: 0.7810\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2410 - acc: 0.7905 - val_loss: 1.2996 - val_acc: 0.7870\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.2453 - acc: 0.7823 - val_loss: 1.2970 - val_acc: 0.7980\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.2300 - acc: 0.7865 - val_loss: 1.2794 - val_acc: 0.7930\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2206 - acc: 0.7902 - val_loss: 1.2987 - val_acc: 0.7820\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2110 - acc: 0.7893 - val_loss: 1.2692 - val_acc: 0.8000\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.1997 - acc: 0.7927 - val_loss: 1.2614 - val_acc: 0.7960\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1943 - acc: 0.7988 - val_loss: 1.2721 - val_acc: 0.8010\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1725 - acc: 0.7953 - val_loss: 1.2754 - val_acc: 0.7930\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1896 - acc: 0.7928 - val_loss: 1.2657 - val_acc: 0.7970\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1580 - acc: 0.8021 - val_loss: 1.2554 - val_acc: 0.8060\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.1512 - acc: 0.8027 - val_loss: 1.2416 - val_acc: 0.7990\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.1407 - acc: 0.8029 - val_loss: 1.2383 - val_acc: 0.8050\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1331 - acc: 0.8009 - val_loss: 1.2175 - val_acc: 0.8090\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1430 - acc: 0.8013 - val_loss: 1.2361 - val_acc: 0.8030\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.1234 - acc: 0.8094 - val_loss: 1.2345 - val_acc: 0.8010\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.1147 - acc: 0.8094 - val_loss: 1.2220 - val_acc: 0.8070\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1174 - acc: 0.8104 - val_loss: 1.2228 - val_acc: 0.8030\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1142 - acc: 0.8098 - val_loss: 1.2320 - val_acc: 0.8030\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1072 - acc: 0.8074 - val_loss: 1.2128 - val_acc: 0.8090\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.1024 - acc: 0.8132 - val_loss: 1.1903 - val_acc: 0.8180\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 1.0975 - acc: 0.8122 - val_loss: 1.1928 - val_acc: 0.8170\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0952 - acc: 0.8104 - val_loss: 1.2069 - val_acc: 0.8080\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0812 - acc: 0.8142 - val_loss: 1.2149 - val_acc: 0.8010\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0839 - acc: 0.8145 - val_loss: 1.1940 - val_acc: 0.8170\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0764 - acc: 0.8181 - val_loss: 1.1871 - val_acc: 0.8100\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.0533 - acc: 0.8166 - val_loss: 1.2093 - val_acc: 0.7980\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0593 - acc: 0.8211 - val_loss: 1.1890 - val_acc: 0.8090\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0636 - acc: 0.8182 - val_loss: 1.1836 - val_acc: 0.8090\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0494 - acc: 0.8224 - val_loss: 1.1959 - val_acc: 0.8040\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0420 - acc: 0.8225 - val_loss: 1.1913 - val_acc: 0.8070\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0464 - acc: 0.8215 - val_loss: 1.1910 - val_acc: 0.8100\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0312 - acc: 0.8225 - val_loss: 1.1708 - val_acc: 0.8170\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.0261 - acc: 0.8255 - val_loss: 1.1591 - val_acc: 0.8200\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0397 - acc: 0.8237 - val_loss: 1.1652 - val_acc: 0.8100\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0313 - acc: 0.8251 - val_loss: 1.1778 - val_acc: 0.8060\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.0095 - acc: 0.8315 - val_loss: 1.1444 - val_acc: 0.8170\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.0104 - acc: 0.8309 - val_loss: 1.1693 - val_acc: 0.8020\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.0204 - acc: 0.8259 - val_loss: 1.1520 - val_acc: 0.8100\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0168 - acc: 0.8287 - val_loss: 1.1572 - val_acc: 0.8050\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.0073 - acc: 0.8291 - val_loss: 1.1612 - val_acc: 0.8140\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9930 - acc: 0.8304 - val_loss: 1.1476 - val_acc: 0.8090\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.0077 - acc: 0.8271 - val_loss: 1.1415 - val_acc: 0.8140\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.0022 - acc: 0.8302 - val_loss: 1.1792 - val_acc: 0.7990\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 1.0019 - acc: 0.8261 - val_loss: 1.1566 - val_acc: 0.8160\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.0011 - acc: 0.8309 - val_loss: 1.1553 - val_acc: 0.8200\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.9922 - acc: 0.8311 - val_loss: 1.1572 - val_acc: 0.8110\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.9833 - acc: 0.8320 - val_loss: 1.1516 - val_acc: 0.8160\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.9805 - acc: 0.8344 - val_loss: 1.1434 - val_acc: 0.8130\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9825 - acc: 0.8302 - val_loss: 1.1350 - val_acc: 0.8090\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9808 - acc: 0.8365 - val_loss: 1.1348 - val_acc: 0.8110\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9527 - acc: 0.8400 - val_loss: 1.1376 - val_acc: 0.8130\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.9830 - acc: 0.8305 - val_loss: 1.1418 - val_acc: 0.8060\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9619 - acc: 0.8368 - val_loss: 1.1552 - val_acc: 0.8050\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9569 - acc: 0.8386 - val_loss: 1.1742 - val_acc: 0.8020\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9665 - acc: 0.8329 - val_loss: 1.1481 - val_acc: 0.8000\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9574 - acc: 0.8375 - val_loss: 1.1370 - val_acc: 0.8060\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9500 - acc: 0.8375 - val_loss: 1.1317 - val_acc: 0.8130\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9513 - acc: 0.8394 - val_loss: 1.1383 - val_acc: 0.8120\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9573 - acc: 0.8390 - val_loss: 1.1397 - val_acc: 0.8140\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9485 - acc: 0.8403 - val_loss: 1.1351 - val_acc: 0.8130\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.9453 - acc: 0.8358 - val_loss: 1.1141 - val_acc: 0.8090\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9393 - acc: 0.8458 - val_loss: 1.1152 - val_acc: 0.8170\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.9388 - acc: 0.8458 - val_loss: 1.1372 - val_acc: 0.8060\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9315 - acc: 0.8430 - val_loss: 1.1367 - val_acc: 0.8080\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9236 - acc: 0.8447 - val_loss: 1.1590 - val_acc: 0.8050\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9314 - acc: 0.8404 - val_loss: 1.1256 - val_acc: 0.8200\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.9157 - acc: 0.8450 - val_loss: 1.1552 - val_acc: 0.8050\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.9264 - acc: 0.8433 - val_loss: 1.1017 - val_acc: 0.8200\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.9291 - acc: 0.8444 - val_loss: 1.1282 - val_acc: 0.8040\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9246 - acc: 0.8430 - val_loss: 1.0974 - val_acc: 0.8240\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9059 - acc: 0.8522 - val_loss: 1.1059 - val_acc: 0.8130\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9110 - acc: 0.8479 - val_loss: 1.1087 - val_acc: 0.8180\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9145 - acc: 0.8472 - val_loss: 1.1133 - val_acc: 0.8200\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9175 - acc: 0.8438 - val_loss: 1.1065 - val_acc: 0.8190\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9085 - acc: 0.8469 - val_loss: 1.1202 - val_acc: 0.8190\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.9101 - acc: 0.8414 - val_loss: 1.1106 - val_acc: 0.8190\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.9006 - acc: 0.8502 - val_loss: 1.1108 - val_acc: 0.8190\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.8980 - acc: 0.8499 - val_loss: 1.0947 - val_acc: 0.8250\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9123 - acc: 0.8439 - val_loss: 1.1070 - val_acc: 0.8200\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.9026 - acc: 0.8504 - val_loss: 1.1205 - val_acc: 0.8140\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.9013 - acc: 0.8490 - val_loss: 1.1125 - val_acc: 0.8120\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.8985 - acc: 0.8458 - val_loss: 1.1104 - val_acc: 0.8140\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.8871 - acc: 0.8522 - val_loss: 1.1152 - val_acc: 0.8080\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 4s 450us/step - loss: 2.9368 - acc: 0.3939 - val_loss: 1.9897 - val_acc: 0.5580\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.9940 - acc: 0.5436 - val_loss: 1.5879 - val_acc: 0.6390\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.7057 - acc: 0.5985 - val_loss: 1.4183 - val_acc: 0.6710\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.5455 - acc: 0.6424 - val_loss: 1.3330 - val_acc: 0.6920\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4260 - acc: 0.6659 - val_loss: 1.2663 - val_acc: 0.7090\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.3553 - acc: 0.6880 - val_loss: 1.2150 - val_acc: 0.7210\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2858 - acc: 0.6984 - val_loss: 1.1793 - val_acc: 0.7260\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.2152 - acc: 0.7145 - val_loss: 1.1426 - val_acc: 0.7450\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1793 - acc: 0.7241 - val_loss: 1.1175 - val_acc: 0.7540\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1336 - acc: 0.7343 - val_loss: 1.0927 - val_acc: 0.7560\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0914 - acc: 0.7434 - val_loss: 1.0664 - val_acc: 0.7650\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.0412 - acc: 0.7577 - val_loss: 1.0465 - val_acc: 0.7710\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.0165 - acc: 0.7591 - val_loss: 1.0350 - val_acc: 0.7770\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.9855 - acc: 0.7652 - val_loss: 1.0108 - val_acc: 0.7870\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.9535 - acc: 0.7729 - val_loss: 0.9993 - val_acc: 0.7890\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.9155 - acc: 0.7813 - val_loss: 0.9887 - val_acc: 0.7960\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.8950 - acc: 0.7878 - val_loss: 0.9812 - val_acc: 0.7990\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.8701 - acc: 0.7920 - val_loss: 0.9775 - val_acc: 0.7870\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8418 - acc: 0.7940 - val_loss: 0.9691 - val_acc: 0.7990\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.8148 - acc: 0.8029 - val_loss: 0.9591 - val_acc: 0.8000\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.8116 - acc: 0.8051 - val_loss: 0.9576 - val_acc: 0.8030\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.7820 - acc: 0.8091 - val_loss: 0.9458 - val_acc: 0.8040\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.7658 - acc: 0.8093 - val_loss: 0.9503 - val_acc: 0.8030\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7510 - acc: 0.8160 - val_loss: 0.9430 - val_acc: 0.8100\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7381 - acc: 0.8195 - val_loss: 0.9454 - val_acc: 0.8090\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.7146 - acc: 0.8266 - val_loss: 0.9279 - val_acc: 0.8140\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6827 - acc: 0.8306 - val_loss: 0.9426 - val_acc: 0.8190\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.6961 - acc: 0.8295 - val_loss: 0.9335 - val_acc: 0.8130\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 59us/step - loss: 0.6693 - acc: 0.8319 - val_loss: 0.9329 - val_acc: 0.8100\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6493 - acc: 0.8374 - val_loss: 0.9533 - val_acc: 0.8120\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6506 - acc: 0.8396 - val_loss: 0.9399 - val_acc: 0.8160\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6417 - acc: 0.8468 - val_loss: 0.9468 - val_acc: 0.8080\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6199 - acc: 0.8418 - val_loss: 0.9430 - val_acc: 0.8120\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.6118 - acc: 0.8483 - val_loss: 0.9429 - val_acc: 0.8150\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.6141 - acc: 0.8445 - val_loss: 0.9257 - val_acc: 0.8190\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6074 - acc: 0.8452 - val_loss: 0.9411 - val_acc: 0.8140\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.5993 - acc: 0.8444 - val_loss: 0.9315 - val_acc: 0.8150\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.5990 - acc: 0.8453 - val_loss: 0.9328 - val_acc: 0.8160\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.5895 - acc: 0.8512 - val_loss: 0.9344 - val_acc: 0.8230\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.5687 - acc: 0.8603 - val_loss: 0.9338 - val_acc: 0.8190\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.5669 - acc: 0.8548 - val_loss: 0.9523 - val_acc: 0.8170\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.5582 - acc: 0.8547 - val_loss: 0.9452 - val_acc: 0.8160\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.5512 - acc: 0.8552 - val_loss: 0.9386 - val_acc: 0.8250\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.5518 - acc: 0.8603 - val_loss: 0.9496 - val_acc: 0.8230\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.5424 - acc: 0.8631 - val_loss: 0.9629 - val_acc: 0.8190\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.5366 - acc: 0.8603 - val_loss: 0.9543 - val_acc: 0.8240\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.5325 - acc: 0.8653 - val_loss: 0.9622 - val_acc: 0.8210\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.5443 - acc: 0.8571 - val_loss: 0.9507 - val_acc: 0.8280\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.5252 - acc: 0.8653 - val_loss: 0.9516 - val_acc: 0.8270\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.5080 - acc: 0.8661 - val_loss: 0.9496 - val_acc: 0.8210\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.5198 - acc: 0.8644 - val_loss: 0.9523 - val_acc: 0.8200\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.5121 - acc: 0.8677 - val_loss: 0.9545 - val_acc: 0.8200\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.5114 - acc: 0.8676 - val_loss: 0.9665 - val_acc: 0.8210\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.5072 - acc: 0.8681 - val_loss: 0.9526 - val_acc: 0.8250\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.5075 - acc: 0.8690 - val_loss: 0.9612 - val_acc: 0.8250\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.5127 - acc: 0.8671 - val_loss: 0.9484 - val_acc: 0.8240\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4802 - acc: 0.8743 - val_loss: 0.9533 - val_acc: 0.8250\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4862 - acc: 0.8726 - val_loss: 0.9673 - val_acc: 0.8180\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.4949 - acc: 0.8692 - val_loss: 0.9665 - val_acc: 0.8180\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4770 - acc: 0.8773 - val_loss: 0.9606 - val_acc: 0.8220\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.4773 - acc: 0.8767 - val_loss: 0.9767 - val_acc: 0.8210\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4810 - acc: 0.8742 - val_loss: 0.9674 - val_acc: 0.8220\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4844 - acc: 0.8797 - val_loss: 0.9902 - val_acc: 0.8180\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4887 - acc: 0.8732 - val_loss: 0.9734 - val_acc: 0.8220\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4768 - acc: 0.8777 - val_loss: 0.9862 - val_acc: 0.8240\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.4561 - acc: 0.8811 - val_loss: 0.9842 - val_acc: 0.8240\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4645 - acc: 0.8789 - val_loss: 0.9901 - val_acc: 0.8280\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.4681 - acc: 0.8792 - val_loss: 0.9791 - val_acc: 0.8240\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.4613 - acc: 0.8804 - val_loss: 1.0026 - val_acc: 0.8260\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4656 - acc: 0.8812 - val_loss: 1.0022 - val_acc: 0.8270\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4602 - acc: 0.8794 - val_loss: 0.9978 - val_acc: 0.8250\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.4472 - acc: 0.8805 - val_loss: 1.0089 - val_acc: 0.8280\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4474 - acc: 0.8840 - val_loss: 1.0010 - val_acc: 0.8320\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.4568 - acc: 0.8827 - val_loss: 0.9899 - val_acc: 0.8280\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4302 - acc: 0.8866 - val_loss: 1.0045 - val_acc: 0.8250\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4509 - acc: 0.8845 - val_loss: 1.0047 - val_acc: 0.8220\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4505 - acc: 0.8847 - val_loss: 1.0107 - val_acc: 0.8190\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.4417 - acc: 0.8869 - val_loss: 0.9861 - val_acc: 0.8200\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4391 - acc: 0.8814 - val_loss: 0.9956 - val_acc: 0.8220\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 0s 60us/step - loss: 0.4553 - acc: 0.8865 - val_loss: 1.0123 - val_acc: 0.8210\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4408 - acc: 0.8889 - val_loss: 1.0137 - val_acc: 0.8200\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.4428 - acc: 0.8905 - val_loss: 1.0201 - val_acc: 0.8170\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4378 - acc: 0.8872 - val_loss: 1.0142 - val_acc: 0.8230\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.4188 - acc: 0.8877 - val_loss: 1.0211 - val_acc: 0.8190\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4178 - acc: 0.8924 - val_loss: 1.0191 - val_acc: 0.8200\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.4421 - acc: 0.8860 - val_loss: 1.0324 - val_acc: 0.8190\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.4355 - acc: 0.8851 - val_loss: 1.0337 - val_acc: 0.8180\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4353 - acc: 0.8851 - val_loss: 1.0232 - val_acc: 0.8190\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4330 - acc: 0.8906 - val_loss: 1.0275 - val_acc: 0.8160\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.4320 - acc: 0.8889 - val_loss: 1.0332 - val_acc: 0.8200\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4373 - acc: 0.8877 - val_loss: 1.0299 - val_acc: 0.8160\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.4300 - acc: 0.8864 - val_loss: 1.0337 - val_acc: 0.8150\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4338 - acc: 0.8903 - val_loss: 1.0279 - val_acc: 0.8150\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4362 - acc: 0.8851 - val_loss: 1.0433 - val_acc: 0.8150\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4242 - acc: 0.8877 - val_loss: 1.0360 - val_acc: 0.8190\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.4303 - acc: 0.8891 - val_loss: 1.0355 - val_acc: 0.8100\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.4442 - acc: 0.8856 - val_loss: 1.0476 - val_acc: 0.8140\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.4235 - acc: 0.8901 - val_loss: 1.0395 - val_acc: 0.8100\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.4163 - acc: 0.8918 - val_loss: 1.0493 - val_acc: 0.8080\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.4210 - acc: 0.8871 - val_loss: 1.0578 - val_acc: 0.8090\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 4s 475us/step - loss: 2.5546 - acc: 0.4920 - val_loss: 1.6722 - val_acc: 0.6360\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.4216 - acc: 0.6962 - val_loss: 1.2915 - val_acc: 0.7200\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.0829 - acc: 0.7661 - val_loss: 1.1617 - val_acc: 0.7380\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.8829 - acc: 0.8096 - val_loss: 1.0657 - val_acc: 0.7790\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.7326 - acc: 0.8396 - val_loss: 0.9948 - val_acc: 0.7870\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.6098 - acc: 0.8675 - val_loss: 0.9722 - val_acc: 0.7960\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.5115 - acc: 0.8881 - val_loss: 0.9228 - val_acc: 0.8110\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.4278 - acc: 0.9078 - val_loss: 0.9202 - val_acc: 0.8100\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.3680 - acc: 0.9213 - val_loss: 0.9198 - val_acc: 0.8100\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.3142 - acc: 0.9336 - val_loss: 0.9340 - val_acc: 0.8150\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2785 - acc: 0.9352 - val_loss: 0.9394 - val_acc: 0.8110\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.2452 - acc: 0.9451 - val_loss: 0.9366 - val_acc: 0.8190\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.2308 - acc: 0.9441 - val_loss: 0.9394 - val_acc: 0.8220\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.2066 - acc: 0.9480 - val_loss: 0.9387 - val_acc: 0.8180\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1898 - acc: 0.9496 - val_loss: 0.9874 - val_acc: 0.8080\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1742 - acc: 0.9533 - val_loss: 1.0108 - val_acc: 0.8050\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1644 - acc: 0.9543 - val_loss: 1.0218 - val_acc: 0.8180\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1622 - acc: 0.9538 - val_loss: 1.0384 - val_acc: 0.8080\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.1497 - acc: 0.9564 - val_loss: 1.0537 - val_acc: 0.8030\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.1474 - acc: 0.9565 - val_loss: 1.0440 - val_acc: 0.8140\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1427 - acc: 0.9574 - val_loss: 1.0871 - val_acc: 0.8030\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1400 - acc: 0.9593 - val_loss: 1.0681 - val_acc: 0.8060\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.1313 - acc: 0.9602 - val_loss: 1.0700 - val_acc: 0.8050\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1294 - acc: 0.9582 - val_loss: 1.0527 - val_acc: 0.8090\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1277 - acc: 0.9598 - val_loss: 1.1345 - val_acc: 0.7940\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.1219 - acc: 0.9608 - val_loss: 1.1389 - val_acc: 0.7980\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.1189 - acc: 0.9605 - val_loss: 1.1460 - val_acc: 0.8020\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1184 - acc: 0.9582 - val_loss: 1.1103 - val_acc: 0.7970\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.1230 - acc: 0.9587 - val_loss: 1.1264 - val_acc: 0.7990\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1112 - acc: 0.9605 - val_loss: 1.1230 - val_acc: 0.7950\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1104 - acc: 0.9590 - val_loss: 1.0886 - val_acc: 0.7980\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1100 - acc: 0.9605 - val_loss: 1.1348 - val_acc: 0.8040\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1154 - acc: 0.9580 - val_loss: 1.1143 - val_acc: 0.7950\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1093 - acc: 0.9604 - val_loss: 1.1072 - val_acc: 0.7990\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.1060 - acc: 0.9599 - val_loss: 1.1740 - val_acc: 0.8020\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1046 - acc: 0.9594 - val_loss: 1.1722 - val_acc: 0.7930\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1078 - acc: 0.9594 - val_loss: 1.1545 - val_acc: 0.8010\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1043 - acc: 0.9619 - val_loss: 1.1301 - val_acc: 0.7990\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.1053 - acc: 0.9603 - val_loss: 1.1497 - val_acc: 0.7980\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1055 - acc: 0.9592 - val_loss: 1.1596 - val_acc: 0.7960\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.0966 - acc: 0.9605 - val_loss: 1.1626 - val_acc: 0.7940\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0956 - acc: 0.9605 - val_loss: 1.1632 - val_acc: 0.7930\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0994 - acc: 0.9595 - val_loss: 1.1922 - val_acc: 0.7980\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.1011 - acc: 0.9599 - val_loss: 1.1766 - val_acc: 0.7980\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0962 - acc: 0.9628 - val_loss: 1.1747 - val_acc: 0.7960\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0974 - acc: 0.9627 - val_loss: 1.1850 - val_acc: 0.7920\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0998 - acc: 0.9587 - val_loss: 1.2177 - val_acc: 0.7930\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.0961 - acc: 0.9613 - val_loss: 1.1631 - val_acc: 0.7930\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 0.0946 - acc: 0.9609 - val_loss: 1.1882 - val_acc: 0.7910\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.0976 - acc: 0.9599 - val_loss: 1.1866 - val_acc: 0.7920\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0897 - acc: 0.9614 - val_loss: 1.1740 - val_acc: 0.7940\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.0967 - acc: 0.9602 - val_loss: 1.2293 - val_acc: 0.7870\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0960 - acc: 0.9597 - val_loss: 1.2171 - val_acc: 0.7840\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0903 - acc: 0.9610 - val_loss: 1.2128 - val_acc: 0.7900\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0891 - acc: 0.9627 - val_loss: 1.2340 - val_acc: 0.7940\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.0902 - acc: 0.9622 - val_loss: 1.2120 - val_acc: 0.7920\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0889 - acc: 0.9622 - val_loss: 1.2485 - val_acc: 0.7900\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.0870 - acc: 0.9615 - val_loss: 1.2135 - val_acc: 0.7990\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0901 - acc: 0.9617 - val_loss: 1.2243 - val_acc: 0.7870\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.0828 - acc: 0.9632 - val_loss: 1.1995 - val_acc: 0.7940\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.0844 - acc: 0.9619 - val_loss: 1.2070 - val_acc: 0.7960\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0871 - acc: 0.9605 - val_loss: 1.2397 - val_acc: 0.7890\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.0882 - acc: 0.9628 - val_loss: 1.2650 - val_acc: 0.7890\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0882 - acc: 0.9610 - val_loss: 1.2299 - val_acc: 0.7880\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.0820 - acc: 0.9620 - val_loss: 1.2821 - val_acc: 0.7820\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0874 - acc: 0.9590 - val_loss: 1.2069 - val_acc: 0.7930\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0821 - acc: 0.9618 - val_loss: 1.2442 - val_acc: 0.7970\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.0845 - acc: 0.9617 - val_loss: 1.2493 - val_acc: 0.7960\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0825 - acc: 0.9585 - val_loss: 1.2547 - val_acc: 0.7920\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0836 - acc: 0.9613 - val_loss: 1.2614 - val_acc: 0.7960\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0812 - acc: 0.9627 - val_loss: 1.2457 - val_acc: 0.7920\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0821 - acc: 0.9622 - val_loss: 1.2794 - val_acc: 0.7990\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0831 - acc: 0.9595 - val_loss: 1.2895 - val_acc: 0.7850\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 0.0819 - acc: 0.9615 - val_loss: 1.2719 - val_acc: 0.7900\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0828 - acc: 0.9613 - val_loss: 1.2515 - val_acc: 0.7990\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0796 - acc: 0.9610 - val_loss: 1.2734 - val_acc: 0.7930\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0764 - acc: 0.9619 - val_loss: 1.2997 - val_acc: 0.7850\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0783 - acc: 0.9623 - val_loss: 1.2883 - val_acc: 0.7950\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0802 - acc: 0.9617 - val_loss: 1.2519 - val_acc: 0.7920\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0768 - acc: 0.9635 - val_loss: 1.2954 - val_acc: 0.7910\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0791 - acc: 0.9609 - val_loss: 1.2688 - val_acc: 0.7930\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0792 - acc: 0.9623 - val_loss: 1.3243 - val_acc: 0.7940\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.0759 - acc: 0.9629 - val_loss: 1.3352 - val_acc: 0.7850\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 0.0785 - acc: 0.9604 - val_loss: 1.3028 - val_acc: 0.7890\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 0.0756 - acc: 0.9624 - val_loss: 1.2766 - val_acc: 0.7920\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0779 - acc: 0.9612 - val_loss: 1.2792 - val_acc: 0.7930\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0760 - acc: 0.9612 - val_loss: 1.3092 - val_acc: 0.7890\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0763 - acc: 0.9622 - val_loss: 1.3005 - val_acc: 0.7880\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0763 - acc: 0.9607 - val_loss: 1.3125 - val_acc: 0.7940\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0768 - acc: 0.9615 - val_loss: 1.3086 - val_acc: 0.7900\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0785 - acc: 0.9595 - val_loss: 1.3032 - val_acc: 0.7960\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 0.0761 - acc: 0.9612 - val_loss: 1.3128 - val_acc: 0.7890\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.0769 - acc: 0.9613 - val_loss: 1.3264 - val_acc: 0.7870\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0773 - acc: 0.9612 - val_loss: 1.3497 - val_acc: 0.7870\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0738 - acc: 0.9624 - val_loss: 1.3377 - val_acc: 0.7930\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 0.0710 - acc: 0.9620 - val_loss: 1.3598 - val_acc: 0.7900\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0721 - acc: 0.9637 - val_loss: 1.3609 - val_acc: 0.7950\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 0.0753 - acc: 0.9610 - val_loss: 1.3568 - val_acc: 0.7980\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 0.0735 - acc: 0.9595 - val_loss: 1.3621 - val_acc: 0.7950\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 0.0731 - acc: 0.9605 - val_loss: 1.3612 - val_acc: 0.7910\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 4s 464us/step - loss: 3.9786 - acc: 0.2897 - val_loss: 2.8856 - val_acc: 0.5440\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 2.8417 - acc: 0.4909 - val_loss: 2.2708 - val_acc: 0.5840\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 2.4314 - acc: 0.5526 - val_loss: 2.0431 - val_acc: 0.6290\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 2.2119 - acc: 0.6017 - val_loss: 1.9208 - val_acc: 0.6820\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 2.0878 - acc: 0.6257 - val_loss: 1.8319 - val_acc: 0.6880\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.9995 - acc: 0.6377 - val_loss: 1.7608 - val_acc: 0.6950\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.9163 - acc: 0.6537 - val_loss: 1.7043 - val_acc: 0.6990\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.8482 - acc: 0.6630 - val_loss: 1.6669 - val_acc: 0.7020\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.8145 - acc: 0.6646 - val_loss: 1.6338 - val_acc: 0.7030\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.7507 - acc: 0.6725 - val_loss: 1.6060 - val_acc: 0.7020\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.7317 - acc: 0.6755 - val_loss: 1.5764 - val_acc: 0.7020\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.6943 - acc: 0.6788 - val_loss: 1.5544 - val_acc: 0.7030\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.6716 - acc: 0.6832 - val_loss: 1.5325 - val_acc: 0.7040\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.6363 - acc: 0.6857 - val_loss: 1.5140 - val_acc: 0.7050\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.6312 - acc: 0.6865 - val_loss: 1.4997 - val_acc: 0.7030\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.6061 - acc: 0.6928 - val_loss: 1.4829 - val_acc: 0.7120\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.5994 - acc: 0.6902 - val_loss: 1.4768 - val_acc: 0.7070\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.5740 - acc: 0.6947 - val_loss: 1.4542 - val_acc: 0.7130\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.5573 - acc: 0.6977 - val_loss: 1.4405 - val_acc: 0.7200\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.5356 - acc: 0.7008 - val_loss: 1.4371 - val_acc: 0.7170\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.5332 - acc: 0.7056 - val_loss: 1.4305 - val_acc: 0.7250\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.5266 - acc: 0.7016 - val_loss: 1.4161 - val_acc: 0.7240\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.5117 - acc: 0.7042 - val_loss: 1.4069 - val_acc: 0.7260\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.5210 - acc: 0.7010 - val_loss: 1.4039 - val_acc: 0.7280\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4971 - acc: 0.7042 - val_loss: 1.3951 - val_acc: 0.7300\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.4842 - acc: 0.7098 - val_loss: 1.3887 - val_acc: 0.7310\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4692 - acc: 0.7127 - val_loss: 1.3874 - val_acc: 0.7400\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4725 - acc: 0.7108 - val_loss: 1.3686 - val_acc: 0.7400\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4508 - acc: 0.7117 - val_loss: 1.3697 - val_acc: 0.7330\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.4637 - acc: 0.7129 - val_loss: 1.3691 - val_acc: 0.7300\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4472 - acc: 0.7167 - val_loss: 1.3579 - val_acc: 0.7360\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4453 - acc: 0.7145 - val_loss: 1.3582 - val_acc: 0.7350\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.4318 - acc: 0.7156 - val_loss: 1.3537 - val_acc: 0.7340\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4320 - acc: 0.7171 - val_loss: 1.3519 - val_acc: 0.7320\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.4273 - acc: 0.7209 - val_loss: 1.3455 - val_acc: 0.7400\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.4052 - acc: 0.7278 - val_loss: 1.3480 - val_acc: 0.7380\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.4100 - acc: 0.7217 - val_loss: 1.3467 - val_acc: 0.7350\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.4023 - acc: 0.7253 - val_loss: 1.3324 - val_acc: 0.7410\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3947 - acc: 0.7263 - val_loss: 1.3202 - val_acc: 0.7520\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.3918 - acc: 0.7285 - val_loss: 1.3152 - val_acc: 0.7410\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3880 - acc: 0.7273 - val_loss: 1.3023 - val_acc: 0.7520\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.3838 - acc: 0.7275 - val_loss: 1.3191 - val_acc: 0.7350\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3844 - acc: 0.7271 - val_loss: 1.3013 - val_acc: 0.7470\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3804 - acc: 0.7271 - val_loss: 1.3055 - val_acc: 0.7450\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3700 - acc: 0.7306 - val_loss: 1.2995 - val_acc: 0.7450\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3701 - acc: 0.7304 - val_loss: 1.3024 - val_acc: 0.7460\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.3638 - acc: 0.7329 - val_loss: 1.2958 - val_acc: 0.7460\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3570 - acc: 0.7331 - val_loss: 1.2904 - val_acc: 0.7550\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3499 - acc: 0.7388 - val_loss: 1.2837 - val_acc: 0.7590\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.3445 - acc: 0.7400 - val_loss: 1.2913 - val_acc: 0.7510\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3517 - acc: 0.7364 - val_loss: 1.2790 - val_acc: 0.7600\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3498 - acc: 0.7365 - val_loss: 1.2817 - val_acc: 0.7500\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3342 - acc: 0.7357 - val_loss: 1.2718 - val_acc: 0.7510\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3397 - acc: 0.7412 - val_loss: 1.2745 - val_acc: 0.7500\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3367 - acc: 0.7380 - val_loss: 1.2695 - val_acc: 0.7480\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.3413 - acc: 0.7390 - val_loss: 1.2694 - val_acc: 0.7530\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3272 - acc: 0.7359 - val_loss: 1.2644 - val_acc: 0.7540\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3310 - acc: 0.7397 - val_loss: 1.2571 - val_acc: 0.7600\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3245 - acc: 0.7443 - val_loss: 1.2563 - val_acc: 0.7570\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.3167 - acc: 0.7456 - val_loss: 1.2683 - val_acc: 0.7520\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3150 - acc: 0.7430 - val_loss: 1.2538 - val_acc: 0.7580\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3159 - acc: 0.7405 - val_loss: 1.2552 - val_acc: 0.7610\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3050 - acc: 0.7443 - val_loss: 1.2507 - val_acc: 0.7650\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3096 - acc: 0.7433 - val_loss: 1.2476 - val_acc: 0.7610\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.3087 - acc: 0.7461 - val_loss: 1.2541 - val_acc: 0.7580\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3023 - acc: 0.7409 - val_loss: 1.2392 - val_acc: 0.7650\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2984 - acc: 0.7473 - val_loss: 1.2476 - val_acc: 0.7570\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3050 - acc: 0.7494 - val_loss: 1.2484 - val_acc: 0.7620\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2959 - acc: 0.7442 - val_loss: 1.2443 - val_acc: 0.7610\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2920 - acc: 0.7471 - val_loss: 1.2342 - val_acc: 0.7730\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.3006 - acc: 0.7476 - val_loss: 1.2291 - val_acc: 0.7780\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.2799 - acc: 0.7479 - val_loss: 1.2329 - val_acc: 0.7770\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2825 - acc: 0.7472 - val_loss: 1.2341 - val_acc: 0.7720\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2905 - acc: 0.7469 - val_loss: 1.2400 - val_acc: 0.7660\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2824 - acc: 0.7511 - val_loss: 1.2230 - val_acc: 0.7750\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2783 - acc: 0.7481 - val_loss: 1.2285 - val_acc: 0.7780\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.2710 - acc: 0.7543 - val_loss: 1.2261 - val_acc: 0.7670\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.2744 - acc: 0.7518 - val_loss: 1.2247 - val_acc: 0.7700\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2725 - acc: 0.7571 - val_loss: 1.2190 - val_acc: 0.7740\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2654 - acc: 0.7565 - val_loss: 1.2196 - val_acc: 0.7770\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.2612 - acc: 0.7570 - val_loss: 1.2197 - val_acc: 0.7760\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2762 - acc: 0.7557 - val_loss: 1.2227 - val_acc: 0.7750\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2689 - acc: 0.7557 - val_loss: 1.2089 - val_acc: 0.7810\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2589 - acc: 0.7538 - val_loss: 1.2089 - val_acc: 0.7770\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2588 - acc: 0.7522 - val_loss: 1.2078 - val_acc: 0.7790\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2535 - acc: 0.7518 - val_loss: 1.2106 - val_acc: 0.7810\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.2436 - acc: 0.7546 - val_loss: 1.2131 - val_acc: 0.7780\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.2498 - acc: 0.7567 - val_loss: 1.2314 - val_acc: 0.7680\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2625 - acc: 0.7514 - val_loss: 1.2092 - val_acc: 0.7820\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.2454 - acc: 0.7596 - val_loss: 1.2078 - val_acc: 0.7880\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.2358 - acc: 0.7585 - val_loss: 1.1993 - val_acc: 0.7850\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.2462 - acc: 0.7578 - val_loss: 1.1952 - val_acc: 0.7880\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.2350 - acc: 0.7583 - val_loss: 1.2038 - val_acc: 0.7820\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 71us/step - loss: 1.2472 - acc: 0.7582 - val_loss: 1.2095 - val_acc: 0.7800\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 69us/step - loss: 1.2416 - acc: 0.7601 - val_loss: 1.2038 - val_acc: 0.7860\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.2317 - acc: 0.7607 - val_loss: 1.2070 - val_acc: 0.7850\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.2404 - acc: 0.7595 - val_loss: 1.1969 - val_acc: 0.7800\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2516 - acc: 0.7592 - val_loss: 1.2018 - val_acc: 0.7850\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2390 - acc: 0.7596 - val_loss: 1.1981 - val_acc: 0.7860\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.2358 - acc: 0.7588 - val_loss: 1.2016 - val_acc: 0.7870\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "7982/7982 [==============================] - 4s 484us/step - loss: 3.1501 - acc: 0.3096 - val_loss: 2.3304 - val_acc: 0.5310\n",
            "Epoch 2/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 2.3762 - acc: 0.4595 - val_loss: 1.8591 - val_acc: 0.5540\n",
            "Epoch 3/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 2.1008 - acc: 0.5085 - val_loss: 1.6707 - val_acc: 0.6090\n",
            "Epoch 4/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.9422 - acc: 0.5476 - val_loss: 1.5686 - val_acc: 0.6500\n",
            "Epoch 5/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.8458 - acc: 0.5708 - val_loss: 1.5004 - val_acc: 0.6710\n",
            "Epoch 6/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.7710 - acc: 0.5943 - val_loss: 1.4474 - val_acc: 0.6920\n",
            "Epoch 7/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.7163 - acc: 0.6037 - val_loss: 1.3997 - val_acc: 0.6940\n",
            "Epoch 8/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.6937 - acc: 0.6100 - val_loss: 1.3780 - val_acc: 0.6990\n",
            "Epoch 9/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.6281 - acc: 0.6193 - val_loss: 1.3334 - val_acc: 0.7040\n",
            "Epoch 10/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.5987 - acc: 0.6310 - val_loss: 1.2936 - val_acc: 0.7040\n",
            "Epoch 11/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.5615 - acc: 0.6434 - val_loss: 1.2784 - val_acc: 0.7100\n",
            "Epoch 12/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.5408 - acc: 0.6398 - val_loss: 1.2452 - val_acc: 0.7190\n",
            "Epoch 13/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.5121 - acc: 0.6543 - val_loss: 1.2257 - val_acc: 0.7250\n",
            "Epoch 14/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.4834 - acc: 0.6622 - val_loss: 1.2047 - val_acc: 0.7270\n",
            "Epoch 15/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.4524 - acc: 0.6685 - val_loss: 1.1961 - val_acc: 0.7350\n",
            "Epoch 16/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4441 - acc: 0.6642 - val_loss: 1.1744 - val_acc: 0.7410\n",
            "Epoch 17/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.4345 - acc: 0.6694 - val_loss: 1.1642 - val_acc: 0.7500\n",
            "Epoch 18/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.4266 - acc: 0.6694 - val_loss: 1.1534 - val_acc: 0.7450\n",
            "Epoch 19/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.4092 - acc: 0.6716 - val_loss: 1.1348 - val_acc: 0.7510\n",
            "Epoch 20/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3874 - acc: 0.6789 - val_loss: 1.1283 - val_acc: 0.7530\n",
            "Epoch 21/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3700 - acc: 0.6795 - val_loss: 1.1158 - val_acc: 0.7580\n",
            "Epoch 22/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.3776 - acc: 0.6761 - val_loss: 1.1024 - val_acc: 0.7650\n",
            "Epoch 23/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3643 - acc: 0.6823 - val_loss: 1.1006 - val_acc: 0.7700\n",
            "Epoch 24/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3430 - acc: 0.6893 - val_loss: 1.0848 - val_acc: 0.7620\n",
            "Epoch 25/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3362 - acc: 0.6932 - val_loss: 1.0743 - val_acc: 0.7640\n",
            "Epoch 26/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.3295 - acc: 0.6942 - val_loss: 1.0694 - val_acc: 0.7740\n",
            "Epoch 27/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.3065 - acc: 0.6966 - val_loss: 1.0551 - val_acc: 0.7730\n",
            "Epoch 28/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2942 - acc: 0.7033 - val_loss: 1.0454 - val_acc: 0.7760\n",
            "Epoch 29/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.3105 - acc: 0.6902 - val_loss: 1.0488 - val_acc: 0.7830\n",
            "Epoch 30/100\n",
            "7982/7982 [==============================] - 0s 61us/step - loss: 1.3072 - acc: 0.7005 - val_loss: 1.0394 - val_acc: 0.7790\n",
            "Epoch 31/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.2968 - acc: 0.6974 - val_loss: 1.0373 - val_acc: 0.7880\n",
            "Epoch 32/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2903 - acc: 0.6999 - val_loss: 1.0296 - val_acc: 0.7840\n",
            "Epoch 33/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2703 - acc: 0.7020 - val_loss: 1.0240 - val_acc: 0.7840\n",
            "Epoch 34/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.2676 - acc: 0.7043 - val_loss: 1.0189 - val_acc: 0.7880\n",
            "Epoch 35/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2722 - acc: 0.7053 - val_loss: 1.0168 - val_acc: 0.7850\n",
            "Epoch 36/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2643 - acc: 0.7058 - val_loss: 1.0121 - val_acc: 0.7910\n",
            "Epoch 37/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2617 - acc: 0.7032 - val_loss: 1.0041 - val_acc: 0.7930\n",
            "Epoch 38/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2345 - acc: 0.7102 - val_loss: 1.0012 - val_acc: 0.7970\n",
            "Epoch 39/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2465 - acc: 0.7091 - val_loss: 0.9956 - val_acc: 0.7930\n",
            "Epoch 40/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.2504 - acc: 0.7080 - val_loss: 0.9968 - val_acc: 0.7940\n",
            "Epoch 41/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.2268 - acc: 0.7096 - val_loss: 0.9922 - val_acc: 0.7940\n",
            "Epoch 42/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.2350 - acc: 0.7127 - val_loss: 0.9938 - val_acc: 0.7970\n",
            "Epoch 43/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.2303 - acc: 0.7077 - val_loss: 0.9903 - val_acc: 0.8010\n",
            "Epoch 44/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2224 - acc: 0.7132 - val_loss: 0.9882 - val_acc: 0.7980\n",
            "Epoch 45/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.2372 - acc: 0.7068 - val_loss: 0.9792 - val_acc: 0.8020\n",
            "Epoch 46/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.2241 - acc: 0.7131 - val_loss: 0.9842 - val_acc: 0.8000\n",
            "Epoch 47/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.2205 - acc: 0.7156 - val_loss: 0.9800 - val_acc: 0.7990\n",
            "Epoch 48/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1845 - acc: 0.7221 - val_loss: 0.9784 - val_acc: 0.7930\n",
            "Epoch 49/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.2078 - acc: 0.7142 - val_loss: 0.9770 - val_acc: 0.8030\n",
            "Epoch 50/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.2013 - acc: 0.7204 - val_loss: 0.9740 - val_acc: 0.7930\n",
            "Epoch 51/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.2046 - acc: 0.7155 - val_loss: 0.9669 - val_acc: 0.7990\n",
            "Epoch 52/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1974 - acc: 0.7199 - val_loss: 0.9694 - val_acc: 0.8030\n",
            "Epoch 53/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1670 - acc: 0.7258 - val_loss: 0.9622 - val_acc: 0.7930\n",
            "Epoch 54/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.1887 - acc: 0.7172 - val_loss: 0.9600 - val_acc: 0.8010\n",
            "Epoch 55/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1968 - acc: 0.7170 - val_loss: 0.9588 - val_acc: 0.7930\n",
            "Epoch 56/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1837 - acc: 0.7206 - val_loss: 0.9487 - val_acc: 0.8000\n",
            "Epoch 57/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1772 - acc: 0.7167 - val_loss: 0.9478 - val_acc: 0.8040\n",
            "Epoch 58/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1866 - acc: 0.7192 - val_loss: 0.9496 - val_acc: 0.7990\n",
            "Epoch 59/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1712 - acc: 0.7221 - val_loss: 0.9403 - val_acc: 0.8050\n",
            "Epoch 60/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.1969 - acc: 0.7200 - val_loss: 0.9423 - val_acc: 0.7990\n",
            "Epoch 61/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.1801 - acc: 0.7189 - val_loss: 0.9402 - val_acc: 0.7980\n",
            "Epoch 62/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.1725 - acc: 0.7199 - val_loss: 0.9352 - val_acc: 0.7970\n",
            "Epoch 63/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1795 - acc: 0.7209 - val_loss: 0.9345 - val_acc: 0.8000\n",
            "Epoch 64/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1861 - acc: 0.7201 - val_loss: 0.9268 - val_acc: 0.7960\n",
            "Epoch 65/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1570 - acc: 0.7202 - val_loss: 0.9269 - val_acc: 0.8020\n",
            "Epoch 66/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1726 - acc: 0.7209 - val_loss: 0.9264 - val_acc: 0.8000\n",
            "Epoch 67/100\n",
            "7982/7982 [==============================] - 0s 63us/step - loss: 1.1575 - acc: 0.7295 - val_loss: 0.9305 - val_acc: 0.8020\n",
            "Epoch 68/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1690 - acc: 0.7196 - val_loss: 0.9254 - val_acc: 0.8020\n",
            "Epoch 69/100\n",
            "7982/7982 [==============================] - 1s 67us/step - loss: 1.1631 - acc: 0.7187 - val_loss: 0.9219 - val_acc: 0.8000\n",
            "Epoch 70/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.1560 - acc: 0.7200 - val_loss: 0.9213 - val_acc: 0.7980\n",
            "Epoch 71/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1579 - acc: 0.7271 - val_loss: 0.9163 - val_acc: 0.8010\n",
            "Epoch 72/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1323 - acc: 0.7316 - val_loss: 0.9123 - val_acc: 0.8040\n",
            "Epoch 73/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1504 - acc: 0.7281 - val_loss: 0.9112 - val_acc: 0.8100\n",
            "Epoch 74/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1478 - acc: 0.7223 - val_loss: 0.9110 - val_acc: 0.8020\n",
            "Epoch 75/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1503 - acc: 0.7303 - val_loss: 0.9112 - val_acc: 0.8050\n",
            "Epoch 76/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1389 - acc: 0.7329 - val_loss: 0.9092 - val_acc: 0.8020\n",
            "Epoch 77/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1430 - acc: 0.7269 - val_loss: 0.9062 - val_acc: 0.8030\n",
            "Epoch 78/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1384 - acc: 0.7294 - val_loss: 0.9030 - val_acc: 0.8080\n",
            "Epoch 79/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1311 - acc: 0.7306 - val_loss: 0.9042 - val_acc: 0.8090\n",
            "Epoch 80/100\n",
            "7982/7982 [==============================] - 1s 68us/step - loss: 1.1390 - acc: 0.7206 - val_loss: 0.9004 - val_acc: 0.8090\n",
            "Epoch 81/100\n",
            "7982/7982 [==============================] - 1s 70us/step - loss: 1.1431 - acc: 0.7275 - val_loss: 0.9014 - val_acc: 0.8020\n",
            "Epoch 82/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1317 - acc: 0.7259 - val_loss: 0.8941 - val_acc: 0.8040\n",
            "Epoch 83/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1324 - acc: 0.7274 - val_loss: 0.8977 - val_acc: 0.8040\n",
            "Epoch 84/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1302 - acc: 0.7281 - val_loss: 0.8948 - val_acc: 0.8040\n",
            "Epoch 85/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.1183 - acc: 0.7274 - val_loss: 0.8948 - val_acc: 0.8110\n",
            "Epoch 86/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1399 - acc: 0.7328 - val_loss: 0.8954 - val_acc: 0.8110\n",
            "Epoch 87/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1257 - acc: 0.7296 - val_loss: 0.8918 - val_acc: 0.8100\n",
            "Epoch 88/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1397 - acc: 0.7271 - val_loss: 0.8964 - val_acc: 0.8070\n",
            "Epoch 89/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1053 - acc: 0.7377 - val_loss: 0.8886 - val_acc: 0.8090\n",
            "Epoch 90/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1178 - acc: 0.7320 - val_loss: 0.8873 - val_acc: 0.8050\n",
            "Epoch 91/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.1037 - acc: 0.7301 - val_loss: 0.8865 - val_acc: 0.8090\n",
            "Epoch 92/100\n",
            "7982/7982 [==============================] - 0s 62us/step - loss: 1.1371 - acc: 0.7215 - val_loss: 0.8910 - val_acc: 0.8090\n",
            "Epoch 93/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1244 - acc: 0.7328 - val_loss: 0.8836 - val_acc: 0.8120\n",
            "Epoch 94/100\n",
            "7982/7982 [==============================] - 1s 63us/step - loss: 1.1464 - acc: 0.7286 - val_loss: 0.8822 - val_acc: 0.8090\n",
            "Epoch 95/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1221 - acc: 0.7331 - val_loss: 0.8871 - val_acc: 0.8060\n",
            "Epoch 96/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1150 - acc: 0.7314 - val_loss: 0.8927 - val_acc: 0.8050\n",
            "Epoch 97/100\n",
            "7982/7982 [==============================] - 1s 66us/step - loss: 1.1363 - acc: 0.7250 - val_loss: 0.8835 - val_acc: 0.8060\n",
            "Epoch 98/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1203 - acc: 0.7335 - val_loss: 0.8857 - val_acc: 0.8030\n",
            "Epoch 99/100\n",
            "7982/7982 [==============================] - 1s 65us/step - loss: 1.1155 - acc: 0.7315 - val_loss: 0.8860 - val_acc: 0.8060\n",
            "Epoch 100/100\n",
            "7982/7982 [==============================] - 1s 64us/step - loss: 1.1131 - acc: 0.7380 - val_loss: 0.8851 - val_acc: 0.8040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG1TuzcgwJRh",
        "colab_type": "text"
      },
      "source": [
        "#### 3. Make scatter plots showing how validation accuracy varies with the hyperparameter values.\n",
        "\n",
        "You should make three scatter plots: one showing val_acc vs input_dropout_rates; a second showing val_acc vs hidden_dropout_rates; and a third showing val_acc vs l2_penalties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjYsEWD1n5QZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "outputId": "aa9b81e7-9a3c-4636-bc61-fbd0b06fbec8"
      },
      "source": [
        "plt.scatter(input_dropout_rates, val_acc)\n",
        "plt.show()\n",
        "plt.scatter(hidden_dropout_rates, val_acc)\n",
        "plt.show()\n",
        "plt.scatter(l2_penalties, val_acc)\n",
        "plt.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noDczS5yD3swscf8PAlpwrOKzzGAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOejNzBLnoDczS5yD3swscf8P67vUq/vlDgsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTJP1yoJwn2I",
        "colab_type": "text"
      },
      "source": [
        "#### 4. Based on the scatter plots in part 3, which hyperparameter settings seem best?  Please pick a specific number for each hyperparameter.  We don't know the exact best values, but we can make a good enough determination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWUmZhPfw39N",
        "colab_type": "text"
      },
      "source": [
        "(put your answer here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aaEpqD-w6vL",
        "colab_type": "text"
      },
      "source": [
        "#### 5. Suppose we had used a grid search with 8 combinations of parameter settings instead of the random search procedure with 8 combinations.  How many unique values would we have been able to try for each of the three hyperparameters?  Would the results have given us as good of a view of how model performance depended on each model parameter as what we saw here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K60ZZAIxWHN",
        "colab_type": "text"
      },
      "source": [
        "(put your answer here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNQIUuV2YEuY",
        "colab_type": "text"
      },
      "source": [
        "## Refit models to the combined training and validation set data\n",
        "When we are making test set predictions, we want to do so with a model that was fit using as much data as possible.\n",
        "\n",
        "### Chollet's Model\n",
        "\n",
        "You don't need to do anything here other than run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf4WT59OYMyc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "18783a58-2c2c-43a7-c060-59c77bb48b04"
      },
      "source": [
        "history = chollet_model.fit(x_train_and_val,\n",
        "                    y_train_and_val,\n",
        "                    epochs=20,\n",
        "                    batch_size=512)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.2128 - acc: 0.9418\n",
            "Epoch 2/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.1675 - acc: 0.9481\n",
            "Epoch 3/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1514 - acc: 0.9506\n",
            "Epoch 4/20\n",
            "8982/8982 [==============================] - 0s 45us/step - loss: 0.1317 - acc: 0.9520\n",
            "Epoch 5/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1276 - acc: 0.9538\n",
            "Epoch 6/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.1217 - acc: 0.9537\n",
            "Epoch 7/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1178 - acc: 0.9552\n",
            "Epoch 8/20\n",
            "8982/8982 [==============================] - 0s 48us/step - loss: 0.1141 - acc: 0.9552\n",
            "Epoch 9/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.1145 - acc: 0.9544\n",
            "Epoch 10/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1115 - acc: 0.9557\n",
            "Epoch 11/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1095 - acc: 0.9547\n",
            "Epoch 12/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.1079 - acc: 0.9561\n",
            "Epoch 13/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.1042 - acc: 0.9561\n",
            "Epoch 14/20\n",
            "8982/8982 [==============================] - 0s 45us/step - loss: 0.1073 - acc: 0.9534\n",
            "Epoch 15/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1022 - acc: 0.9544\n",
            "Epoch 16/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.1021 - acc: 0.9550\n",
            "Epoch 17/20\n",
            "8982/8982 [==============================] - 0s 46us/step - loss: 0.1032 - acc: 0.9555\n",
            "Epoch 18/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1004 - acc: 0.9562\n",
            "Epoch 19/20\n",
            "8982/8982 [==============================] - 0s 45us/step - loss: 0.1004 - acc: 0.9532\n",
            "Epoch 20/20\n",
            "8982/8982 [==============================] - 0s 47us/step - loss: 0.1022 - acc: 0.9559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7sruJY_Zmeq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "030bb0c8-714e-4c9f-c58e-82bd2f3b2193"
      },
      "source": [
        "chollet_model.evaluate(x_test, y_test)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2246/2246 [==============================] - 0s 80us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3978797875656577, 0.783615316064466]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCQ4YhOjxiz-",
        "colab_type": "text"
      },
      "source": [
        "#### 6. Refit your final model to the combined training and validation set data\n",
        "\n",
        "You will need to redefine and refit your model using the hyperparameter settings you selected in part 4.  You should be fitting to x_train_and_val and y_train_and_val using rmsprop with 100 epochs and a batch size of 512."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsbUROQ02N5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "365e1938-4c80-44d4-8294-f4ca74903f73"
      },
      "source": [
        "final_model = models.Sequential()\n",
        "\n",
        "final_model.add(layers.Dropout(rate = 0.8))\n",
        "final_model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "final_model.add(layers.Dropout(rate = 0.2))\n",
        "final_model.add(layers.Dense(64, activation='relu'))\n",
        "final_model.add(layers.Dropout(rate = 0.2))\n",
        "final_model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "final_model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = final_model.fit(x_train_and_val,\n",
        "                    y_train_and_val,\n",
        "                    epochs=100,\n",
        "                    batch_size=512)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8982/8982 [==============================] - 4s 412us/step - loss: 2.9384 - acc: 0.3887\n",
            "Epoch 2/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 2.0825 - acc: 0.5356\n",
            "Epoch 3/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 1.8249 - acc: 0.5888\n",
            "Epoch 4/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 1.6882 - acc: 0.6161\n",
            "Epoch 5/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 1.5845 - acc: 0.6336\n",
            "Epoch 6/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 1.5081 - acc: 0.6501\n",
            "Epoch 7/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 1.4688 - acc: 0.6553\n",
            "Epoch 8/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 1.4098 - acc: 0.6663\n",
            "Epoch 9/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 1.3634 - acc: 0.6811\n",
            "Epoch 10/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 1.3159 - acc: 0.6856\n",
            "Epoch 11/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 1.2783 - acc: 0.6964\n",
            "Epoch 12/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 1.2447 - acc: 0.6986\n",
            "Epoch 13/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 1.2236 - acc: 0.7040\n",
            "Epoch 14/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 1.2071 - acc: 0.7081\n",
            "Epoch 15/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 1.1696 - acc: 0.7178\n",
            "Epoch 16/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 1.1551 - acc: 0.7194\n",
            "Epoch 17/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 1.1260 - acc: 0.7233\n",
            "Epoch 18/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 1.1177 - acc: 0.7281\n",
            "Epoch 19/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 1.0953 - acc: 0.7233\n",
            "Epoch 20/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 1.0616 - acc: 0.7383\n",
            "Epoch 21/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 1.0654 - acc: 0.7376\n",
            "Epoch 22/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 1.0433 - acc: 0.7407\n",
            "Epoch 23/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 1.0204 - acc: 0.7473\n",
            "Epoch 24/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 1.0070 - acc: 0.7493\n",
            "Epoch 25/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 1.0031 - acc: 0.7488\n",
            "Epoch 26/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.9677 - acc: 0.7531\n",
            "Epoch 27/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.9761 - acc: 0.7533\n",
            "Epoch 28/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 0.9526 - acc: 0.7614\n",
            "Epoch 29/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.9505 - acc: 0.7584\n",
            "Epoch 30/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 0.9307 - acc: 0.7609\n",
            "Epoch 31/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.9319 - acc: 0.7629\n",
            "Epoch 32/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.9098 - acc: 0.7728\n",
            "Epoch 33/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.9286 - acc: 0.7661\n",
            "Epoch 34/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.8737 - acc: 0.7797\n",
            "Epoch 35/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.8864 - acc: 0.7703\n",
            "Epoch 36/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.8688 - acc: 0.7713\n",
            "Epoch 37/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.8619 - acc: 0.7779\n",
            "Epoch 38/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.8720 - acc: 0.7743\n",
            "Epoch 39/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.8639 - acc: 0.7801\n",
            "Epoch 40/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.8677 - acc: 0.7739\n",
            "Epoch 41/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.8427 - acc: 0.7809\n",
            "Epoch 42/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.8458 - acc: 0.7777\n",
            "Epoch 43/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.8221 - acc: 0.7856\n",
            "Epoch 44/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.8205 - acc: 0.7878\n",
            "Epoch 45/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.8298 - acc: 0.7837\n",
            "Epoch 46/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 0.8108 - acc: 0.7861\n",
            "Epoch 47/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.8144 - acc: 0.7905\n",
            "Epoch 48/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 0.8104 - acc: 0.7889\n",
            "Epoch 49/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.8023 - acc: 0.7931\n",
            "Epoch 50/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 0.7923 - acc: 0.7915\n",
            "Epoch 51/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.7838 - acc: 0.7983\n",
            "Epoch 52/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.7853 - acc: 0.7944\n",
            "Epoch 53/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.7818 - acc: 0.7931\n",
            "Epoch 54/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 0.7678 - acc: 0.8003\n",
            "Epoch 55/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.7776 - acc: 0.7960\n",
            "Epoch 56/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 0.7771 - acc: 0.7916\n",
            "Epoch 57/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.7523 - acc: 0.7995\n",
            "Epoch 58/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.7448 - acc: 0.7977\n",
            "Epoch 59/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.7591 - acc: 0.7928\n",
            "Epoch 60/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.7698 - acc: 0.7925\n",
            "Epoch 61/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.7482 - acc: 0.7980\n",
            "Epoch 62/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 0.7579 - acc: 0.8013\n",
            "Epoch 63/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.7322 - acc: 0.8066\n",
            "Epoch 64/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.7272 - acc: 0.8094\n",
            "Epoch 65/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.7480 - acc: 0.8065\n",
            "Epoch 66/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.7311 - acc: 0.8078\n",
            "Epoch 67/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.7103 - acc: 0.8140\n",
            "Epoch 68/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 0.7316 - acc: 0.8052\n",
            "Epoch 69/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.7522 - acc: 0.7985\n",
            "Epoch 70/100\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.7109 - acc: 0.8152\n",
            "Epoch 71/100\n",
            "8982/8982 [==============================] - 1s 60us/step - loss: 0.7156 - acc: 0.8116\n",
            "Epoch 72/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 0.7036 - acc: 0.8107\n",
            "Epoch 73/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 0.6858 - acc: 0.8141\n",
            "Epoch 74/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 0.7031 - acc: 0.8086\n",
            "Epoch 75/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 0.6982 - acc: 0.8138\n",
            "Epoch 76/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.7230 - acc: 0.8029\n",
            "Epoch 77/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6852 - acc: 0.8122\n",
            "Epoch 78/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.6992 - acc: 0.8093\n",
            "Epoch 79/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 0.7127 - acc: 0.8104\n",
            "Epoch 80/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.6885 - acc: 0.8091\n",
            "Epoch 81/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6846 - acc: 0.8152\n",
            "Epoch 82/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.6902 - acc: 0.8105\n",
            "Epoch 83/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6988 - acc: 0.8132\n",
            "Epoch 84/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6830 - acc: 0.8132\n",
            "Epoch 85/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6855 - acc: 0.8092\n",
            "Epoch 86/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.6845 - acc: 0.8180\n",
            "Epoch 87/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6797 - acc: 0.8152\n",
            "Epoch 88/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6737 - acc: 0.8179\n",
            "Epoch 89/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.6842 - acc: 0.8144\n",
            "Epoch 90/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6756 - acc: 0.8165\n",
            "Epoch 91/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.6708 - acc: 0.8210\n",
            "Epoch 92/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.6748 - acc: 0.8150\n",
            "Epoch 93/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.6776 - acc: 0.8145\n",
            "Epoch 94/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6812 - acc: 0.8152\n",
            "Epoch 95/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.6789 - acc: 0.8108\n",
            "Epoch 96/100\n",
            "8982/8982 [==============================] - 1s 58us/step - loss: 0.6617 - acc: 0.8204\n",
            "Epoch 97/100\n",
            "8982/8982 [==============================] - 1s 56us/step - loss: 0.6646 - acc: 0.8180\n",
            "Epoch 98/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.6566 - acc: 0.8223\n",
            "Epoch 99/100\n",
            "8982/8982 [==============================] - 1s 59us/step - loss: 0.6567 - acc: 0.8229\n",
            "Epoch 100/100\n",
            "8982/8982 [==============================] - 1s 57us/step - loss: 0.6613 - acc: 0.8203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeNbclNmyqy3",
        "colab_type": "text"
      },
      "source": [
        "#### 7. Find the test set performance of your final model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WucnaK6g2pAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "08129b82-8b6c-42cc-cbe2-14ec8de854ec"
      },
      "source": [
        "final_model.evaluate(x_test, y_test)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2246/2246 [==============================] - 1s 601us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9073030613196926, 0.8081032947462155]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCBSumHDyu0Z",
        "colab_type": "text"
      },
      "source": [
        "#### 8. Discuss the relative performance of Chollet's model and your final model on the training set and on the validation set.  Does it seem like you have addressed the problem of overfitting the training data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsTeGEQezAWj",
        "colab_type": "text"
      },
      "source": [
        "(put your answer here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QZolvaPzBm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}